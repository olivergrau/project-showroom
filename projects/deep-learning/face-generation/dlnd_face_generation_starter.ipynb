{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bdd8d56",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "In this project, you'll define and train a Generative Adverserial network of your own creation on a dataset of faces. Your goal is to get a generator network to generate *new* images of faces that look as realistic as possible!\n",
    "\n",
    "The project will be broken down into a series of tasks from **defining new architectures training adversarial networks**. At the end of the notebook, you'll be able to visualize the results of your trained Generator to see how it performs; your generated samples should look like fairly realistic faces with small amounts of noise.\n",
    "\n",
    "### Get the Data\n",
    "\n",
    "You'll be using the [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to train your adversarial networks.\n",
    "\n",
    "This dataset has higher resolution images than datasets you have previously worked with (like MNIST or SVHN) you've been working with, and so, you should prepare to define deeper networks and train them for a longer time to get good results. It is suggested that you utilize a GPU for training.\n",
    "\n",
    "### Pre-processed Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. Some sample data is show below.\n",
    "\n",
    "<img src='assets/processed_face_data.png' width=60% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6442670",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run this once to unzip the file\n",
    "!unzip processed-celeba-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b809b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from typing import Tuple, Callable, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, Normalize\n",
    "\n",
    "import os\n",
    "import tests\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d14926-87b6-4490-8800-2f2134f75b6d",
   "metadata": {},
   "source": [
    "I've tried to fix the random values, but that also fixes the results a bit. So only execute it if you want the noise be stabilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f85980-a70e-4d51-b197-8d90a2e97d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45791d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'processed_celeba_small/celeba/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c54a5",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "The [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations, you'll only need the images. Note that these are color images with [3 color channels (RGB)](https://en.wikipedia.org/wiki/Channel_(digital_image)#RGB_Images) each.\n",
    "\n",
    "### Pre-process and Load the Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. This *pre-processed* dataset is a smaller subset of the very large CelebA dataset and contains roughly 30,000 images. \n",
    "\n",
    "Your first task consists in building the dataloader. To do so, you need to do the following:\n",
    "* implement the get_transforms function\n",
    "* create a custom Dataset class that reads the CelebA data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c223cb",
   "metadata": {},
   "source": [
    "### Exercise: implement the get_transforms function\n",
    "\n",
    "The `get_transforms` function should output a [`torchvision.transforms.Compose`](https://pytorch.org/vision/stable/generated/torchvision.transforms.Compose.html#torchvision.transforms.Compose) of different transformations. You have two constraints:\n",
    "* the function takes a tuple of size as input and should **resize the images** to the input size\n",
    "* the output images should have values **ranging from -1 to 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(size: Tuple[int, int]) -> Callable:\n",
    "    \"\"\" Transforms to apply to the image.\"\"\"\n",
    "    transforms = [\n",
    "        Resize(size),  # Resize the image to the specified size\n",
    "        ToTensor(),    # Convert the image to a PyTorch tensor\n",
    "        Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "    ]\n",
    "    \n",
    "    return Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce5b03c",
   "metadata": {},
   "source": [
    "### Exercise: implement the DatasetDirectory class\n",
    "\n",
    "\n",
    "The `DatasetDirectory` class is a torch Dataset that reads from the above data directory. The `__getitem__` method should output a transformed tensor and the `__len__` method should output the number of files in our dataset. You can look at [this custom dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) for ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDirectory(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class that loads images from a folder.\n",
    "    \n",
    "    Args:\n",
    "    - directory (str): Location of the images.\n",
    "    - transforms (Callable): Transform function to apply to the images.\n",
    "    - extension (str): File format to filter images by (e.g., '.jpg', '.png').\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, directory: str, transforms: Callable = None, extension: str = '.jpg'):\n",
    "        self.directory = directory\n",
    "        self.transforms = transforms if transforms else get_transforms((64, 64)) # more flexibility for transformations\n",
    "        self.extension = extension\n",
    "        self.image_paths = [os.path.join(directory, f) for f in os.listdir(directory) \n",
    "                            if f.endswith(extension)]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of items in the dataset.\"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> torch.Tensor:\n",
    "        \"\"\"Loads an image, applies transformation, and returns it.\"\"\"\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "        \n",
    "        # Apply the transformations to the image\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# run this cell to verify your dataset implementation\n",
    "dataset = DatasetDirectory(data_dir, get_transforms((64, 64)))\n",
    "tests.check_dataset_outputs(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93073f6a",
   "metadata": {},
   "source": [
    "The functions below will help you visualize images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2873455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def denormalize(images):\n",
    "    \"\"\"Transform images from [-1.0, 1.0] to [0, 255] and cast them to uint8.\"\"\"\n",
    "    return ((images + 1.) / 2. * 255).astype(np.uint8)\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "    img = dataset[idx].numpy()\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img = denormalize(img)\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1adf3d",
   "metadata": {},
   "source": [
    "## Model implementation\n",
    "\n",
    "As you know, a GAN is comprised of two adversarial networks, a discriminator and a generator. Now that we have a working data pipeline, we need to implement the discriminator and the generator. \n",
    "\n",
    "Feel free to implement any additional class or function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5524988a",
   "metadata": {},
   "source": [
    "### Exercise: Create the discriminator\n",
    "\n",
    "The discriminator's job is to score real and fake images. You have two constraints here:\n",
    "* the discriminator takes as input a **batch of 64x64x3 images**\n",
    "* the output should be a single value (=score)\n",
    "\n",
    "Feel free to get inspiration from the different architectures we talked about in the course, such as DCGAN, WGAN-GP or DRAGAN.\n",
    "\n",
    "#### Some tips\n",
    "* To scale down from the input image, you can either use `Conv2d` layers with the correct hyperparameters or Pooling layers.\n",
    "* If you plan on using gradient penalty, do not use Batch Normalization layers in the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd847c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ec957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Define the discriminator network\n",
    "        self.model = nn.Sequential(\n",
    "            # First convolution layer\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Third convolution layer\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Fourth convolution layer\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Final convolution layer\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # 4x4 -> 1x1\n",
    "\n",
    "            # Commented out because I am going to use the Wasserstein Distance and hence a Wasserstein-GAN\n",
    "            # nn.Sigmoid()  # Output a single value representing real or fake\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for the discriminator.\"\"\"\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa808c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# run this cell to check your discriminator implementation\n",
    "discriminator = Discriminator()\n",
    "tests.check_discriminator(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9551875",
   "metadata": {},
   "source": [
    "### Exercise: create the generator\n",
    "\n",
    "The generator's job creates the \"fake images\" and learns the dataset distribution. You have three constraints here:\n",
    "* the generator takes as input a vector of dimension `[batch_size, latent_dimension, 1, 1]`\n",
    "* the generator must outputs **64x64x3 images**\n",
    "\n",
    "Feel free to get inspiration from the different architectures we talked about in the course, such as DCGAN, WGAN-GP or DRAGAN.\n",
    "\n",
    "#### Some tips:\n",
    "* to scale up from the latent vector input, you can use `ConvTranspose2d` layers\n",
    "* as often with Gan, **Batch Normalization** helps with training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim: int):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Define the generator network\n",
    "        self.model = nn.Sequential(\n",
    "            # First layer: latent vector -> 4x4x512 feature maps\n",
    "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=1, padding=0),  # 1x1 -> 4x4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Second layer: 4x4x512 -> 8x8x256 feature maps\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Third layer: 8x8x256 -> 16x16x128 feature maps\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Fourth layer: 16x16x128 -> 32x32x64 feature maps\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Fifth layer: 32x32x64 -> 64x64x3 (RGB image)\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),  # 32x32 -> 64x64\n",
    "            nn.Tanh()  # Output range in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for the generator.\"\"\"\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1)  # Reshape the latent vector to start with 1x1 spatial dimensions\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d230959",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# run this cell to verify your generator implementation\n",
    "latent_dim = 128\n",
    "generator = Generator(latent_dim)\n",
    "tests.check_generator(generator, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443201f",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "\n",
    "In the following section, we create the optimizers for the generator and discriminator. You may want to experiment with different optimizers, learning rates and other hyperparameters as they tend to impact the output quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c420c8b0",
   "metadata": {},
   "source": [
    "### Exercise: implement the optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68af2f4-58ce-4360-86ef-cbc955e79efa",
   "metadata": {},
   "source": [
    "I've added two new parameters **lr** and **betas** for finetuning the optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60458540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import Module\n",
    "\n",
    "def create_optimizers(generator: Module, discriminator: Module, lr: float = 0.0002, betas=(0.5, 0.999)):\n",
    "    \"\"\"\n",
    "    This function returns the optimizers for the generator and the discriminator.\n",
    "    \n",
    "    Args:\n",
    "    - generator: The generator model.\n",
    "    - discriminator: The discriminator model.\n",
    "    - lr: Learning rate for the optimizers (default is 0.0002).\n",
    "    - betas: Betas for the Adam optimizer (default is (0.5, 0.999)).\n",
    "    \n",
    "    Returns:\n",
    "    - g_optimizer: Optimizer for the generator.\n",
    "    - d_optimizer: Optimizer for the discriminator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizer for the generator\n",
    "    g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=betas)\n",
    "    \n",
    "    # Optimizer for the discriminator\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=betas)\n",
    "    \n",
    "    return g_optimizer, d_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed73bd2",
   "metadata": {},
   "source": [
    "## Losses implementation\n",
    "\n",
    "In this section, we are going to implement the loss function for the generator and the discriminator. You can and should experiment with different loss function.\n",
    "\n",
    "Some tips:\n",
    "* You can choose the commonly used the binary cross entropy loss or select other losses we have discovered in the course, such as the Wasserstein distance.\n",
    "* You may want to implement a gradient penalty function as discussed in the course. It is not required and the code will work whether you implement it or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c589fbb1",
   "metadata": {},
   "source": [
    "### Exercise: implement the generator loss\n",
    "\n",
    "The generator's goal is to get the discriminator to think its generated images (= \"fake\" images) are real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067dc714-a80e-4e50-ad9b-ba3b55e67264",
   "metadata": {},
   "source": [
    "Because I am going to use the **Wasserstein** loss I must implement a function for the Lipschitz Criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits):\n",
    "    \"\"\"\n",
    "    Wasserstein Generator Loss.\n",
    "    \n",
    "    Args:\n",
    "    - fake_logits (torch.Tensor): The critic scores for the generated (fake) images.\n",
    "    \n",
    "    Returns:\n",
    "    - loss (torch.Tensor): The generator loss to be minimized.\n",
    "    \"\"\"\n",
    "    # In WGAN, we want to maximize the critic's output on fake images\n",
    "    # Equivalent to minimizing -fake_logits\n",
    "    loss = -torch.mean(fake_logits)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680485a",
   "metadata": {},
   "source": [
    "### Exercise: implement the discriminator loss\n",
    "\n",
    "We want the discriminator to give high scores to real images and low scores to fake ones and the discriminator loss should reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not needed, because of the Wasserstein distance (and the modified discriminator (no sigmoid) becomes a critic)\n",
    "# def discriminator_loss(real_logits, fake_logits):\n",
    "#    \"\"\" Discriminator loss, takes the fake and real logits as inputs. \"\"\"\n",
    "#    # TODO: implement the discriminator loss \n",
    "#    return loss\n",
    "\n",
    "def critic_loss(real_logits, fake_logits):\n",
    "    \"\"\"\n",
    "    Wasserstein Critic Loss.\n",
    "    \n",
    "    Args:\n",
    "    - real_logits (torch.Tensor): The critic scores for real images.\n",
    "    - fake_logits (torch.Tensor): The critic scores for generated (fake) images.\n",
    "    \n",
    "    Returns:\n",
    "    - loss (torch.Tensor): The critic loss to be minimized.\n",
    "    \"\"\"\n",
    "    # Wasserstein Loss for Critic: maximize E[critic(real)] - E[critic(fake)]\n",
    "    loss = -torch.mean(real_logits) + torch.mean(fake_logits)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d9c44",
   "metadata": {},
   "source": [
    "### Exercise (Optional): Implement the gradient Penalty\n",
    "\n",
    "In the course, we discussed the importance of gradient penalty in training certain types of Gans. Implementing this function is not required and depends on some of the design decision you made (discriminator architecture, loss functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f922e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "#    \"\"\" This function enforces \"\"\"\n",
    "#    gp = 0\n",
    "#    # TODO (Optional): implement the gradient penalty\n",
    "#    return gp\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "\n",
    "def gradient_penalty(critic, real_samples, fake_samples, device):\n",
    "    \"\"\"\n",
    "    Computes the gradient penalty for WGAN-GP.\n",
    "    \n",
    "    Args:\n",
    "    - critic (torch.nn.Module): The critic (discriminator) model.\n",
    "    - real_samples (torch.Tensor): Batch of real images.\n",
    "    - fake_samples (torch.Tensor): Batch of generated images.\n",
    "    - device (torch.device): The device to run computations on (e.g., 'cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    - gp (torch.Tensor): The gradient penalty value.\n",
    "    \"\"\"\n",
    "    # Step 1: Interpolate between real and fake samples\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)  # Random weight for interpolation\n",
    "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    interpolates = interpolates.to(device)\n",
    "\n",
    "    # Step 2: Get critic scores for the interpolated samples\n",
    "    interpolates.requires_grad_(True)  # Enable gradient calculation\n",
    "    critic_interpolates = critic(interpolates)\n",
    "\n",
    "    # Step 3: Compute gradients with respect to the interpolated samples\n",
    "    gradients = autograd.grad(\n",
    "        outputs=critic_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(critic_interpolates, device=device),  # Same shape as critic output\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]  # Gradient tensor\n",
    "\n",
    "    # Step 4: Compute gradient norm\n",
    "    gradients = gradients.view(batch_size, -1)  # Flatten the gradients\n",
    "    gradient_norm = gradients.norm(2, dim=1)  # Compute L2 norm per sample\n",
    "\n",
    "    # Step 5: Compute gradient penalty as (||gradient||_2 - 1)^2\n",
    "    gp = torch.mean((gradient_norm - 1) ** 2)\n",
    "\n",
    "    return gp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b8171",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator. You'll use your functions real_loss and fake_loss to help you calculate the discriminator losses.\n",
    "\n",
    "* You should train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and should have an opposing loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673d1d6",
   "metadata": {},
   "source": [
    "### Exercise: implement the generator step and the discriminator step functions\n",
    "\n",
    "Each function should do the following:\n",
    "* calculate the loss\n",
    "* backpropagate the gradient\n",
    "* perform one optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are building a Wasserstein GAN so a slightly modified signature for one generator step must be used.\n",
    "# def generator_step(batch_size: int, latent_dim: int) -> Dict:\n",
    "#    \"\"\" One training step of the generator. \"\"\"\n",
    "#    # TODO: implement the generator step (foward pass, loss calculation and backward pass)\n",
    "#    return {'loss': g_loss}\n",
    "\n",
    "import torch\n",
    "from typing import Dict\n",
    "\n",
    "def generator_step(generator: torch.nn.Module, critic: torch.nn.Module, g_optimizer: torch.optim.Optimizer, \n",
    "                   batch_size: int, latent_dim: int, device: torch.device) -> Dict:\n",
    "    \"\"\"\n",
    "    One training step for the generator in WGAN.\n",
    "    \n",
    "    Args:\n",
    "    - generator (torch.nn.Module): The generator model.\n",
    "    - critic (torch.nn.Module): The critic (discriminator) model.\n",
    "    - g_optimizer (torch.optim.Optimizer): Optimizer for the generator.\n",
    "    - batch_size (int): The batch size.\n",
    "    - latent_dim (int): The latent dimension size.\n",
    "    - device (torch.device): Device to run the models (e.g. 'cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    - Dict: A dictionary containing the generator loss.\n",
    "    \"\"\"\n",
    "    # Step 1: Sample random latent vectors\n",
    "    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "\n",
    "    # Step 2: Generate fake images\n",
    "    fake_images = generator(z)\n",
    "\n",
    "    # Step 3: Get the critic's output for the fake images\n",
    "    fake_logits = critic(fake_images)\n",
    "\n",
    "    # Step 4: Compute the generator loss (Wasserstein loss)\n",
    "    g_loss = generator_loss(fake_logits)\n",
    "\n",
    "    # Step 5: Backpropagation and optimization step for the generator\n",
    "    g_optimizer.zero_grad()  # Clear any accumulated gradients\n",
    "    g_loss.backward()  # Backpropagate the loss\n",
    "    g_optimizer.step()  # Update the generator's weights\n",
    "\n",
    "    # Return the generator loss for tracking purposes\n",
    "    return {'loss': g_loss.item()}\n",
    "\n",
    "\n",
    "# The same goes for the discriminator step, some modifications for the signature of the function first (additional parameters for the critic etc.)\n",
    "# def discriminator_step(batch_size: int, latent_dim: int, real_images: torch.Tensor) -> Dict:\n",
    "#    \"\"\" One training step of the discriminator. \"\"\"\n",
    "#    # TODO: implement the discriminator step (foward pass, loss calculation and backward pass)\n",
    "#    return {'loss': d_loss, 'gp': gp}\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "def discriminator_step(generator: torch.nn.Module, critic: torch.nn.Module, d_optimizer: torch.optim.Optimizer, \n",
    "                       batch_size: int, latent_dim: int, real_images: torch.Tensor, device: torch.device, \n",
    "                       lambda_gp: float = 10.0) -> Dict:\n",
    "    \"\"\"\n",
    "    One training step for the discriminator (critic) in WGAN-GP.\n",
    "    \n",
    "    Args:\n",
    "    - generator (torch.nn.Module): The generator model.\n",
    "    - critic (torch.nn.Module): The critic (discriminator) model.\n",
    "    - d_optimizer (torch.optim.Optimizer): Optimizer for the critic.\n",
    "    - batch_size (int): The batch size.\n",
    "    - latent_dim (int): The latent dimension size.\n",
    "    - real_images (torch.Tensor): Batch of real images.\n",
    "    - device (torch.device): Device to run the models (e.g. 'cuda' or 'cpu').\n",
    "    - lambda_gp (float): Weight of the gradient penalty term (default is 10.0).\n",
    "    \n",
    "    Returns:\n",
    "    - Dict: A dictionary containing the critic loss and gradient penalty.\n",
    "    \"\"\"\n",
    "    # Step 1: Sample random latent vectors\n",
    "    z = torch.randn(batch_size, latent_dim, device=device)\n",
    "\n",
    "    # Step 2: Generate fake images\n",
    "    fake_images = generator(z)\n",
    "\n",
    "    # Step 3: Get the critic's output for real and fake images\n",
    "    real_logits = critic(real_images)\n",
    "    fake_logits = critic(fake_images.detach())  # Detach so that gradients are not propagated to the generator\n",
    "\n",
    "    # Step 4: Compute the Wasserstein critic loss\n",
    "    d_loss = critic_loss(real_logits, fake_logits)\n",
    "\n",
    "    # Step 5: Compute the gradient penalty\n",
    "    gp = gradient_penalty(critic, real_samples=real_images, fake_samples=fake_images, device=device)\n",
    "\n",
    "    # Step 6: Combine critic loss with gradient penalty\n",
    "    total_d_loss = d_loss + lambda_gp * gp\n",
    "\n",
    "    # Step 7: Backpropagation and optimization step for the critic\n",
    "    d_optimizer.zero_grad()  # Clear any accumulated gradients\n",
    "    total_d_loss.backward()  # Backpropagate the loss\n",
    "    d_optimizer.step()  # Update the critic's weights\n",
    "\n",
    "    # Return the critic loss and gradient penalty for tracking purposes\n",
    "    return {'loss': total_d_loss.item(), 'gp': gp.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0ccbc",
   "metadata": {},
   "source": [
    "### Main training loop\n",
    "\n",
    "You don't have to implement anything here but you can experiment with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can experiment with different dimensions of latent spaces\n",
    "latent_dim = 128\n",
    "\n",
    "# update to cpu if you do not have access to a gpu\n",
    "device = 'cuda'\n",
    "\n",
    "# number of epochs to train your model\n",
    "n_epochs = 15\n",
    "\n",
    "# number of images in each batch\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399aa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print_every = 50\n",
    "\n",
    "# Create optimizers for the discriminator D and generator G\n",
    "generator = Generator(latent_dim).to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "g_optimizer, d_optimizer = create_optimizers(generator, discriminator)\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, \n",
    "                        num_workers=4, \n",
    "                        drop_last=True,\n",
    "                        pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015ed0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def display(fixed_latent_vector: torch.Tensor):\n",
    "    \"\"\" helper function to display images during training \"\"\"\n",
    "    fig = plt.figure(figsize=(14, 4))\n",
    "    plot_size = 16\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "        img = fixed_latent_vector[idx, ...].detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = denormalize(img)\n",
    "        ax.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212f75a",
   "metadata": {},
   "source": [
    "### Exercise: implement the training strategy\n",
    "\n",
    "You should experiment with different training strategies. For example:\n",
    "\n",
    "* train the generator more often than the discriminator. \n",
    "* added noise to the input image\n",
    "* use label smoothing\n",
    "\n",
    "Implement with your training strategy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ade28-0bab-4cb7-bdaf-5611967c47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "fixed_latent_vector = torch.randn(16, latent_dim, 1, 1).float().cuda()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_i, real_images in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)  # Move real images to the correct device (GPU/CPU)\n",
    "\n",
    "        ####################################\n",
    "        # Training strategy implementation\n",
    "        ####################################\n",
    "        \n",
    "        # Step 1: Train the critic (discriminator) with gradient penalty\n",
    "        d_loss = discriminator_step(generator, discriminator, d_optimizer, batch_size, latent_dim, real_images, device)\n",
    "        \n",
    "        # Step 2: Train the generator every 4 critic steps\n",
    "        if batch_i % 3 == 0: # changed that to experiment with the results\n",
    "            g_loss = generator_step(generator, discriminator, g_optimizer, batch_size, latent_dim, device)\n",
    "        \n",
    "        ####################################\n",
    "        \n",
    "        # Print and store the losses at intervals\n",
    "        if batch_i % print_every == 0:\n",
    "            # Append discriminator loss and generator loss\n",
    "            d = d_loss['loss']  # Critic loss\n",
    "            g = g_loss['loss']  # Generator loss\n",
    "            losses.append((d, g))\n",
    "            \n",
    "            # Print discriminator and generator loss\n",
    "            time = str(datetime.now()).split('.')[0]\n",
    "            print(f'{time} | Epoch [{epoch+1}/{n_epochs}] | Batch {batch_i}/{len(dataloader)} | d_loss: {d:.4f} | g_loss: {g:.4f}')\n",
    "    \n",
    "    # After every epoch, display generated images using the fixed latent vector\n",
    "    generator.eval()  # Set generator to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for faster evaluation\n",
    "        generated_images = generator(fixed_latent_vector)\n",
    "    \n",
    "    # Display generated images (replace 'display' with your preferred visualization method)\n",
    "    display(generated_images)\n",
    "    \n",
    "    generator.train()  # Set generator back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4643ce4",
   "metadata": {},
   "source": [
    "### Training losses\n",
    "\n",
    "Plot the training losses for the generator and discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3744a98",
   "metadata": {},
   "source": [
    "### Question: What do you notice about your generated samples and how might you improve this model?\n",
    "When you answer this question, consider the following factors:\n",
    "* The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result\n",
    "* Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223967c",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "The generated faces are increasingly shaped per epoch until it comes to an apparent standstill. The changes to the faces then become smaller and smaller. What is also noticeable is that the training often gets stuck in the noise. The only thing to do then is to stop and start again. This is due to the fact that GANs are generally difficult to train. There are some random variables such as the noise, the learning rates (of the ADAM optimizer) or that the discriminator or generator become too weak/strong, and the initialization of the weights, etc. All those factors make the training of GANs somewhat unstable. \n",
    "\n",
    "Another interesting fact is that due to the stochastic nature of a WGAN, the discriminator often becomes too strong, so that the generator no longer has any significant learning progress. Then the training has no progress again.\n",
    "\n",
    "Opportunities for **improvement**:\n",
    "\n",
    "- Statiblize noise vector with fixed seed (see code above)\n",
    "- Hyperparameter tuning\n",
    "- Modification of lambda for the gradient penalty\n",
    "- Change ratio to discriminator and generator training (3 Discriminator and 1 Generator seems to be a good choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8713e",
   "metadata": {},
   "source": [
    "### Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\".  \n",
    "\n",
    "Submit the notebook using the ***SUBMIT*** button in the bottom right corner of the Project Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3311bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
