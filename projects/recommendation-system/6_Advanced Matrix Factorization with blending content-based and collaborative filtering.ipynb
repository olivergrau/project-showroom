{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Recommendation System: Matrix Factorization with Blending\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Recommendation systems are a cornerstone of modern applications, driving personalized experiences in e-commerce, streaming services, and more. This notebook explores **Matrix Factorization with Blending**, a hybrid approach that combines **Collaborative Filtering (CF)** and **Content-Based Filtering (CBF)** to create a robust and flexible recommendation system.\n",
    "\n",
    "---\n",
    "\n",
    "## Concepts Behind This Approach\n",
    "\n",
    "### 1. **Collaborative Filtering (CF)**\n",
    "Collaborative filtering uses patterns in user-item interactions to make recommendations. It operates on the principle that users with similar preferences will like similar items. A popular method within CF is **Matrix Factorization**, where the user-item interaction matrix is decomposed into latent feature matrices:\n",
    "- **User Latent Matrix (U)**: Represents user preferences in a latent feature space.\n",
    "- **Item Latent Matrix (V)**: Represents item characteristics in the same space.\n",
    "\n",
    "By learning these latent representations, CF predicts unknown interactions. However, CF struggles with **cold-start problems**, where new users or items lack interaction data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Content-Based Filtering (CBF)**\n",
    "Content-based filtering leverages item metadata (e.g., genres, descriptions, tags) or user attributes to recommend items. It computes the similarity between users and items based on these features. While effective for cold-start items, it can lead to a lack of diversity and serendipity in recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Blending CF and CBF**\n",
    "The hybrid approach addresses the limitations of standalone methods:\n",
    "- **Matrix Factorization (CF)** excels at learning implicit patterns but struggles with sparse data.\n",
    "- **Content-Based Filtering (CBF)** provides strong cold-start support but lacks diversity.\n",
    "\n",
    "By blending these methods, we create a system that:\n",
    "1. Predicts user-item interactions using **latent factors** (CF).\n",
    "2. Incorporates **item content features** to improve accuracy and handle cold-start scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective of This Notebook\n",
    "\n",
    "1. **Prepare Data**:\n",
    "   - Construct the user-item interaction matrix.\n",
    "   - Extract item content features (e.g., genres, descriptions).\n",
    "\n",
    "2. **Implement Matrix Factorization**:\n",
    "   - Train a collaborative filtering model using user-item interactions.\n",
    "\n",
    "3. **Incorporate Content-Based Features**:\n",
    "   - Blend content-based predictions with collaborative filtering predictions.\n",
    "\n",
    "4. **Evaluate the System**:\n",
    "   - Assess the blended model's performance using metrics like RMSE and Precision@K.\n",
    "\n",
    "---\n",
    "\n",
    "By combining the strengths of collaborative and content-based filtering, this notebook aims to build a recommendation system that is both accurate and versatile, capable of handling sparse data and cold-start challenges effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 70312\n",
      "Validation data size: 15102\n",
      "Test data size: 15422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Data\n",
    "ratings = pd.read_csv('data/ml-latest-small/ratings.csv')  # User-item interactions\n",
    "movies = pd.read_csv('data/ml-latest-small/movies.csv')  # Movie metadata (title, genres)\n",
    "tags = pd.read_csv('data/ml-latest-small/tags.csv')  # User-defined tags for movies\n",
    "\n",
    "# Step 2: Preprocess and Join Data\n",
    "# Merge ratings with movies to include genres\n",
    "ratings = ratings.merge(movies, on='movieId', how='left')\n",
    "\n",
    "# Join tags with movies and group tags by movieId\n",
    "tags['tag'] = tags['tag'].str.lower()\n",
    "\n",
    "# Group tags by movieId and concatenate into a single string\n",
    "tags_grouped = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Merge tags into the main dataset\n",
    "ratings = ratings.merge(tags_grouped, on='movieId', how='left')\n",
    "\n",
    "# Fill missing tags with empty strings\n",
    "ratings['tag'] = ratings['tag'].fillna('')\n",
    "\n",
    "# Combine genres and tags into a single column for content features\n",
    "ratings['content'] = ratings['genres'] + ' ' + ratings['tag']\n",
    "\n",
    "# Step 3: Split Data into Training, Validation, and Test Sets\n",
    "# Sort by userId and timestamp to maintain temporal consistency\n",
    "ratings = ratings.sort_values(by=['userId', 'timestamp'])\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "\n",
    "# Function to split data for each user\n",
    "def split_user_data(group):\n",
    "    n = len(group)\n",
    "    train_end = int(n * train_ratio)\n",
    "    val_end = int(n * (train_ratio + val_ratio))\n",
    "    \n",
    "    train = group.iloc[:train_end]\n",
    "    val = group.iloc[train_end:val_end]\n",
    "    test = group.iloc[val_end:]\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Apply splitting logic\n",
    "train_data, val_data, test_data = [], [], []\n",
    "for _, group in ratings.groupby('userId'):\n",
    "    train, val, test = split_user_data(group)\n",
    "    train_data.append(train)\n",
    "    val_data.append(val)\n",
    "    test_data.append(test)\n",
    "\n",
    "# Concatenate results\n",
    "train_data = pd.concat(train_data)\n",
    "val_data = pd.concat(val_data)\n",
    "test_data = pd.concat(test_data)\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets validate the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train_data:\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "title        0\n",
      "genres       0\n",
      "tag          0\n",
      "content      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in val_data:\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "title        0\n",
      "genres       0\n",
      "tag          0\n",
      "content      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test_data:\n",
      "userId       0\n",
      "movieId      0\n",
      "rating       0\n",
      "timestamp    0\n",
      "title        0\n",
      "genres       0\n",
      "tag          0\n",
      "content      0\n",
      "dtype: int64\n",
      "\n",
      "Negative or zero ratings in train_data: 0\n",
      "Negative or zero ratings in val_data: 0\n",
      "Negative or zero ratings in test_data: 0\n",
      "\n",
      "Train data ranges:\n",
      "             userId        movieId        rating     timestamp\n",
      "count  70312.000000   70312.000000  70312.000000  7.031200e+04\n",
      "mean     326.205712   16105.237029      3.521625  1.195010e+09\n",
      "std      182.652155   31564.404509      1.036593  2.152768e+08\n",
      "min        1.000000       1.000000      0.500000  8.281246e+08\n",
      "25%      177.000000    1097.000000      3.000000  9.978129e+08\n",
      "50%      325.000000    2753.000000      3.500000  1.172012e+09\n",
      "75%      477.000000    6564.000000      4.000000  1.431968e+09\n",
      "max      610.000000  193609.000000      5.000000  1.537158e+09\n",
      "Validation data ranges:\n",
      "             userId        movieId        rating     timestamp\n",
      "count  15102.000000   15102.000000  15102.000000  1.510200e+04\n",
      "mean     326.128327   22597.441001      3.444180  1.220187e+09\n",
      "std      182.596161   37341.776954      1.043328  2.127727e+08\n",
      "min        1.000000       1.000000      0.500000  8.281246e+08\n",
      "25%      177.000000    1269.000000      3.000000  1.053071e+09\n",
      "50%      325.000000    3507.000000      3.500000  1.201196e+09\n",
      "75%      477.000000   32587.000000      4.000000  1.436000e+09\n",
      "max      610.000000  193581.000000      5.000000  1.537158e+09\n",
      "Test data ranges:\n",
      "             userId        movieId        rating     timestamp\n",
      "count  15422.000000   15422.000000  15422.000000  1.542200e+04\n",
      "mean     325.770523   31521.170989      3.466250  1.241862e+09\n",
      "std      182.498233   46264.549864      1.065527  2.194151e+08\n",
      "min        1.000000       1.000000      0.500000  8.281246e+08\n",
      "25%      177.000000    1407.000000      3.000000  1.056908e+09\n",
      "50%      325.000000    4296.000000      3.500000  1.242147e+09\n",
      "75%      477.000000   54764.500000      4.000000  1.461563e+09\n",
      "max      610.000000  193587.000000      5.000000  1.537799e+09\n",
      "\n",
      "Total rows in original data: 100836\n",
      "Total rows in splits: 100836\n",
      "Splits are consistent with the original data.\n",
      "\n",
      "Sample rows from train_data:\n",
      "     userId  movieId  rating  timestamp  \\\n",
      "43        1      804     4.0  964980499   \n",
      "73        1     1210     5.0  964980499   \n",
      "120       1     2018     5.0  964980523   \n",
      "171       1     2628     4.0  964980523   \n",
      "183       1     2826     4.0  964980523   \n",
      "\n",
      "                                                 title  \\\n",
      "43                                She's the One (1996)   \n",
      "73   Star Wars: Episode VI - Return of the Jedi (1983)   \n",
      "120                                       Bambi (1942)   \n",
      "171   Star Wars: Episode I - The Phantom Menace (1999)   \n",
      "183                           13th Warrior, The (1999)   \n",
      "\n",
      "                       genres                                     tag  \\\n",
      "43             Comedy|Romance                                           \n",
      "73    Action|Adventure|Sci-Fi  darth vader luke skywalker space opera   \n",
      "120  Animation|Children|Drama                                           \n",
      "171   Action|Adventure|Sci-Fi                       prequel the force   \n",
      "183  Action|Adventure|Fantasy                                           \n",
      "\n",
      "                                               content  \n",
      "43                                     Comedy|Romance   \n",
      "73   Action|Adventure|Sci-Fi darth vader luke skywa...  \n",
      "120                          Animation|Children|Drama   \n",
      "171          Action|Adventure|Sci-Fi prequel the force  \n",
      "183                          Action|Adventure|Fantasy   \n",
      "\n",
      "Sample rows from val_data:\n",
      "     userId  movieId  rating  timestamp                           title  \\\n",
      "54        1     1030     3.0  964982903            Pete's Dragon (1977)   \n",
      "122       1     2033     5.0  964982903      Black Cauldron, The (1985)   \n",
      "230       1     4006     4.0  964982903  Transformers: The Movie (1986)   \n",
      "4         1       50     5.0  964982931      Usual Suspects, The (1995)   \n",
      "36        1      608     5.0  964982931                    Fargo (1996)   \n",
      "\n",
      "                                   genres  \\\n",
      "54   Adventure|Animation|Children|Musical   \n",
      "122  Adventure|Animation|Children|Fantasy   \n",
      "230   Adventure|Animation|Children|Sci-Fi   \n",
      "4                  Crime|Mystery|Thriller   \n",
      "36            Comedy|Crime|Drama|Thriller   \n",
      "\n",
      "                                                   tag  \\\n",
      "54                                              disney   \n",
      "122                                                      \n",
      "230                                                      \n",
      "4    mindfuck suspense thriller tricky twist ending...   \n",
      "36   based on a true story dark comedy kidnapping s...   \n",
      "\n",
      "                                               content  \n",
      "54         Adventure|Animation|Children|Musical disney  \n",
      "122              Adventure|Animation|Children|Fantasy   \n",
      "230               Adventure|Animation|Children|Sci-Fi   \n",
      "4    Crime|Mystery|Thriller mindfuck suspense thril...  \n",
      "36   Comedy|Crime|Drama|Thriller based on a true st...  \n",
      "\n",
      "Sample rows from test_data:\n",
      "     userId  movieId  rating  timestamp  \\\n",
      "210       1     3386     5.0  964983484   \n",
      "105       1     1625     5.0  964983504   \n",
      "205       1     3176     1.0  964983504   \n",
      "106       1     1644     3.0  964983536   \n",
      "209       1     3273     5.0  964983536   \n",
      "\n",
      "                                      title                          genres  \\\n",
      "210                              JFK (1991)          Drama|Mystery|Thriller   \n",
      "105                        Game, The (1997)          Drama|Mystery|Thriller   \n",
      "205         Talented Mr. Ripley, The (1999)          Drama|Mystery|Thriller   \n",
      "106  I Know What You Did Last Summer (1997)         Horror|Mystery|Thriller   \n",
      "209                         Scream 3 (2000)  Comedy|Horror|Mystery|Thriller   \n",
      "\n",
      "                                                   tag  \\\n",
      "210                                 politics president   \n",
      "105  mindfuck mystery plot twist psychological susp...   \n",
      "205  based on a book creepy disturbing jude law mur...   \n",
      "106                                                      \n",
      "209                                      slasher spoof   \n",
      "\n",
      "                                               content  \n",
      "210          Drama|Mystery|Thriller politics president  \n",
      "105  Drama|Mystery|Thriller mindfuck mystery plot t...  \n",
      "205  Drama|Mystery|Thriller based on a book creepy ...  \n",
      "106                           Horror|Mystery|Thriller   \n",
      "209       Comedy|Horror|Mystery|Thriller slasher spoof  \n"
     ]
    }
   ],
   "source": [
    "# check for missing values\n",
    "print(\"Missing values in train_data:\")\n",
    "print(train_data.isnull().sum())\n",
    "print()\n",
    "print(\"Missing values in val_data:\")\n",
    "print(val_data.isnull().sum())\n",
    "print()\n",
    "print(\"Missing values in test_data:\")\n",
    "print(test_data.isnull().sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# check for negative or zero ratings\n",
    "print(\"Negative or zero ratings in train_data:\", (train_data['rating'] <= 0).sum())\n",
    "print(\"Negative or zero ratings in val_data:\", (val_data['rating'] <= 0).sum())\n",
    "print(\"Negative or zero ratings in test_data:\", (test_data['rating'] <= 0).sum())\n",
    "\n",
    "print()\n",
    "\n",
    "# check for rating ranges\n",
    "print(\"Train data ranges:\")\n",
    "print(train_data.describe())\n",
    "print(\"Validation data ranges:\")\n",
    "print(val_data.describe())\n",
    "print(\"Test data ranges:\")\n",
    "print(test_data.describe())\n",
    "\n",
    "print()\n",
    "\n",
    "total_rows = len(ratings)\n",
    "split_rows = len(train_data) + len(val_data) + len(test_data)\n",
    "\n",
    "print(f\"Total rows in original data: {total_rows}\")\n",
    "print(f\"Total rows in splits: {split_rows}\")\n",
    "\n",
    "if total_rows == split_rows:\n",
    "    print(\"Splits are consistent with the original data.\")\n",
    "else:\n",
    "    print(\"Inconsistency in splits!\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Sample rows from train_data:\")\n",
    "print(train_data.head())\n",
    "print()\n",
    "print(\"Sample rows from val_data:\")\n",
    "print(val_data.head())\n",
    "print()\n",
    "print(\"Sample rows from test_data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanations:\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It is widely used in information retrieval and text mining to represent textual data in a way that highlights relevant terms while reducing the impact of common but less informative words.\n",
    "\n",
    "---\n",
    "\n",
    "### Components of TF-IDF\n",
    "\n",
    "1. **Term Frequency (TF)**:\n",
    "   Measures how often a term appears in a document.\n",
    "   $$\n",
    "   TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "   $$\n",
    "\n",
    "2. **Inverse Document Frequency (IDF)**:\n",
    "   Reduces the weight of terms that appear in many documents (common words).\n",
    "   $$\n",
    "   IDF(t, D) = \\log\\left(\\frac{\\text{Total number of documents in the corpus } D}{\\text{Number of documents containing term } t}\\right)\n",
    "   $$\n",
    "\n",
    "3. **TF-IDF Score**:\n",
    "   Combines TF and IDF to calculate the importance of a term in a document:\n",
    "   $$\n",
    "   TF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Why Use TF-IDF?\n",
    "\n",
    "- **Highlights Important Words**:\n",
    "  Words that are frequent in a document but rare in the corpus receive higher scores.\n",
    "- **Ignores Common Words**:\n",
    "  Words like \"the,\" \"is,\" or \"and\" have low scores due to high document frequency.\n",
    "- **Sparse Representation**:\n",
    "  TF-IDF produces sparse vectors, ideal for efficient storage and computation in machine learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### Application in Recommendation Systems\n",
    "\n",
    "In recommendation systems, TF-IDF is used to extract features from item metadata (e.g., genres, tags, descriptions). These features can then be integrated with collaborative filtering methods to enhance recommendations by incorporating content-based insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create sparse matrices (scipy) so that we can use efficient data structures for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a Simplified TF-IDF Matrix\n",
    "\n",
    "#### Example Scenario:\n",
    "We have a small dataset of 3 movies with genres and tags:\n",
    "\n",
    "| **Movie ID** | **Genres**         | **Tags**           |\n",
    "|--------------|--------------------|--------------------|\n",
    "| 1            | Action Adventure  | epic, battle       |\n",
    "| 2            | Comedy Drama       | funny, heartwarming|\n",
    "| 3            | Action Sci-Fi      | space, futuristic  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Combined Content (Genres + Tags):\n",
    "We combine genres and tags into a single string for each movie (as in your `ratings['content']` column):\n",
    "1. Movie 1: `Action Adventure epic battle`\n",
    "2. Movie 2: `Comedy Drama funny heartwarming`\n",
    "3. Movie 3: `Action Sci-Fi space futuristic`\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Vocabulary Creation\n",
    "The **vocabulary** consists of all unique words across the combined content:\n",
    "$$\n",
    "\\text{Vocabulary} = \\{\\text{Action, Adventure, epic, battle, Comedy, Drama, funny, heartwarming, Sci-Fi, space, futuristic}\\}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Document-Term Matrix\n",
    "Create a matrix where each row represents a movie, and each column corresponds to a word from the vocabulary. The value is the **Term Frequency (TF)** for that word in the movie's content:\n",
    "\n",
    "| Movie ID | Action | Adventure | epic | battle | Comedy | Drama | funny | heartwarming | Sci-Fi | space | futuristic |\n",
    "|----------|--------|-----------|------|--------|--------|-------|-------|--------------|--------|-------|------------|\n",
    "| 1        | 1      | 1         | 1    | 1      | 0      | 0     | 0     | 0            | 0      | 0     | 0          |\n",
    "| 2        | 0      | 0         | 0    | 0      | 1      | 1     | 1     | 1            | 0      | 0     | 0          |\n",
    "| 3        | 1      | 0         | 0    | 0      | 0      | 0     | 0     | 0            | 1      | 1     | 1          |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Compute Inverse Document Frequency (IDF)\n",
    "The **IDF** for each word is computed as:\n",
    "$$\n",
    "\\text{IDF}(t) = \\log \\left( \\frac{N}{1 + \\text{DF}(t)} \\right)\n",
    "$$\n",
    "Where:\n",
    "- $ N = 3 $: Total number of documents (movies).\n",
    "- $ \\text{DF}(t) $: Number of documents containing the term $ t $.\n",
    "\n",
    "| Term         | DF  | IDF                  |\n",
    "|--------------|-----|----------------------|\n",
    "| Action       | 2   | $ \\log\\left(\\frac{3}{1+2}\\right) = 0.0 $ |\n",
    "| Adventure    | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| epic         | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| battle       | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| Comedy       | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| Drama        | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| funny        | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| heartwarming | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| Sci-Fi       | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| space        | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "| futuristic   | 1   | $ \\log\\left(\\frac{3}{1+1}\\right) = 0.405 $ |\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Compute TF-IDF Matrix\n",
    "Each value is computed as:\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\cdot \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "| Movie ID | Action | Adventure | epic | battle | Comedy | Drama | funny | heartwarming | Sci-Fi | space | futuristic |\n",
    "|----------|--------|-----------|------|--------|--------|-------|-------|--------------|--------|-------|------------|\n",
    "| 1        | 0.0    | 0.405     | 0.405| 0.405  | 0.0    | 0.0   | 0.0   | 0.0          | 0.0    | 0.0   | 0.0        |\n",
    "| 2        | 0.0    | 0.0       | 0.0  | 0.0    | 0.405  | 0.405 | 0.405 | 0.405        | 0.0    | 0.0   | 0.0        |\n",
    "| 3        | 0.0    | 0.0       | 0.0  | 0.0    | 0.0    | 0.0   | 0.0   | 0.0          | 0.405  | 0.405 | 0.405      |\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Notes:\n",
    "1. **Sparse Representation**: TF-IDF matrices are usually sparse, as most words don't appear in every document.\n",
    "2. **Use in Recommendations**: This matrix can be used to compute the similarity between movies or blended with collaborative filtering for hybrid recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of idx mappings: (610, 9724)\n",
      "Sparse train matrix shape: (610, 9724)\n",
      "Sparse validation matrix shape: (610, 9724)\n",
      "Sparse test matrix shape: (610, 9724)\n",
      "\n",
      "Content matrix shape (TF-IDF): (100836, 1675)\n",
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Step 4: Create Sparse Interaction Matrices\n",
    "def create_sparse_matrix(data, user_mapping, movie_mapping, rating_col='rating'):\n",
    "    row = data['userId'].map(user_mapping).values\n",
    "    col = data['movieId'].map(movie_mapping).values\n",
    "    values = data[rating_col].values\n",
    "    return csr_matrix((values, (row, col)), shape=(len(user_mapping), len(movie_mapping)))\n",
    "\n",
    "# Create mappings for userId and movieId to indices\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(ratings['userId'].unique())}\n",
    "movie_mapping = {movie_id: idx for idx, movie_id in enumerate(ratings['movieId'].unique())}\n",
    "\n",
    "print(f\"Shape of idx mappings: ({len(user_mapping)}, {len(movie_mapping)})\")\n",
    "\n",
    "train_sparse = create_sparse_matrix(train_data, user_mapping, movie_mapping)\n",
    "val_sparse = create_sparse_matrix(val_data, user_mapping, movie_mapping)\n",
    "test_sparse = create_sparse_matrix(test_data, user_mapping, movie_mapping)\n",
    "\n",
    "print(f\"Sparse train matrix shape: {train_sparse.shape}\")\n",
    "print(f\"Sparse validation matrix shape: {val_sparse.shape}\")\n",
    "print(f\"Sparse test matrix shape: {test_sparse.shape}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Step 6: Extract Content-Based Features as Sparse Matrix\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "content_matrix = tfidf_vectorizer.fit_transform(ratings['content'])\n",
    "\n",
    "print(f\"Content matrix shape (TF-IDF): {content_matrix.shape}\")\n",
    "\n",
    "# Final Outputs\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data for correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing users in train_data: 0\n",
      "Missing users in val_data: 0\n",
      "Missing users in test_data: 0\n",
      "Missing movies in train_data: 0\n",
      "Missing movies in val_data: 0\n",
      "Missing movies in test_data: 0\n",
      "\n",
      "Train sparse shape: (610, 9724)\n",
      "Validation sparse shape: (610, 9724)\n",
      "Test sparse shape: (610, 9724)\n",
      "Expected shape: (610, 9724)\n",
      "\n",
      "Non-zero train row indices: [0 0 0 0 0 0 0 0 0 0]\n",
      "Non-zero validation row indices: [0 0 0 0 0 0 0 0 0 0]\n",
      "Non-zero test row indices: [0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Mean rating per user (train): [4.38888889 3.85       3.14814815 3.70860927 3.63333333 3.57990868\n",
      " 3.47169811 3.625      3.59375    3.40816327]\n"
     ]
    }
   ],
   "source": [
    "# Ensure all user IDs in train, val, and test data exist in user_mapping\n",
    "missing_users_train = train_data[~train_data['userId'].isin(user_mapping.keys())]\n",
    "missing_users_val = val_data[~val_data['userId'].isin(user_mapping.keys())]\n",
    "missing_users_test = test_data[~test_data['userId'].isin(user_mapping.keys())]\n",
    "\n",
    "print(\"Missing users in train_data:\", len(missing_users_train))\n",
    "print(\"Missing users in val_data:\", len(missing_users_val))\n",
    "print(\"Missing users in test_data:\", len(missing_users_test))\n",
    "\n",
    "# Ensure all movie IDs in train, val, and test data exist in movie_mapping\n",
    "missing_movies_train = train_data[~train_data['movieId'].isin(movie_mapping.keys())]\n",
    "missing_movies_val = val_data[~val_data['movieId'].isin(movie_mapping.keys())]\n",
    "missing_movies_test = test_data[~test_data['movieId'].isin(movie_mapping.keys())]\n",
    "\n",
    "print(\"Missing movies in train_data:\", len(missing_movies_train))\n",
    "print(\"Missing movies in val_data:\", len(missing_movies_val))\n",
    "print(\"Missing movies in test_data:\", len(missing_movies_test))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Train sparse shape:\", train_sparse.shape)\n",
    "print(\"Validation sparse shape:\", val_sparse.shape)\n",
    "print(\"Test sparse shape:\", test_sparse.shape)\n",
    "print(f\"Expected shape: ({len(user_mapping)}, {len(movie_mapping)})\")\n",
    "\n",
    "print()\n",
    "\n",
    "train_rows, train_cols = train_sparse.nonzero()\n",
    "val_rows, val_cols = val_sparse.nonzero()\n",
    "test_rows, test_cols = test_sparse.nonzero()\n",
    "\n",
    "print(\"Non-zero train row indices:\", train_rows[:10])\n",
    "print(\"Non-zero validation row indices:\", val_rows[:10])\n",
    "print(\"Non-zero test row indices:\", test_rows[:10])\n",
    "\n",
    "print()\n",
    "\n",
    "# Recompute row means to validate normalization\n",
    "train_sums = np.array(train_sparse.sum(axis=1)).flatten()\n",
    "train_counts = np.diff(train_sparse.indptr)\n",
    "train_means = np.divide(train_sums, train_counts, where=train_counts != 0)\n",
    "\n",
    "print(\"Mean rating per user (train):\", train_means[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implementing Matrix Factorization\n",
    "\n",
    "## Outline for Implementing Matrix Factorization\n",
    "\n",
    "1. **Understand the Matrix Factorization Objective**  \n",
    "   - Decompose the user-item interaction matrix (e.g., ratings) into two lower-dimensional matrices:\n",
    "     - **User matrix (P)**: Represents user preferences in a latent feature space.\n",
    "     - **Item matrix (Q)**: Represents item attributes in the same latent feature space.  \n",
    "   - The product of these two matrices approximates the original user-item interaction matrix.\n",
    "\n",
    "2. **Set Up the Mathematical Framework**  \n",
    "   - Define the reconstruction loss function to minimize:\n",
    "     - Mean Squared Error (MSE) or similar loss.\n",
    "     - Regularization terms to prevent overfitting.\n",
    "   - Represent the optimization as:  \n",
    "     $$\n",
    "     \\min_{P, Q} \\sum_{(u, i) \\in \\text{observed}} (R_{ui} - P_u^T Q_i)^2 + \\lambda (||P||^2 + ||Q||^2)\n",
    "     $$  \n",
    "     where:\n",
    "     - $ R_{ui} $ is the actual rating or interaction.\n",
    "     - $ P_u $ and $ Q_i $ are the latent feature vectors for the user $ u $ and item $ i $.\n",
    "     - $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "3. **Incorporating User and Item Biases**  \n",
    "   - **What are User Biases?**  \n",
    "     User biases ($ b_u $) account for individual user tendencies to rate items higher or lower than average. For example, some users consistently give higher ratings regardless of the item.  \n",
    "   - **Effect of User Bias**:  \n",
    "     - Captures individual user behavior and improves prediction accuracy by correcting for systematic rating tendencies.  \n",
    "     - A user who tends to rate all movies lower can be modeled with a negative bias, while a generous rater would have a positive bias.  \n",
    "   - **Mathematical Representation**:  \n",
    "     To include biases, the prediction formula becomes:  \n",
    "     $$\n",
    "     \\hat{R}_{ui} = \\mu + b_u + b_i + P_u^T Q_i\n",
    "     $$  \n",
    "     where:\n",
    "     - $ \\mu $ is the global average rating.\n",
    "     - $ b_u $ is the user bias for user $ u $.\n",
    "     - $ b_i $ is the item bias for item $ i $.\n",
    "     - $ P_u^T Q_i $ is the dot product of user and item latent vectors.  \n",
    "   - **Effect on the Loss Function**:  \n",
    "     The loss function is updated to include the biases:  \n",
    "     $$\n",
    "     \\min_{P, Q, b_u, b_i} \\sum_{(u, i) \\in \\text{observed}} \\left( R_{ui} - (\\mu + b_u + b_i + P_u^T Q_i) \\right)^2 + \\lambda \\left( ||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2 \\right)\n",
    "     $$  \n",
    "     Biases are updated alongside the latent matrices during optimization.\n",
    "\n",
    "4. **Initialize Latent Matrices (P and Q)**  \n",
    "   - Randomly initialize the user and item latent feature matrices with small values.\n",
    "   - Define the number of latent features (dimensionality).\n",
    "   - Initialize biases ($ b_u $ and $ b_i $) to zero and calculate the global bias ($ \\mu $) as the mean of observed ratings.\n",
    "\n",
    "5. **Implement the Optimization Algorithm**  \n",
    "   - Use **Stochastic Gradient Descent (SGD)** or a similar optimization method:\n",
    "     - Loop through observed user-item pairs.\n",
    "     - Calculate prediction error.\n",
    "     - Update latent factors $ P $ and $ Q $ based on the gradient of the loss function.\n",
    "     - Update user and item biases $ b_u $ and $ b_i $.\n",
    "   - Optionally, implement batch or mini-batch optimization.\n",
    "\n",
    "6. **Regularization**  \n",
    "   - Include regularization terms in the gradient updates.\n",
    "   - Prevents overfitting by penalizing large weights in the matrices and biases.\n",
    "\n",
    "7. **Implement the Training Loop**  \n",
    "   - Define stopping criteria:\n",
    "     - Maximum number of iterations/epochs.\n",
    "     - Convergence threshold based on loss improvement.\n",
    "   - Monitor training loss and validation loss.\n",
    "\n",
    "8. **Evaluate the Model**  \n",
    "   - Compute reconstruction accuracy on a validation set (e.g., Root Mean Squared Error).\n",
    "   - Compare predicted values with actual values.\n",
    "\n",
    "9. **Optional Enhancements**  \n",
    "   - Experiment with different numbers of latent features.\n",
    "   - Add content-based features to the factorization model for better performance.\n",
    "   - Evaluate ranking metrics like Precision@K and Recall@K for top-\\(K\\) recommendations.\n",
    "\n",
    "10. **Save and Reuse the Model**  \n",
    "    - Save the learned matrices $ P $ and $ Q $, user biases $ b_u $, item biases $ b_i $, and global bias $ \\mu $ for future use.\n",
    "    - Provide functions to predict ratings for any user-item pair.\n",
    "\n",
    "11. **Test the Implementation**  \n",
    "    - Use synthetic or small real datasets to test and debug the implementation.\n",
    "    - Ensure the model trains without exploding/vanishing gradients.\n",
    "\n",
    "---\n",
    "\n",
    "This framework ensures a structured approach to implementing matrix factorization for recommendation systems, with a focus on incorporating **user and item biases** for improved prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize latent matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_latent_matrices(num_users, num_items, latent_dim):\n",
    "    \"\"\"\n",
    "    Initialize the user and item latent matrices with small random values.\n",
    "    \n",
    "    Parameters:\n",
    "        num_users (int): Number of users.\n",
    "        num_items (int): Number of items.\n",
    "        latent_dim (int): Number of latent dimensions (features).\n",
    "        \n",
    "    Returns:\n",
    "        P (numpy.ndarray): User latent matrix of shape (num_users, latent_dim).\n",
    "        Q (numpy.ndarray): Item latent matrix of shape (num_items, latent_dim).\n",
    "    \"\"\"\n",
    "    # Random initialization of user and item matrices\n",
    "    P = np.random.normal(scale=0.01, size=(num_users, latent_dim))\n",
    "    Q = np.random.normal(scale=0.01, size=(num_items, latent_dim))\n",
    "    \n",
    "    return P, Q\n",
    "\n",
    "def compute_sparse_loss(rows, cols, values, P, Q, b_u, b_i, mu, regularization, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the loss for a given sparse interaction matrix, including bias terms.\n",
    "\n",
    "    Parameters:\n",
    "        rows (np.ndarray): Row indices of the non-zero entries.\n",
    "        cols (np.ndarray): Column indices of the non-zero entries.\n",
    "        values (np.ndarray): Corresponding non-zero values.\n",
    "        P (np.ndarray): User latent matrix.\n",
    "        Q (np.ndarray): Item latent matrix (transposed).\n",
    "        b_u (np.ndarray): User biases.\n",
    "        b_i (np.ndarray): Item biases.\n",
    "        mu (float): Global bias.\n",
    "        regularization (float): Regularization parameter.\n",
    "        normalize (bool): Whether to compute normalized loss (MSE).\n",
    "\n",
    "    Returns:\n",
    "        float: Computed loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(rows) == len(cols) == len(values), f\"Invalid sizes: {len(rows)}, {len(cols)}, {len(values)}\"\n",
    "\n",
    "    loss = 0\n",
    "    for idx in range(len(values)):\n",
    "        u = rows[idx]\n",
    "        i = cols[idx]\n",
    "\n",
    "        assert 0 <= u < len(P), f\"Invalid user index: {u}\"\n",
    "        assert 0 <= i < len(Q.T), f\"Invalid item index: {i}\"\n",
    "\n",
    "        rating = values[idx]\n",
    "\n",
    "        # Compute prediction including biases\n",
    "        prediction = mu + b_u[u] + b_i[i] + np.dot(P[u, :], Q[:, i])\n",
    "        error = rating - prediction\n",
    "        loss += error**2\n",
    "\n",
    "    # Add regularization term (includes biases)\n",
    "    loss += regularization * (\n",
    "        np.linalg.norm(P)**2 + np.linalg.norm(Q)**2 + np.linalg.norm(b_u)**2 + np.linalg.norm(b_i)**2\n",
    "    )\n",
    "\n",
    "    # Normalize the loss if requested\n",
    "    if normalize:\n",
    "        loss /= len(values)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matrix_factorization(train_sparse, val_sparse, num_users, num_items, latent_dim, \n",
    "                                epochs, learning_rate, regularization, patience=5, clip_value=5.0):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using SGD with regularization, early stopping, and gradient clipping.\n",
    "    Utilizes sparse matrices for efficient computation.\n",
    "\n",
    "    Parameters:\n",
    "        train_sparse (csr_matrix): Training interaction matrix (sparse).\n",
    "        val_sparse (csr_matrix): Validation interaction matrix (sparse).\n",
    "        num_users (int): Total number of users.\n",
    "        num_items (int): Total number of items.\n",
    "        latent_dim (int): Number of latent features.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for gradient descent.\n",
    "        regularization (float): Regularization parameter.\n",
    "        patience (int): Early stopping patience.\n",
    "        clip_value (float): Maximum value for gradient clipping.\n",
    "\n",
    "    Returns:\n",
    "        P (np.ndarray): Learned user latent matrix.\n",
    "        Q (np.ndarray): Learned item latent matrix.\n",
    "        train_losses (list): Training losses over epochs.\n",
    "        val_losses (list): Validation losses over epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize latent matrices\n",
    "    P, Q = initialize_latent_matrices(num_users, num_items, latent_dim)\n",
    "\n",
    "    # Convert item latent matrix for matrix operations\n",
    "    Q = Q.T\n",
    "\n",
    "    # Initialize biases\n",
    "    mu = train_sparse.data.mean()  # Global bias\n",
    "    b_u = np.zeros(num_users)  # User biases\n",
    "    b_i = np.zeros(num_items)  # Item biases\n",
    "\n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Get non-zero training indices for sparse matrix\n",
    "    train_rows, train_cols = train_sparse.nonzero()\n",
    "    train_values = train_sparse.data\n",
    "\n",
    "    assert len(train_rows) == len(train_cols) == len(train_values), f\"Invalid sizes: {len(train_rows)}, {len(train_cols)}, {len(train_values)}\"\n",
    "    \n",
    "    # Get non-zero validation indices for sparse matrix\n",
    "    val_rows, val_cols = val_sparse.nonzero()\n",
    "    val_values = val_sparse.data\n",
    "\n",
    "    assert len(val_rows) == len(val_cols) == len(val_values), f\"Invalid sizes: {len(val_rows)}, {len(val_cols)}, {len(val_values)}\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training indices\n",
    "        shuffle_indices = np.random.permutation(len(train_rows))\n",
    "        train_rows = train_rows[shuffle_indices]\n",
    "        train_cols = train_cols[shuffle_indices]\n",
    "        train_values = train_values[shuffle_indices]\n",
    "\n",
    "        # SGD for each non-zero entry in the sparse matrix\n",
    "        for idx in range(len(train_rows)):\n",
    "            u = train_rows[idx]\n",
    "            i = train_cols[idx]\n",
    "\n",
    "            assert 0 <= u < len(P), f\"Invalid user index: {u} at index {idx}\"\n",
    "            assert 0 <= i < len(Q.T), f\"Invalid item index: {i} at index {idx}\"\n",
    "\n",
    "            rating = train_values[idx]\n",
    "\n",
    "            # Compute prediction and error\n",
    "            prediction = mu + b_u[u] + b_i[i] + np.dot(P[u, :], Q[:, i])\n",
    "            error = rating - prediction\n",
    "\n",
    "            # Update biases with gradient clipping\n",
    "            delta_b_u = learning_rate * (error - regularization * b_u[u])\n",
    "            delta_b_i = learning_rate * (error - regularization * b_i[i])\n",
    "            b_u[u] += np.clip(delta_b_u, -clip_value, clip_value)\n",
    "            b_i[i] += np.clip(delta_b_i, -clip_value, clip_value)\n",
    "\n",
    "            # Update user and item latent vectors with gradient clipping\n",
    "            delta_P = learning_rate * (error * Q[:, i] - regularization * P[u, :])\n",
    "            delta_Q = learning_rate * (error * P[u, :] - regularization * Q[:, i])\n",
    "            P[u, :] += np.clip(delta_P, -clip_value, clip_value)\n",
    "            Q[:, i] += np.clip(delta_Q, -clip_value, clip_value)\n",
    "\n",
    "        # Compute training and validation losses\n",
    "        train_loss = compute_sparse_loss(train_rows, train_cols, train_values, P, Q, b_u, b_i, mu, regularization)\n",
    "        val_loss = compute_sparse_loss(val_rows, val_cols, val_values, P, Q, b_u, b_i, mu, regularization)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train MSE: {train_loss:.4f}, Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return P, Q.T, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Calculation for One Epoch (Without Content Blending)\n",
    "\n",
    "To calculate the total loss for one epoch, we sum the **squared errors** for all observed user-item interactions and add a **regularization term** to prevent overfitting.\n",
    "\n",
    "#### Total Loss Formula\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{(u, i) \\in \\mathcal{D}} \\left( R_{ui} - \\hat{R}_{ui} \\right)^2 + \\lambda \\cdot \\left( ||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2 \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "1. $ \\mathcal{D} $: Set of all observed user-item interactions in the training data.\n",
    "2. $ R_{ui} $: Actual rating for user $ u $ and item $ i $.\n",
    "3. $ \\hat{R}_{ui} $: Predicted rating for user $ u $ and item $ i $, calculated as:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = \\mu + b_u[u] + b_i[i] + P[u, :] \\cdot Q[i, :]^T\n",
    "   $$\n",
    "   - $ \\mu $: Global bias (average rating across all interactions).\n",
    "   - $ b_u[u] $: User-specific bias.\n",
    "   - $ b_i[i] $: Item-specific bias.\n",
    "   - $ P[u, :] $: Latent vector for user $ u $.\n",
    "   - $ Q[i, :] $: Latent vector for item $ i $.\n",
    "\n",
    "4. $ \\lambda $: Regularization parameter (controls the strength of the regularization).\n",
    "5. $ ||P||^2 $: Frobenius norm of the user latent matrix $ P $.\n",
    "6. $ ||Q||^2 $: Frobenius norm of the item latent matrix $ Q $.\n",
    "7. $ ||b_u||^2 $: Squared norm of the user biases.\n",
    "8. $ ||b_i||^2 $: Squared norm of the item biases.\n",
    "\n",
    "---\n",
    "\n",
    "#### Explanation\n",
    "\n",
    "1. **Error Term**:\n",
    "   - For each observed interaction $ (u, i) $, compute the squared difference between the actual rating ($ R_{ui} $) and the predicted rating ($ \\hat{R}_{ui} $):\n",
    "     $$\n",
    "     \\left( R_{ui} - \\hat{R}_{ui} \\right)^2\n",
    "     $$\n",
    "\n",
    "2. **Regularization Term**:\n",
    "   - Add penalties for the magnitudes of $ P $, $ Q $, $ b_u $, and $ b_i $ to prevent overfitting:\n",
    "     $$\n",
    "     \\lambda \\cdot \\left( ||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2 \\right)\n",
    "     $$\n",
    "\n",
    "3. **Sum Over All Observations**:\n",
    "   - Calculate the error term for all observed user-item pairs in the training data $ \\mathcal{D} $ and add the regularization penalty.\n",
    "\n",
    "---\n",
    "\n",
    "#### Example for a Single Epoch\n",
    "For a dataset with 3 user-item interactions:\n",
    "$$\n",
    "\\mathcal{D} = \\{(u=0, i=1, R_{ui}=4.0), (u=1, i=2, R_{ui}=3.5), (u=2, i=0, R_{ui}=5.0)\\}\n",
    "$$\n",
    "\n",
    "The total loss would be:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{(u, i) \\in \\mathcal{D}} \\left( R_{ui} - (\\mu + b_u[u] + b_i[i] + P[u, :] \\cdot Q[i, :]^T) \\right)^2 + \\lambda \\cdot \\left( ||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2 \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train MSE: 0.7944, Val MSE: 0.8509\n",
      "Epoch 2/50, Train MSE: 0.7525, Val MSE: 0.8201\n",
      "Epoch 3/50, Train MSE: 0.7301, Val MSE: 0.8170\n",
      "Epoch 4/50, Train MSE: 0.7149, Val MSE: 0.8099\n",
      "Epoch 5/50, Train MSE: 0.7026, Val MSE: 0.7987\n",
      "Epoch 6/50, Train MSE: 0.6930, Val MSE: 0.7977\n",
      "Epoch 7/50, Train MSE: 0.6838, Val MSE: 0.7887\n",
      "Epoch 8/50, Train MSE: 0.6727, Val MSE: 0.7904\n",
      "Epoch 9/50, Train MSE: 0.6590, Val MSE: 0.7910\n",
      "Epoch 10/50, Train MSE: 0.6405, Val MSE: 0.7862\n",
      "Epoch 11/50, Train MSE: 0.6180, Val MSE: 0.7845\n",
      "Epoch 12/50, Train MSE: 0.5904, Val MSE: 0.7819\n",
      "Epoch 13/50, Train MSE: 0.5610, Val MSE: 0.7805\n",
      "Epoch 14/50, Train MSE: 0.5282, Val MSE: 0.7758\n",
      "Epoch 15/50, Train MSE: 0.4947, Val MSE: 0.7717\n",
      "Epoch 16/50, Train MSE: 0.4607, Val MSE: 0.7745\n",
      "Epoch 17/50, Train MSE: 0.4272, Val MSE: 0.7712\n",
      "Epoch 18/50, Train MSE: 0.3957, Val MSE: 0.7716\n",
      "Epoch 19/50, Train MSE: 0.3659, Val MSE: 0.7757\n",
      "Epoch 20/50, Train MSE: 0.3384, Val MSE: 0.7768\n",
      "Epoch 21/50, Train MSE: 0.3127, Val MSE: 0.7792\n",
      "Epoch 22/50, Train MSE: 0.2890, Val MSE: 0.7836\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Final user latent matrix (P): (610, 42)\n",
      "Final item latent matrix (Q): (9724, 42)\n"
     ]
    }
   ],
   "source": [
    "# Integration with Data Preparation Code\n",
    "# Sparse matrices should already be prepared (train_sparse, val_sparse, test_sparse).\n",
    "\n",
    "# {'latent_dim': 42, 'learning_rate': 0.012376565252378615, 'regularization': 3.677000997278053e-05}\n",
    "\n",
    "# Define parameters\n",
    "latent_dim = 42\n",
    "learning_rate = 0.012376565252378615\n",
    "regularization = 3.677000997278053e-05\n",
    "epochs = 50\n",
    "patience = 5\n",
    "\n",
    "# Get the number of users and items from mappings\n",
    "num_users = train_sparse.shape[0]\n",
    "num_items = train_sparse.shape[1]\n",
    "\n",
    "# Perform matrix factorization with SGD on sparse matrices\n",
    "P, Q, train_losses, val_losses = matrix_factorization(\n",
    "    train_sparse, val_sparse, num_users, num_items, latent_dim, epochs, learning_rate, regularization, patience\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(\"Training complete.\")\n",
    "print(f\"Final user latent matrix (P): {P.shape}\")\n",
    "print(f\"Final item latent matrix (Q): {Q.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization Training Loop Example (Without Content Blending)\n",
    "To understand what is going on in the training loop we can have a look at a simplified math example with small matrices and values.\n",
    "\n",
    "#### Initial Setup\n",
    "We use small matrices for simplicity:\n",
    "\n",
    "1. **User Latent Matrix ($ P $)**:\n",
    "   $$\n",
    "   P =\n",
    "   \\begin{bmatrix}\n",
    "   0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\\n",
    "   0.2 & 0.3 & 0.1 & 0.5 & 0.4 \\\\\n",
    "   0.3 & 0.1 & 0.2 & 0.4 & 0.3\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   Shape: $ (3, 5) $ (3 users, 5 latent dimensions).\n",
    "\n",
    "2. **Item Latent Matrix ($ Q $)**:\n",
    "   $$\n",
    "   Q =\n",
    "   \\begin{bmatrix}\n",
    "   0.4 & 0.3 & 0.5 & 0.2 & 0.1 \\\\\n",
    "   0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\\n",
    "   0.5 & 0.4 & 0.3 & 0.2 & 0.1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   Shape: $ (3, 5) $ (3 items, 5 latent dimensions).\n",
    "\n",
    "3. **Biases**:\n",
    "   - Global bias ($ \\mu $): $ 3.0 $\n",
    "   - User biases ($ b_u $): $ [0.1, 0.2, 0.3] $\n",
    "   - Item biases ($ b_i $): $ [0.2, 0.3, 0.4] $\n",
    "\n",
    "4. **Known Interaction**:\n",
    "   - User $ u = 0 $, Item $ i = 1 $, Rating $ R_{ui} = 4.0 $.\n",
    "\n",
    "5. **Learning Parameters**:\n",
    "   - Learning rate ($ \\eta $): $ 0.01 $\n",
    "   - Regularization ($ \\lambda $): $ 0.1 $\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Calculation\n",
    "\n",
    "1. **Compute Prediction ($ \\hat{R}_{ui} $)**:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = \\mu + b_u[u] + b_i[i] + P[u, :] \\cdot Q[i, :]^T\n",
    "   $$\n",
    "   Substituting values:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = 3.0 + 0.1 + 0.3 + \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} \\cdot\n",
    "   \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\ 0.5 \\end{bmatrix}\n",
    "   $$\n",
    "   Dot product:\n",
    "   $$\n",
    "   \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} \\cdot\n",
    "   \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\ 0.5 \\end{bmatrix} = (0.1 \\cdot 0.1) + (0.2 \\cdot 0.2) + (0.3 \\cdot 0.3) + (0.4 \\cdot 0.4) + (0.5 \\cdot 0.5) = 0.55\n",
    "   $$\n",
    "   Prediction:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = 3.0 + 0.1 + 0.3 + 0.55 = 3.95\n",
    "   $$\n",
    "\n",
    "2. **Compute Error ($ E_{ui} $)**:\n",
    "   $$\n",
    "   E_{ui} = R_{ui} - \\hat{R}_{ui} = 4.0 - 3.95 = 0.05\n",
    "   $$\n",
    "\n",
    "3. **Update Biases**:\n",
    "   $$\n",
    "   b_u[u] \\leftarrow b_u[u] + \\eta \\cdot (E_{ui} - \\lambda \\cdot b_u[u])\n",
    "   $$\n",
    "   Substituting values:\n",
    "   $$\n",
    "   b_u[0] \\leftarrow 0.1 + 0.01 \\cdot (0.05 - 0.1 \\cdot 0.1) = 0.1 + 0.01 \\cdot (0.05 - 0.01) = 0.1 + 0.0004 = 0.1004\n",
    "   $$\n",
    "   Similarly, for $ b_i[i] $:\n",
    "   $$\n",
    "   b_i[1] \\leftarrow 0.3 + 0.01 \\cdot (0.05 - 0.1 \\cdot 0.3) = 0.3 + 0.01 \\cdot (0.05 - 0.03) = 0.3 + 0.0002 = 0.3002\n",
    "   $$\n",
    "\n",
    "4. **Update Latent Matrices ($ P $ and $ Q $)**:\n",
    "   $$\n",
    "   P[u, :] \\leftarrow P[u, :] + \\eta \\cdot (E_{ui} \\cdot Q[i, :] - \\lambda \\cdot P[u, :])\n",
    "   $$\n",
    "   Substituting values:\n",
    "   $$\n",
    "   P[0, :] \\leftarrow \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} +\n",
    "   0.01 \\cdot (0.05 \\cdot \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} -\n",
    "   0.1 \\cdot \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix})\n",
    "   $$\n",
    "   Compute intermediate terms:\n",
    "   $$\n",
    "   0.05 \\cdot \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 0.005 & 0.01 & 0.015 & 0.02 & 0.025 \\end{bmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   0.1 \\cdot \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} = \\begin{bmatrix} 0.01 & 0.02 & 0.03 & 0.04 & 0.05 \\end{bmatrix}\n",
    "   $$\n",
    "   $$\n",
    "   \\begin{bmatrix} 0.005 & 0.01 & 0.015 & 0.02 & 0.025 \\end{bmatrix} -\n",
    "   \\begin{bmatrix} 0.01 & 0.02 & 0.03 & 0.04 & 0.05 \\end{bmatrix} =\n",
    "   \\begin{bmatrix} -0.005 & -0.01 & -0.015 & -0.02 & -0.025 \\end{bmatrix}\n",
    "   $$\n",
    "   Scale by $ \\eta $:\n",
    "   $$\n",
    "   0.01 \\cdot \\begin{bmatrix} -0.005 & -0.01 & -0.015 & -0.02 & -0.025 \\end{bmatrix} =\n",
    "   \\begin{bmatrix} -0.00005 & -0.0001 & -0.00015 & -0.0002 & -0.00025 \\end{bmatrix}\n",
    "   $$\n",
    "   Update $ P[u, :] $:\n",
    "   $$\n",
    "   P[0, :] \\leftarrow \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix} +\n",
    "   \\begin{bmatrix} -0.00005 & -0.0001 & -0.00015 & -0.0002 & -0.00025 \\end{bmatrix} =\n",
    "   \\begin{bmatrix} 0.09995 & 0.1999 & 0.29985 & 0.3998 & 0.49975 \\end{bmatrix}\n",
    "   $$\n",
    "   Similarly, update $ Q[i, :] $:\n",
    "   $$\n",
    "   Q[1, :] \\leftarrow Q[1, :] + \\eta \\cdot (E_{ui} \\cdot P[u, :] - \\lambda \\cdot Q[i, :])\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary of Updates\n",
    "After one loop:\n",
    "- Updated $ P[0, :] $: $ \\begin{bmatrix} 0.09995 & 0.1999 & 0.29985 & 0.3998 & 0.49975 \\end{bmatrix} $\n",
    "- Updated $ Q[1, :] $: Similar update process.\n",
    "- Updated $ b_u[0] $: $ 0.1004 $\n",
    "- Updated $ b_i[1] $: $ 0.3002 $\n",
    "\n",
    "This process repeats for all observed user-item interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "    Parameters:\n",
    "        trial (optuna.Trial): A trial object for hyperparameter suggestions.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation loss for the best set of hyperparameters.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 5, 50)  # Latent dimensions\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)  # Learning rate\n",
    "    regularization = trial.suggest_float(\"regularization\", 1e-5, 1e-1, log=True)  # Regularization parameter\n",
    "    patience = 5  # Fixed patience for early stopping\n",
    "    epochs = 50  # Fixed number of epochs per trial\n",
    "\n",
    "    # Train the model with the suggested hyperparameters\n",
    "    _, _, _, val_losses = matrix_factorization(\n",
    "        train_sparse=train_sparse,\n",
    "        val_sparse=val_sparse,\n",
    "        num_users=len(user_mapping),\n",
    "        num_items=len(movie_mapping),\n",
    "        latent_dim=latent_dim,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        regularization=regularization,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    # Return the last validation loss as the objective value\n",
    "    return val_losses[-1]\n",
    "\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # Run for 30 trials\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Best validation loss\n",
    "print(f\"Best validation loss: {study.best_value:.4f}\")\n",
    "\n",
    "# Best trial details\n",
    "print(f\"Best trial: {study.best_trial}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Incorporating Content-Based Features into Matrix Factorization\n",
    "\n",
    "## 1. Why Combine Content-Based and Collaborative Filtering?\n",
    "- Collaborative filtering leverages historical user-item interaction data to recommend items. However, it struggles with:\n",
    "  - **Cold Start Problem**: Recommending items or users with little or no interaction data.\n",
    "  - **Sparse Data**: Performance issues when the user-item interaction matrix is sparse.\n",
    "- Content-based filtering uses item metadata (e.g., descriptions, tags, genres) to extract features and generate recommendations.\n",
    "- Combining the two approaches enhances the recommendation system by:\n",
    "  - Enabling predictions for new items/users using metadata.\n",
    "  - Improving overall recommendation quality by blending collaborative and content-based signals.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Content-Based Feature Extraction\n",
    "To integrate content-based insights, item metadata must be represented in a machine-readable format. A common method is **TF-IDF (Term Frequency-Inverse Document Frequency)**.\n",
    "\n",
    "### TF-IDF Overview\n",
    "- **Term Frequency (TF)**: Measures how often a term appears in an item description.\n",
    "  $$\n",
    "  TF(t, d) = \\frac{\\text{Frequency of term } t \\text{ in item } d}{\\text{Total terms in item } d}\n",
    "  $$\n",
    "- **Inverse Document Frequency (IDF)**: Reduces the weight of common terms across all items.\n",
    "  $$\n",
    "  IDF(t, D) = \\log \\left( \\frac{\\text{Total items in the dataset}}{\\text{Number of items containing term } t} \\right)\n",
    "  $$\n",
    "- **TF-IDF Score**: Combines TF and IDF to compute term importance in item descriptions.\n",
    "  $$\n",
    "  TF\\text{-}IDF(t, d, D) = TF(t, d) \\cdot IDF(t, D)\n",
    "  $$\n",
    "\n",
    "### Implementation in Recommendation Systems\n",
    "- Use **TF-IDF Vectorizer** to transform item metadata into a sparse matrix (items x features).\n",
    "- Resulting features represent items in a high-dimensional space, capturing content semantics.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Combining Content-Based Features with Collaborative Filtering\n",
    "To blend content-based and collaborative filtering:\n",
    "1. **Feature Matrix Construction**:\n",
    "   - Create a **content matrix** (e.g., from TF-IDF) for all items.\n",
    "   - Optionally, reduce dimensions using techniques like **Truncated SVD** for computational efficiency.\n",
    "2. **Augment Item Representations**:\n",
    "   - Enhance collaborative filtering's item feature matrix (`Q`) with content-based features.\n",
    "   - Example: Concatenate or blend the learned item latent features with the content matrix.\n",
    "3. **Reformulate Prediction Function**:\n",
    "   - Combine user latent matrix (`P`) with both collaborative and content-based item features.\n",
    "   $$\n",
    "   \\hat{R}_{ui} = \\mu + b_u + b_i + P_u \\cdot Q_i + P_u \\cdot \\text{Content}_i\n",
    "   $$\n",
    "   where:\n",
    "   - $ P_u $: User latent features.\n",
    "   - $ Q_i $: Collaborative latent item features.\n",
    "   - $ \\text{Content}_i $: Content-based item features.\n",
    "4. **Optimize**:\n",
    "   - Train the model with both collaborative and content-based features using the same loss function, ensuring effective joint learning.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Practical Considerations\n",
    "- **Scalability**: TF-IDF features can lead to high-dimensional matrices; consider dimensionality reduction.\n",
    "- **Data Integrity**: Ensure metadata is well-prepared, clean, and consistent.\n",
    "- **Weight Balancing**: Experiment with weight ratios between collaborative and content-based features for optimal results.\n",
    "\n",
    "By incorporating content-based features into matrix factorization, the recommendation system leverages metadata for robust predictions, addressing cold start and sparsity challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Loss: Matrix Factorization With and Without Content Blending\n",
    "\n",
    "### 1. **Matrix Factorization Without Content Blending**\n",
    "The loss is calculated using only the collaborative filtering components:\n",
    "$$\n",
    "\\text{Loss}_{\\text{no content}} = \\sum_{(u, i) \\in \\mathcal{D}} \\left( R_{ui} - (\\mu + b_u[u] + b_i[i] + P[u, :] \\cdot Q[i, :]^T) \\right)^2 + \\lambda \\cdot \\left( ||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2 \\right)\n",
    "$$\n",
    "\n",
    "- **Error Term**:\n",
    "  - The predicted rating $ \\hat{R}_{ui} $ is derived only from the collaborative latent factors $ P $ and $ Q $.\n",
    "- **Regularization**:\n",
    "  - Penalizes the magnitudes of $ P $, $ Q $, user biases $ b_u $, and item biases $ b_i $.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Matrix Factorization With Content Blending**\n",
    "The loss incorporates both collaborative and content-based components:\n",
    "$$\n",
    "\\text{Loss}_{\\text{content}} = \\sum_{(u, i) \\in \\mathcal{D}} \\left( R_{ui} - (\\mu + b_u[u] + b_i[i] + P[u, :] \\cdot [Q[i, :], \\text{Content}_i]^T) \\right)^2 + \\lambda \\cdot \\left( ||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2 \\right)\n",
    "$$\n",
    "\n",
    "- **Error Term**:\n",
    "  - The predicted rating $ \\hat{R}_{ui} $ now includes the contribution of the augmented item latent vector:\n",
    "    - $ [Q[i, :], \\text{Content}_i] $: Combines collaborative features ($ Q $) and content-based features ($ \\text{Content}_i $).\n",
    "- **Regularization**:\n",
    "  - Similar to the non-content case but focuses only on $ P $, $ Q $, $ b_u $, and $ b_i $. The content features ($ \\text{Content}_i $) are static and not regularized.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Comparison\n",
    "Consider the following:\n",
    "- Dataset with $ 3 $ users and $ 3 $ items.\n",
    "- Ratings ($ R_{ui} $):\n",
    "  $$\n",
    "  \\mathcal{D} = \\{(u=0, i=1, R_{ui}=4.0), (u=1, i=2, R_{ui}=3.5), (u=2, i=0, R_{ui}=5.0)\\}\n",
    "  $$\n",
    "- Frobenius norms of $ P $ and $ Q $ and biases are $ ||P||^2 = 1.2 $, $ ||Q||^2 = 0.8 $, $ ||b_u||^2 = 0.2 $, $ ||b_i||^2 = 0.3 $.\n",
    "- Regularization parameter: $ \\lambda = 0.1 $.\n",
    "\n",
    "---\n",
    "\n",
    "**Without Content Blending:**\n",
    "1. Predictions:\n",
    "   - $ \\hat{R}_{0,1} = 3.95 $, $ \\hat{R}_{1,2} = 3.6 $, $ \\hat{R}_{2,0} = 4.8 $.\n",
    "2. Errors:\n",
    "   - $ (R_{0,1} - \\hat{R}_{0,1})^2 = (4.0 - 3.95)^2 = 0.0025 $,\n",
    "   - $ (R_{1,2} - \\hat{R}_{1,2})^2 = (3.5 - 3.6)^2 = 0.01 $,\n",
    "   - $ (R_{2,0} - \\hat{R}_{2,0})^2 = (5.0 - 4.8)^2 = 0.04 $.\n",
    "3. Total Error:\n",
    "   $$\n",
    "   \\text{Error} = 0.0025 + 0.01 + 0.04 = 0.0525\n",
    "   $$\n",
    "4. Regularization:\n",
    "   $$\n",
    "   \\lambda \\cdot (||P||^2 + ||Q||^2 + ||b_u||^2 + ||b_i||^2) = 0.1 \\cdot (1.2 + 0.8 + 0.2 + 0.3) = 0.25\n",
    "   $$\n",
    "5. Total Loss:\n",
    "   $$\n",
    "   \\text{Loss}_{\\text{no content}} = 0.0525 + 0.25 = 0.3025\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "**With Content Blending:**\n",
    "1. Predictions:\n",
    "   - Incorporates both $ Q[i, :] $ and $ \\text{Content}_i $. Suppose the content-based contribution improves predictions:\n",
    "     - $ \\hat{R}_{0,1} = 4.0 $, $ \\hat{R}_{1,2} = 3.55 $, $ \\hat{R}_{2,0} = 4.9 $.\n",
    "2. Errors:\n",
    "   - $ (R_{0,1} - \\hat{R}_{0,1})^2 = (4.0 - 4.0)^2 = 0.0 $,\n",
    "   - $ (R_{1,2} - \\hat{R}_{1,2})^2 = (3.5 - 3.55)^2 = 0.0025 $,\n",
    "   - $ (R_{2,0} - \\hat{R}_{2,0})^2 = (5.0 - 4.9)^2 = 0.01 $.\n",
    "3. Total Error:\n",
    "   $$\n",
    "   \\text{Error} = 0.0 + 0.0025 + 0.01 = 0.0125\n",
    "   $$\n",
    "4. Regularization:\n",
    "   - Regularization remains unchanged: $ 0.25 $.\n",
    "5. Total Loss:\n",
    "   $$\n",
    "   \\text{Loss}_{\\text{content}} = 0.0125 + 0.25 = 0.2625\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Observations:\n",
    "1. **Without Content Blending**:\n",
    "   - Loss: $ \\text{Loss}_{\\text{no content}} = 0.3025 $.\n",
    "   - Predictions are based only on collaborative filtering.\n",
    "\n",
    "2. **With Content Blending**:\n",
    "   - Loss: $ \\text{Loss}_{\\text{content}} = 0.2625 $.\n",
    "   - Improved predictions reduce the error term, leading to a lower total loss.\n",
    "\n",
    "3. **Impact**:\n",
    "   - Content blending enhances predictions by incorporating metadata, reducing the overall loss.\n",
    "\n",
    "4. **Why is the regularization term not affected by the augmented Q matrix**:\n",
    "   - Because the static TF-IDF matrix cannot be adjusted by the training, so there is no learning parameter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We augment the training code with the content based features created in Step 1: Prepare Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we add the content dimensions to the initialisation of P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_latent_matrices_content(num_users, num_items, latent_dim):\n",
    "    \"\"\"\n",
    "    Initialize the user and item latent matrices with small random values.\n",
    "    \n",
    "    Parameters:\n",
    "        num_users (int): Number of users.\n",
    "        num_items (int): Number of items.\n",
    "        latent_dim (int): Number of latent dimensions (features).\n",
    "        \n",
    "    Returns:\n",
    "        P (numpy.ndarray): User latent matrix of shape (num_users, latent_dim).\n",
    "        Q (numpy.ndarray): Item latent matrix of shape (num_items, latent_dim).\n",
    "    \"\"\"\n",
    "    # Random initialization of user and item matrices\n",
    "    P = np.random.normal(scale=0.01, size=(num_users, latent_dim + content_matrix.shape[1]))\n",
    "    Q = np.random.normal(scale=0.01, size=(num_items, latent_dim))\n",
    "    \n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of $ P $ With and Without Content Blending\n",
    "\n",
    "#### 1. **Initialization of $ P $**\n",
    "- **Without Content Blending**:  \n",
    "  $ P $ is initialized with shape:\n",
    "  $$\n",
    "  P_{\\text{no content}} \\in \\mathbb{R}^{\\text{num\\_users} \\times \\text{latent\\_dim}}\n",
    "  $$\n",
    "- **With Content Blending**:  \n",
    "  $ P $ includes extra dimensions to account for the content matrix:\n",
    "  $$\n",
    "  P_{\\text{content}} \\in \\mathbb{R}^{\\text{num\\_users} \\times (\\text{latent\\_dim} + \\text{content\\_dim})}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **How $ P $ Changes During Training**\n",
    "\n",
    "**Without Content Blending**:\n",
    "- $ P_{\\text{no content}} $ is updated using only collaborative filtering gradients:\n",
    "  $$\n",
    "  \\Delta P[u, :] = \\eta \\cdot (E_{ui} \\cdot Q[i, :] - \\lambda \\cdot P[u, :])\n",
    "  $$\n",
    "  - $ Q $: Item latent factors (collaborative only).\n",
    "  - $ E_{ui} $: Prediction error.\n",
    "\n",
    "**With Content Blending**:\n",
    "- $ P_{\\text{content}} $ has additional dimensions to interact with **content-based features**:\n",
    "  $$\n",
    "  \\Delta P[u, :] = \\eta \\cdot (E_{ui} \\cdot [Q[i, :], \\text{Content}_i] - \\lambda \\cdot P[u, :])\n",
    "  $$\n",
    "  - The collaborative part (first $ \\text{latent\\_dim} $) interacts with $ Q[i, :] $.\n",
    "  - The added content dimensions interact with $ \\text{Content}_i $ (precomputed TF-IDF features).\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Differences Between $ P_{\\text{no content}} $ and $ P_{\\text{content}} $**\n",
    "\n",
    "1. **Dimensionality**:\n",
    "   - $ P_{\\text{no content}} $: Shape is $ (\\text{num\\_users}, \\text{latent\\_dim}) $.\n",
    "   - $ P_{\\text{content}} $: Shape is $ (\\text{num\\_users}, \\text{latent\\_dim} + \\text{content\\_dim}) $.\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - **Without Content**: Entire $ P $ represents user preferences derived only from collaborative signals.\n",
    "   - **With Content**:\n",
    "     - First $ \\text{latent\\_dim} $ columns represent collaborative filtering preferences.\n",
    "     - Last $ \\text{content\\_dim} $ columns encode **user-specific weights** for content features.\n",
    "\n",
    "3. **Post-Training Values**:\n",
    "   - Without content blending, $ P $ learns patterns from user-item interactions alone.\n",
    "   - With content blending:\n",
    "     - The collaborative part of $ P $ (first $ \\text{latent\\_dim} $) behaves similarly to the non-content case.\n",
    "     - The content dimensions of $ P $ adapt to capture user preferences for specific item features (e.g., genres or tags).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Example Illustration**\n",
    "Suppose:\n",
    "- $ \\text{latent\\_dim} = 2 $, $ \\text{content\\_dim} = 3 $.\n",
    "\n",
    "**Without Content Blending**:\n",
    "$$\n",
    "P_{\\text{no content}} = \n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 \\\\\n",
    "0.3 & 0.4 \\\\\n",
    "0.5 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**With Content Blending**:\n",
    "$$\n",
    "P_{\\text{content}} = \n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.05 & 0.03 & 0.04 \\\\\n",
    "0.3 & 0.4 & 0.07 & 0.02 & 0.06 \\\\\n",
    "0.5 & 0.6 & 0.01 & 0.09 & 0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The first two columns (collaborative dimensions) are similar to $ P_{\\text{no content}} $.\n",
    "- The last three columns (content dimensions) represent user-specific preferences for content features.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. **Impact of Content Blending**\n",
    "- **Without Content Blending**: $ P $ contains only collaborative user preferences.\n",
    "- **With Content Blending**: $ P $ incorporates both collaborative preferences and content-based preferences.\n",
    "- The additional dimensions in $ P $ help improve predictions by leveraging content-based metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "1. **Dimensionality**:\n",
    "   - $ P_{\\text{no content}} $: $ \\text{num\\_users} \\times \\text{latent\\_dim} $.\n",
    "   - $ P_{\\text{content}} $: $ \\text{num\\_users} \\times (\\text{latent\\_dim} + \\text{content\\_dim}) $.\n",
    "\n",
    "2. **Post-Training Behavior**:\n",
    "   - Collaborative dimensions in $ P_{\\text{content}} $ behave similarly to $ P_{\\text{no content}} $.\n",
    "   - Content dimensions in $ P_{\\text{content}} $ encode user preferences for item metadata.\n",
    "\n",
    "3. **Why It Matters**:\n",
    "   - Content blending improves predictions by integrating metadata, which helps especially in scenarios like the **cold-start problem**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparse_loss_content(rows, cols, values, P, Q, b_u, b_i, mu, regularization, content_matrix, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the loss for a given sparse interaction matrix, including bias terms and content-based features.\n",
    "\n",
    "    Parameters:\n",
    "        rows (np.ndarray): Row indices of the non-zero entries.\n",
    "        cols (np.ndarray): Column indices of the non-zero entries.\n",
    "        values (np.ndarray): Corresponding non-zero values.\n",
    "        P (np.ndarray): User latent matrix (includes both collaborative and content dimensions).\n",
    "        Q (np.ndarray): Collaborative item latent matrix (transposed).\n",
    "        b_u (np.ndarray): User biases.\n",
    "        b_i (np.ndarray): Item biases.\n",
    "        mu (float): Global bias.\n",
    "        regularization (float): Regularization parameter.\n",
    "        content_matrix (csr_matrix): Sparse matrix of content-based features.\n",
    "        normalize (bool): Whether to compute normalized loss (MSE).\n",
    "\n",
    "    Returns:\n",
    "        float: Computed loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(rows) == len(cols) == len(values), f\"Invalid sizes: {len(rows)}, {len(cols)}, {len(values)}\"\n",
    "\n",
    "    loss = 0\n",
    "    for idx in range(len(values)):\n",
    "        u = rows[idx]\n",
    "        i = cols[idx]\n",
    "\n",
    "        assert 0 <= u < len(P), f\"Invalid user index: {u}\"\n",
    "        assert 0 <= i < len(Q.T), f\"Invalid item index: {i}\"\n",
    "\n",
    "        rating = values[idx]\n",
    "\n",
    "        # Extract content-based features for item i\n",
    "        content_features = content_matrix.getrow(i).toarray().flatten()  # Extract sparse row as dense array\n",
    "\n",
    "        # Augment item latent vector with content features\n",
    "        augmented_Q_i = np.hstack([Q[:, i], content_features])\n",
    "\n",
    "        # Compute prediction including biases\n",
    "        prediction = mu + b_u[u] + b_i[i] + np.dot(P[u, :], augmented_Q_i)\n",
    "        error = rating - prediction\n",
    "        loss += error**2\n",
    "\n",
    "    # Add regularization term (includes biases and collaborative latent vectors)\n",
    "    loss += regularization * (\n",
    "        np.linalg.norm(P)**2 + np.linalg.norm(Q)**2 + np.linalg.norm(b_u)**2 + np.linalg.norm(b_i)**2\n",
    "    )\n",
    "\n",
    "    # Normalize the loss if requested\n",
    "    if normalize:\n",
    "        loss /= len(values)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def matrix_factorization_with_content(train_sparse, val_sparse, content_matrix, num_users, num_items, latent_dim, \n",
    "                                      epochs, learning_rate, regularization, patience=5, clip_value=5.0):\n",
    "    \"\"\"\n",
    "    Perform matrix factorization using SGD with content-based feature integration.\n",
    "\n",
    "    Parameters:\n",
    "        train_sparse (csr_matrix): Training interaction matrix (sparse).\n",
    "        val_sparse (csr_matrix): Validation interaction matrix (sparse).\n",
    "        content_matrix (csr_matrix): Content-based feature matrix (items x features, sparse).\n",
    "        num_users (int): Total number of users.\n",
    "        num_items (int): Total number of items.\n",
    "        latent_dim (int): Number of latent features.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for gradient descent.\n",
    "        regularization (float): Regularization parameter.\n",
    "        patience (int): Early stopping patience.\n",
    "        clip_value (float): Maximum value for gradient clipping.\n",
    "\n",
    "    Returns:\n",
    "        P (np.ndarray): Learned user latent matrix.\n",
    "        Q (np.ndarray): Learned item latent matrix.\n",
    "        train_losses (list): Training losses over epochs.\n",
    "        val_losses (list): Validation losses over epochs.\n",
    "        b_u (np.ndarray): Learned user biases.\n",
    "        b_i (np.ndarray): Learned item biases.\n",
    "        mu (float): Learned global bias.\n",
    "    \"\"\"\n",
    "    # Initialize latent matrices\n",
    "    P, Q = initialize_latent_matrices_content(num_users, num_items, latent_dim)\n",
    "\n",
    "    # Convert item latent matrix for matrix operations\n",
    "    Q = Q.T\n",
    "\n",
    "    # Initialize biases\n",
    "    mu = train_sparse.data.mean()  # Global bias\n",
    "    b_u = np.zeros(num_users)  # User biases\n",
    "    b_i = np.zeros(num_items)  # Item biases\n",
    "\n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Get non-zero training indices for sparse matrix\n",
    "    train_rows, train_cols = train_sparse.nonzero()\n",
    "    train_values = train_sparse.data\n",
    "\n",
    "    assert len(train_rows) == len(train_cols) == len(train_values), f\"Invalid sizes: {len(train_rows)}, {len(train_cols)}, {len(train_values)}\"\n",
    "    \n",
    "    # Get non-zero validation indices for sparse matrix\n",
    "    val_rows, val_cols = val_sparse.nonzero()\n",
    "    val_values = val_sparse.data\n",
    "\n",
    "    assert len(val_rows) == len(val_cols) == len(val_values), f\"Invalid sizes: {len(val_rows)}, {len(val_cols)}, {len(val_values)}\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle training indices\n",
    "        shuffle_indices = np.random.permutation(len(train_rows))\n",
    "        train_rows = train_rows[shuffle_indices]\n",
    "        train_cols = train_cols[shuffle_indices]\n",
    "        train_values = train_values[shuffle_indices]\n",
    "\n",
    "        # SGD for each non-zero entry in the sparse matrix\n",
    "        for idx in range(len(train_rows)):\n",
    "            u = train_rows[idx]\n",
    "            i = train_cols[idx]\n",
    "\n",
    "            assert 0 <= u < len(P), f\"Invalid user index: {u} at index {idx}\"\n",
    "            assert 0 <= i < len(Q.T), f\"Invalid item index: {i} at index {idx}\"\n",
    "\n",
    "            rating = train_values[idx]\n",
    "\n",
    "            # Compute content-based features for item `i`\n",
    "            content_features = content_matrix.getrow(i).toarray().flatten()  # Extract sparse row as dense array\n",
    "\n",
    "            # Augment item latent vector with content features\n",
    "            augmented_Q_i = np.hstack([Q[:, i], content_features])\n",
    "\n",
    "            # Compute prediction and error\n",
    "            prediction = mu + b_u[u] + b_i[i] + np.dot(P[u, :], augmented_Q_i)\n",
    "            error = rating - prediction\n",
    "\n",
    "            # Update biases with gradient clipping\n",
    "            delta_b_u = learning_rate * (error - regularization * b_u[u])\n",
    "            delta_b_i = learning_rate * (error - regularization * b_i[i])\n",
    "            b_u[u] += np.clip(delta_b_u, -clip_value, clip_value)\n",
    "            b_i[i] += np.clip(delta_b_i, -clip_value, clip_value)\n",
    "\n",
    "            # Update user and item latent vectors with gradient clipping\n",
    "            delta_P = learning_rate * (error * augmented_Q_i - regularization * P[u, :])\n",
    "            delta_Q_collab = learning_rate * (error * P[u, :latent_dim] - regularization * Q[:, i])\n",
    "\n",
    "            P[u, :] += np.clip(delta_P, -clip_value, clip_value)\n",
    "            Q[:, i] += np.clip(delta_Q_collab, -clip_value, clip_value)\n",
    "\n",
    "\n",
    "        # Compute training and validation losses\n",
    "        train_loss = compute_sparse_loss_content(train_rows, train_cols, train_values, P, Q, b_u, b_i, mu, regularization, content_matrix)\n",
    "        val_loss = compute_sparse_loss_content(val_rows, val_cols, val_values, P, Q, b_u, b_i, mu, regularization, content_matrix)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train MSE: {train_loss:.4f}, Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return P, Q.T, train_losses, val_losses, b_u, b_i, mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train MSE: 0.8070, Val MSE: 0.8697\n",
      "Epoch 2/100, Train MSE: 0.7551, Val MSE: 0.8487\n",
      "Epoch 3/100, Train MSE: 0.7234, Val MSE: 0.8275\n",
      "Epoch 4/100, Train MSE: 0.7002, Val MSE: 0.8224\n",
      "Epoch 5/100, Train MSE: 0.6810, Val MSE: 0.8198\n",
      "Epoch 6/100, Train MSE: 0.6654, Val MSE: 0.8124\n",
      "Epoch 7/100, Train MSE: 0.6510, Val MSE: 0.8096\n",
      "Epoch 8/100, Train MSE: 0.6385, Val MSE: 0.8099\n",
      "Epoch 9/100, Train MSE: 0.6272, Val MSE: 0.8126\n",
      "Epoch 10/100, Train MSE: 0.6162, Val MSE: 0.8074\n",
      "Epoch 11/100, Train MSE: 0.6053, Val MSE: 0.8062\n",
      "Epoch 12/100, Train MSE: 0.5945, Val MSE: 0.8088\n",
      "Epoch 13/100, Train MSE: 0.5836, Val MSE: 0.8076\n",
      "Epoch 14/100, Train MSE: 0.5717, Val MSE: 0.8081\n",
      "Epoch 15/100, Train MSE: 0.5586, Val MSE: 0.8059\n",
      "Epoch 16/100, Train MSE: 0.5440, Val MSE: 0.8030\n",
      "Epoch 17/100, Train MSE: 0.5282, Val MSE: 0.8081\n",
      "Epoch 18/100, Train MSE: 0.5116, Val MSE: 0.8054\n",
      "Epoch 19/100, Train MSE: 0.4946, Val MSE: 0.8035\n",
      "Epoch 20/100, Train MSE: 0.4766, Val MSE: 0.8044\n",
      "Epoch 21/100, Train MSE: 0.4585, Val MSE: 0.8043\n",
      "Early stopping triggered.\n",
      "Training complete.\n",
      "Final user latent matrix (P): (610, 1703)\n",
      "Final item latent matrix (Q): (9724, 28)\n",
      "User biases shape: (610,)\n",
      "Item biases shape: (9724,)\n",
      "Global bias (mu): 3.5216\n"
     ]
    }
   ],
   "source": [
    "# Integration with Data Preparation Code\n",
    "# Sparse matrices should already be prepared (train_sparse, val_sparse, test_sparse).\n",
    "\n",
    "# 0.8027304317478758 and parameters: {'latent_dim': 28, 'learning_rate': 0.008089574860460297, 'regularization': 1.6073221934120177e-05}\n",
    "\n",
    "# Define parameters\n",
    "latent_dim = 28\n",
    "learning_rate = 0.008089574860460297\n",
    "regularization = 1.6073221934120177e-05\n",
    "epochs = 100\n",
    "patience = 5\n",
    "\n",
    "# Get the number of users and items from mappings\n",
    "num_users = train_sparse.shape[0]\n",
    "num_items = train_sparse.shape[1]\n",
    "\n",
    "# Perform matrix factorization with SGD on sparse matrices\n",
    "P, Q, train_losses, val_losses, b_u, b_i, mu = matrix_factorization_with_content(\n",
    "    train_sparse, val_sparse, content_matrix, num_users, num_items, latent_dim, epochs, learning_rate, regularization, patience\n",
    ")\n",
    "\n",
    "# Output results\n",
    "print(\"Training complete.\")\n",
    "print(f\"Final user latent matrix (P): {P.shape}\")\n",
    "print(f\"Final item latent matrix (Q): {Q.shape}\")\n",
    "print(f\"User biases shape: {b_u.shape}\")\n",
    "print(f\"Item biases shape: {b_i.shape}\")\n",
    "print(f\"Global bias (mu): {mu:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved P matrix to saved_weights/P_matrix.npy\n",
      "Saved Q matrix to saved_weights/Q_matrix.npy\n",
      "Saved user biases to saved_weights/b_u.npy\n",
      "Saved item biases to saved_weights/b_i.npy\n",
      "Saved global bias to saved_weights/mu.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def save_matrices(P, Q, b_u, b_i, mu, \n",
    "                  P_path='P_matrix.npy', Q_path='Q_matrix.npy', \n",
    "                  b_u_path='b_u.npy', b_i_path='b_i.npy', mu_path='mu.npy'):\n",
    "    \"\"\"\n",
    "    Save the trained P, Q matrices, user biases, item biases, and global bias to files.\n",
    "\n",
    "    Args:\n",
    "    - P (np.ndarray): Trained user matrix.\n",
    "    - Q (np.ndarray): Trained item matrix.\n",
    "    - b_u (np.ndarray): User biases.\n",
    "    - b_i (np.ndarray): Item biases.\n",
    "    - mu (float): Global bias.\n",
    "    - P_path (str): File path to save the P matrix.\n",
    "    - Q_path (str): File path to save the Q matrix.\n",
    "    - b_u_path (str): File path to save the user biases.\n",
    "    - b_i_path (str): File path to save the item biases.\n",
    "    - mu_path (str): File path to save the global bias.\n",
    "    \"\"\"\n",
    "    np.save(P_path, P)\n",
    "    np.save(Q_path, Q)\n",
    "    np.save(b_u_path, b_u)\n",
    "    np.save(b_i_path, b_i)\n",
    "    np.save(mu_path, np.array([mu]))  # Save mu as a single-element array\n",
    "    print(f\"Saved P matrix to {P_path}\")\n",
    "    print(f\"Saved Q matrix to {Q_path}\")\n",
    "    print(f\"Saved user biases to {b_u_path}\")\n",
    "    print(f\"Saved item biases to {b_i_path}\")\n",
    "    print(f\"Saved global bias to {mu_path}\")\n",
    "\n",
    "\n",
    "def load_matrices(P_path='P_matrix.npy', Q_path='Q_matrix.npy', \n",
    "                  b_u_path='b_u.npy', b_i_path='b_i.npy', mu_path='mu.npy'):\n",
    "    \"\"\"\n",
    "    Load the trained P, Q matrices, user biases, item biases, and global bias from files.\n",
    "\n",
    "    Args:\n",
    "    - P_path (str): File path to load the P matrix.\n",
    "    - Q_path (str): File path to load the Q matrix.\n",
    "    - b_u_path (str): File path to load the user biases.\n",
    "    - b_i_path (str): File path to load the item biases.\n",
    "    - mu_path (str): File path to load the global bias.\n",
    "\n",
    "    Returns:\n",
    "    - P (np.ndarray): Loaded user matrix.\n",
    "    - Q (np.ndarray): Loaded item matrix.\n",
    "    - b_u (np.ndarray): Loaded user biases.\n",
    "    - b_i (np.ndarray): Loaded item biases.\n",
    "    - mu (float): Loaded global bias.\n",
    "    \"\"\"\n",
    "    P = np.load(P_path)\n",
    "    Q = np.load(Q_path)\n",
    "    b_u = np.load(b_u_path)\n",
    "    b_i = np.load(b_i_path)\n",
    "    mu = np.load(mu_path)[0]  # Extract the single value for mu\n",
    "    print(f\"Loaded P matrix from {P_path} with shape {P.shape}\")\n",
    "    print(f\"Loaded Q matrix from {Q_path} with shape {Q.shape}\")\n",
    "    print(f\"Loaded user biases from {b_u_path} with shape {b_u.shape}\")\n",
    "    print(f\"Loaded item biases from {b_i_path} with shape {b_i.shape}\")\n",
    "    print(f\"Loaded global bias (mu) from {mu_path} with value {mu:.4f}\")\n",
    "    return P, Q, b_u, b_i, mu\n",
    "\n",
    "save_matrices(P, Q, b_u, b_i, mu, \n",
    "              'saved_weights/P_matrix.npy', 'saved_weights/Q_matrix.npy', \n",
    "              'saved_weights/b_u.npy', 'saved_weights/b_i.npy', 'saved_weights/mu.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization Training Loop Example (With Content Blending)\n",
    "\n",
    "#### Initial Setup\n",
    "We use small matrices for simplicity:\n",
    "\n",
    "1. **User Latent Matrix ($ P $)**:\n",
    "   $$\n",
    "   P =\n",
    "   \\begin{bmatrix}\n",
    "   0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.1 & 0.2 \\\\\n",
    "   0.2 & 0.3 & 0.1 & 0.5 & 0.4 & 0.3 & 0.1 \\\\\n",
    "   0.3 & 0.1 & 0.2 & 0.4 & 0.3 & 0.4 & 0.3\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   Shape: $ (3, 7) $ (3 users, $ 5 $ collaborative dimensions + $ 2 $ content dimensions).\n",
    "\n",
    "2. **Collaborative Item Latent Matrix ($ Q $)**:\n",
    "   $$\n",
    "   Q =\n",
    "   \\begin{bmatrix}\n",
    "   0.4 & 0.3 & 0.5 & 0.2 & 0.1 \\\\\n",
    "   0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\\\\n",
    "   0.5 & 0.4 & 0.3 & 0.2 & 0.1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   Shape: $ (3, 5) $ (3 items, $ 5 $ collaborative dimensions).\n",
    "\n",
    "3. **Content Features Matrix ($ \\text{Content} $)**:\n",
    "   $$\n",
    "   \\text{Content} =\n",
    "   \\begin{bmatrix}\n",
    "   0.6 & 0.8 \\\\\n",
    "   0.7 & 0.9 \\\\\n",
    "   0.5 & 0.4\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   Shape: $ (3, 2) $ (3 items, $ 2 $ content-based dimensions).\n",
    "\n",
    "4. **Biases**:\n",
    "   - Global bias ($ \\mu $): $ 3.0 $\n",
    "   - User biases ($ b_u $): $ [0.1, 0.2, 0.3] $\n",
    "   - Item biases ($ b_i $): $ [0.2, 0.3, 0.4] $\n",
    "\n",
    "5. **Known Interaction**:\n",
    "   - User $ u = 0 $, Item $ i = 1 $, Rating $ R_{ui} = 4.0 $.\n",
    "\n",
    "6. **Learning Parameters**:\n",
    "   - Learning rate ($ \\eta $): $ 0.01 $\n",
    "   - Regularization ($ \\lambda $): $ 0.1 $\n",
    "\n",
    "---\n",
    "\n",
    "#### Step-by-Step Calculation\n",
    "\n",
    "1. **Augment Item Latent Vector ($ Q[i, :] $)**:\n",
    "   - $ Q[i, :] $: Collaborative latent vector for item $ i = 1 $:\n",
    "     $$\n",
    "     Q[1, :] = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 \\end{bmatrix}\n",
    "     $$\n",
    "   - $ \\text{Content}[i, :] $: Content-based vector for item $ i = 1 $:\n",
    "     $$\n",
    "     \\text{Content}[1, :] = \\begin{bmatrix} 0.7 & 0.9 \\end{bmatrix}\n",
    "     $$\n",
    "   - Augmented $ Q[i, :] $:\n",
    "     $$\n",
    "     \\text{Augmented } Q[i, :] = \\begin{bmatrix} Q[i, :], \\text{Content}[i, :] \\end{bmatrix}\n",
    "     = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.7 & 0.9 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "2. **Compute Prediction ($ \\hat{R}_{ui} $)**:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = \\mu + b_u[u] + b_i[i] + P[u, :] \\cdot \\text{Augmented } Q[i, :]^T\n",
    "   $$\n",
    "   Substituting values:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = 3.0 + 0.1 + 0.3 +\n",
    "   \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.1 & 0.2 \\end{bmatrix} \\cdot\n",
    "   \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\ 0.5 \\\\ 0.7 \\\\ 0.9 \\end{bmatrix}\n",
    "   $$\n",
    "   Dot product:\n",
    "   $$\n",
    "   \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.1 & 0.2 \\end{bmatrix} \\cdot\n",
    "   \\begin{bmatrix} 0.1 \\\\ 0.2 \\\\ 0.3 \\\\ 0.4 \\\\ 0.5 \\\\ 0.7 \\\\ 0.9 \\end{bmatrix}\n",
    "   = (0.1 \\cdot 0.1) + (0.2 \\cdot 0.2) + (0.3 \\cdot 0.3) + (0.4 \\cdot 0.4) +\n",
    "   (0.5 \\cdot 0.5) + (0.1 \\cdot 0.7) + (0.2 \\cdot 0.9)\n",
    "   $$\n",
    "   $$\n",
    "   = 0.01 + 0.04 + 0.09 + 0.16 + 0.25 + 0.07 + 0.18 = 0.81\n",
    "   $$\n",
    "   Prediction:\n",
    "   $$\n",
    "   \\hat{R}_{ui} = 3.0 + 0.1 + 0.3 + 0.81 = 4.21\n",
    "   $$\n",
    "\n",
    "3. **Compute Error ($ E_{ui} $)**:\n",
    "   $$\n",
    "   E_{ui} = R_{ui} - \\hat{R}_{ui} = 4.0 - 4.21 = -0.21\n",
    "   $$\n",
    "\n",
    "4. **Update Biases**:\n",
    "   $$\n",
    "   b_u[u] \\leftarrow b_u[u] + \\eta \\cdot (E_{ui} - \\lambda \\cdot b_u[u])\n",
    "   $$\n",
    "   Substituting values:\n",
    "   $$\n",
    "   b_u[0] \\leftarrow 0.1 + 0.01 \\cdot (-0.21 - 0.1 \\cdot 0.1) = 0.1 + 0.01 \\cdot (-0.21 - 0.01) = 0.1 - 0.0022 = 0.0978\n",
    "   $$\n",
    "   Similarly, for $ b_i[i] $:\n",
    "   $$\n",
    "   b_i[1] \\leftarrow 0.3 + 0.01 \\cdot (-0.21 - 0.1 \\cdot 0.3) = 0.3 + 0.01 \\cdot (-0.21 - 0.03) = 0.3 - 0.0024 = 0.2976\n",
    "   $$\n",
    "\n",
    "5. **Update Latent Matrices ($ P $ and $ Q $)**:\n",
    "   $$\n",
    "   P[u, :] \\leftarrow P[u, :] + \\eta \\cdot (E_{ui} \\cdot \\text{Augmented } Q[i, :] - \\lambda \\cdot P[u, :])\n",
    "   $$\n",
    "   Substituting values:\n",
    "   $$\n",
    "   P[0, :] \\leftarrow \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.1 & 0.2 \\end{bmatrix} +\n",
    "   0.01 \\cdot (-0.21 \\cdot \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.7 & 0.9 \\end{bmatrix} -\n",
    "   0.1 \\cdot \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.1 & 0.2 \\end{bmatrix})\n",
    "   $$\n",
    "\n",
    "   Compute intermediate updates:\n",
    "   - $ -0.21 \\cdot \\text{Augmented } Q[i, :] $:\n",
    "     $$\n",
    "     \\begin{bmatrix} -0.021 & -0.042 & -0.063 & -0.084 & -0.105 & -0.147 & -0.189 \\end{bmatrix}\n",
    "     $$\n",
    "   - $ -0.1 \\cdot P[u, :] $:\n",
    "     $$\n",
    "     \\begin{bmatrix} -0.01 & -0.02 & -0.03 & -0.04 & -0.05 & -0.01 & -0.02 \\end{bmatrix}\n",
    "     $$\n",
    "   - Combined:\n",
    "     $$\n",
    "     \\begin{bmatrix} -0.031 & -0.062 & -0.093 & -0.124 & -0.155 & -0.157 & -0.209 \\end{bmatrix}\n",
    "     $$\n",
    "   - Scale by $ \\eta = 0.01 $:\n",
    "     $$\n",
    "     \\begin{bmatrix} -0.00031 & -0.00062 & -0.00093 & -0.00124 & -0.00155 & -0.00157 & -0.00209 \\end{bmatrix}\n",
    "     $$\n",
    "   - Update $ P[u, :] $:\n",
    "     $$\n",
    "     \\begin{bmatrix} 0.09969 & 0.19938 & 0.29907 & 0.39876 & 0.49845 & 0.09843 & 0.19791 \\end{bmatrix}\n",
    "     $$\n",
    "\n",
    "   Similarly, update $ Q[i, :] $ using only the collaborative part.\n",
    "\n",
    "---\n",
    "\n",
    "#### Summary of Updates\n",
    "After one loop:\n",
    "- Updated $ P[0, :] $: $ \\begin{bmatrix} 0.09969 & 0.19938 & 0.29907 & 0.39876 & 0.49845 & 0.09843 & 0.19791 \\end{bmatrix} $\n",
    "- Updated $ Q[1, :] $: Collaborative part updated similarly.\n",
    "- Updated $ b_u[0] $: $ 0.0978 $\n",
    "- Updated $ b_i[1] $: $ 0.2976 $\n",
    "\n",
    "This process repeats for all observed user-item interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna search for hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:30:44,579] A new study created in memory with name: no-name-2b3d754b-7b0c-47e9-be22-a04ebdd4b919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train MSE: 0.9331, Val MSE: 0.9627\n",
      "Epoch 2/25, Train MSE: 0.8806, Val MSE: 0.9227\n",
      "Epoch 3/25, Train MSE: 0.8491, Val MSE: 0.8998\n",
      "Epoch 4/25, Train MSE: 0.8270, Val MSE: 0.8850\n",
      "Epoch 5/25, Train MSE: 0.8100, Val MSE: 0.8741\n",
      "Epoch 6/25, Train MSE: 0.7963, Val MSE: 0.8659\n",
      "Epoch 7/25, Train MSE: 0.7847, Val MSE: 0.8587\n",
      "Epoch 8/25, Train MSE: 0.7746, Val MSE: 0.8531\n",
      "Epoch 9/25, Train MSE: 0.7657, Val MSE: 0.8486\n",
      "Epoch 10/25, Train MSE: 0.7577, Val MSE: 0.8445\n",
      "Epoch 11/25, Train MSE: 0.7504, Val MSE: 0.8421\n",
      "Epoch 12/25, Train MSE: 0.7437, Val MSE: 0.8377\n",
      "Epoch 13/25, Train MSE: 0.7375, Val MSE: 0.8352\n",
      "Epoch 14/25, Train MSE: 0.7317, Val MSE: 0.8328\n",
      "Epoch 15/25, Train MSE: 0.7263, Val MSE: 0.8311\n",
      "Epoch 16/25, Train MSE: 0.7212, Val MSE: 0.8290\n",
      "Epoch 17/25, Train MSE: 0.7163, Val MSE: 0.8268\n",
      "Epoch 18/25, Train MSE: 0.7117, Val MSE: 0.8259\n",
      "Epoch 19/25, Train MSE: 0.7073, Val MSE: 0.8246\n",
      "Epoch 20/25, Train MSE: 0.7031, Val MSE: 0.8234\n",
      "Epoch 21/25, Train MSE: 0.6992, Val MSE: 0.8227\n",
      "Epoch 22/25, Train MSE: 0.6953, Val MSE: 0.8210\n",
      "Epoch 23/25, Train MSE: 0.6916, Val MSE: 0.8200\n",
      "Epoch 24/25, Train MSE: 0.6880, Val MSE: 0.8184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:34:44,467] Trial 0 finished with value: 0.8182290129765261 and parameters: {'latent_dim': 11, 'learning_rate': 0.0015489638117980062, 'regularization': 2.624625377395399e-05}. Best is trial 0 with value: 0.8182290129765261.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.6846, Val MSE: 0.8182\n",
      "Epoch 1/25, Train MSE: 0.8596, Val MSE: 0.9066\n",
      "Epoch 2/25, Train MSE: 0.8064, Val MSE: 0.8702\n",
      "Epoch 3/25, Train MSE: 0.7759, Val MSE: 0.8527\n",
      "Epoch 4/25, Train MSE: 0.7543, Val MSE: 0.8446\n",
      "Epoch 5/25, Train MSE: 0.7369, Val MSE: 0.8375\n",
      "Epoch 6/25, Train MSE: 0.7224, Val MSE: 0.8309\n",
      "Epoch 7/25, Train MSE: 0.7100, Val MSE: 0.8253\n",
      "Epoch 8/25, Train MSE: 0.6992, Val MSE: 0.8221\n",
      "Epoch 9/25, Train MSE: 0.6895, Val MSE: 0.8185\n",
      "Epoch 10/25, Train MSE: 0.6805, Val MSE: 0.8167\n",
      "Epoch 11/25, Train MSE: 0.6723, Val MSE: 0.8151\n",
      "Epoch 12/25, Train MSE: 0.6649, Val MSE: 0.8133\n",
      "Epoch 13/25, Train MSE: 0.6577, Val MSE: 0.8144\n",
      "Epoch 14/25, Train MSE: 0.6512, Val MSE: 0.8108\n",
      "Epoch 15/25, Train MSE: 0.6447, Val MSE: 0.8106\n",
      "Epoch 16/25, Train MSE: 0.6388, Val MSE: 0.8095\n",
      "Epoch 17/25, Train MSE: 0.6331, Val MSE: 0.8102\n",
      "Epoch 18/25, Train MSE: 0.6277, Val MSE: 0.8097\n",
      "Epoch 19/25, Train MSE: 0.6225, Val MSE: 0.8075\n",
      "Epoch 20/25, Train MSE: 0.6174, Val MSE: 0.8092\n",
      "Epoch 21/25, Train MSE: 0.6125, Val MSE: 0.8081\n",
      "Epoch 22/25, Train MSE: 0.6077, Val MSE: 0.8087\n",
      "Epoch 23/25, Train MSE: 0.6031, Val MSE: 0.8083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:38:32,218] Trial 1 finished with value: 0.8085951979087731 and parameters: {'latent_dim': 9, 'learning_rate': 0.004075751165456095, 'regularization': 0.0007302273953221217}. Best is trial 1 with value: 0.8085951979087731.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Train MSE: 0.5985, Val MSE: 0.8086\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7022, Val MSE: 0.8185\n",
      "Epoch 2/25, Train MSE: 0.6396, Val MSE: 0.8181\n",
      "Epoch 3/25, Train MSE: 0.5987, Val MSE: 0.8188\n",
      "Epoch 4/25, Train MSE: 0.5676, Val MSE: 0.8118\n",
      "Epoch 5/25, Train MSE: 0.5211, Val MSE: 0.8149\n",
      "Epoch 6/25, Train MSE: 0.4707, Val MSE: 0.8245\n",
      "Epoch 7/25, Train MSE: 0.4166, Val MSE: 0.8213\n",
      "Epoch 8/25, Train MSE: 0.3726, Val MSE: 0.8334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:39:56,095] Trial 2 finished with value: 0.8356080977659864 and parameters: {'latent_dim': 6, 'learning_rate': 0.03560349756849463, 'regularization': 0.0002543069369501906}. Best is trial 1 with value: 0.8085951979087731.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Train MSE: 0.3394, Val MSE: 0.8356\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.8072, Val MSE: 0.8670\n",
      "Epoch 2/25, Train MSE: 0.7552, Val MSE: 0.8469\n",
      "Epoch 3/25, Train MSE: 0.7235, Val MSE: 0.8339\n",
      "Epoch 4/25, Train MSE: 0.6998, Val MSE: 0.8204\n",
      "Epoch 5/25, Train MSE: 0.6812, Val MSE: 0.8167\n",
      "Epoch 6/25, Train MSE: 0.6654, Val MSE: 0.8115\n",
      "Epoch 7/25, Train MSE: 0.6514, Val MSE: 0.8143\n",
      "Epoch 8/25, Train MSE: 0.6384, Val MSE: 0.8087\n",
      "Epoch 9/25, Train MSE: 0.6269, Val MSE: 0.8099\n",
      "Epoch 10/25, Train MSE: 0.6161, Val MSE: 0.8118\n",
      "Epoch 11/25, Train MSE: 0.6055, Val MSE: 0.8084\n",
      "Epoch 12/25, Train MSE: 0.5946, Val MSE: 0.8092\n",
      "Epoch 13/25, Train MSE: 0.5839, Val MSE: 0.8076\n",
      "Epoch 14/25, Train MSE: 0.5721, Val MSE: 0.8097\n",
      "Epoch 15/25, Train MSE: 0.5587, Val MSE: 0.8068\n",
      "Epoch 16/25, Train MSE: 0.5446, Val MSE: 0.8057\n",
      "Epoch 17/25, Train MSE: 0.5296, Val MSE: 0.8071\n",
      "Epoch 18/25, Train MSE: 0.5137, Val MSE: 0.8038\n",
      "Epoch 19/25, Train MSE: 0.4975, Val MSE: 0.8027\n",
      "Epoch 20/25, Train MSE: 0.4805, Val MSE: 0.8012\n",
      "Epoch 21/25, Train MSE: 0.4628, Val MSE: 0.8024\n",
      "Epoch 22/25, Train MSE: 0.4449, Val MSE: 0.8008\n",
      "Epoch 23/25, Train MSE: 0.4267, Val MSE: 0.8019\n",
      "Epoch 24/25, Train MSE: 0.4080, Val MSE: 0.8020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:43:53,318] Trial 3 finished with value: 0.8027304317478758 and parameters: {'latent_dim': 28, 'learning_rate': 0.008089574860460297, 'regularization': 1.6073221934120177e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.3897, Val MSE: 0.8027\n",
      "Epoch 1/25, Train MSE: 0.8905, Val MSE: 0.9322\n",
      "Epoch 2/25, Train MSE: 0.8380, Val MSE: 0.8942\n",
      "Epoch 3/25, Train MSE: 0.8081, Val MSE: 0.8747\n",
      "Epoch 4/25, Train MSE: 0.7874, Val MSE: 0.8624\n",
      "Epoch 5/25, Train MSE: 0.7713, Val MSE: 0.8521\n",
      "Epoch 6/25, Train MSE: 0.7583, Val MSE: 0.8454\n",
      "Epoch 7/25, Train MSE: 0.7473, Val MSE: 0.8396\n",
      "Epoch 8/25, Train MSE: 0.7379, Val MSE: 0.8360\n",
      "Epoch 9/25, Train MSE: 0.7297, Val MSE: 0.8313\n",
      "Epoch 10/25, Train MSE: 0.7225, Val MSE: 0.8285\n",
      "Epoch 11/25, Train MSE: 0.7159, Val MSE: 0.8252\n",
      "Epoch 12/25, Train MSE: 0.7098, Val MSE: 0.8237\n",
      "Epoch 13/25, Train MSE: 0.7046, Val MSE: 0.8214\n",
      "Epoch 14/25, Train MSE: 0.6994, Val MSE: 0.8198\n",
      "Epoch 15/25, Train MSE: 0.6947, Val MSE: 0.8173\n",
      "Epoch 16/25, Train MSE: 0.6903, Val MSE: 0.8153\n",
      "Epoch 17/25, Train MSE: 0.6863, Val MSE: 0.8159\n",
      "Epoch 18/25, Train MSE: 0.6823, Val MSE: 0.8127\n",
      "Epoch 19/25, Train MSE: 0.6788, Val MSE: 0.8121\n",
      "Epoch 20/25, Train MSE: 0.6754, Val MSE: 0.8125\n",
      "Epoch 21/25, Train MSE: 0.6722, Val MSE: 0.8115\n",
      "Epoch 22/25, Train MSE: 0.6690, Val MSE: 0.8104\n",
      "Epoch 23/25, Train MSE: 0.6660, Val MSE: 0.8081\n",
      "Epoch 24/25, Train MSE: 0.6635, Val MSE: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:47:50,089] Trial 4 finished with value: 0.8058520326851342 and parameters: {'latent_dim': 48, 'learning_rate': 0.0028197316916477935, 'regularization': 0.089236012824263}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.6608, Val MSE: 0.8059\n",
      "Epoch 1/25, Train MSE: 1.0311, Val MSE: 1.0507\n",
      "Epoch 2/25, Train MSE: 1.0011, Val MSE: 1.0219\n",
      "Epoch 3/25, Train MSE: 0.9785, Val MSE: 1.0013\n",
      "Epoch 4/25, Train MSE: 0.9604, Val MSE: 0.9856\n",
      "Epoch 5/25, Train MSE: 0.9453, Val MSE: 0.9730\n",
      "Epoch 6/25, Train MSE: 0.9324, Val MSE: 0.9624\n",
      "Epoch 7/25, Train MSE: 0.9211, Val MSE: 0.9535\n",
      "Epoch 8/25, Train MSE: 0.9111, Val MSE: 0.9458\n",
      "Epoch 9/25, Train MSE: 0.9022, Val MSE: 0.9390\n",
      "Epoch 10/25, Train MSE: 0.8941, Val MSE: 0.9329\n",
      "Epoch 11/25, Train MSE: 0.8868, Val MSE: 0.9274\n",
      "Epoch 12/25, Train MSE: 0.8800, Val MSE: 0.9224\n",
      "Epoch 13/25, Train MSE: 0.8738, Val MSE: 0.9179\n",
      "Epoch 14/25, Train MSE: 0.8681, Val MSE: 0.9136\n",
      "Epoch 15/25, Train MSE: 0.8627, Val MSE: 0.9097\n",
      "Epoch 16/25, Train MSE: 0.8577, Val MSE: 0.9062\n",
      "Epoch 17/25, Train MSE: 0.8530, Val MSE: 0.9028\n",
      "Epoch 18/25, Train MSE: 0.8485, Val MSE: 0.8997\n",
      "Epoch 19/25, Train MSE: 0.8444, Val MSE: 0.8968\n",
      "Epoch 20/25, Train MSE: 0.8404, Val MSE: 0.8940\n",
      "Epoch 21/25, Train MSE: 0.8366, Val MSE: 0.8914\n",
      "Epoch 22/25, Train MSE: 0.8331, Val MSE: 0.8890\n",
      "Epoch 23/25, Train MSE: 0.8297, Val MSE: 0.8868\n",
      "Epoch 24/25, Train MSE: 0.8264, Val MSE: 0.8846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:51:46,576] Trial 5 finished with value: 0.882467561898411 and parameters: {'latent_dim': 44, 'learning_rate': 0.0002604460905842133, 'regularization': 0.00019856119275792037}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.8233, Val MSE: 0.8825\n",
      "Epoch 1/25, Train MSE: 0.8050, Val MSE: 0.8709\n",
      "Epoch 2/25, Train MSE: 0.7518, Val MSE: 0.8438\n",
      "Epoch 3/25, Train MSE: 0.7205, Val MSE: 0.8278\n",
      "Epoch 4/25, Train MSE: 0.6966, Val MSE: 0.8189\n",
      "Epoch 5/25, Train MSE: 0.6777, Val MSE: 0.8175\n",
      "Epoch 6/25, Train MSE: 0.6616, Val MSE: 0.8176\n",
      "Epoch 7/25, Train MSE: 0.6473, Val MSE: 0.8080\n",
      "Epoch 8/25, Train MSE: 0.6351, Val MSE: 0.8116\n",
      "Epoch 9/25, Train MSE: 0.6236, Val MSE: 0.8096\n",
      "Epoch 10/25, Train MSE: 0.6119, Val MSE: 0.8111\n",
      "Epoch 11/25, Train MSE: 0.6009, Val MSE: 0.8092\n",
      "Epoch 12/25, Train MSE: 0.5897, Val MSE: 0.8072\n",
      "Epoch 13/25, Train MSE: 0.5780, Val MSE: 0.8108\n",
      "Epoch 14/25, Train MSE: 0.5653, Val MSE: 0.8086\n",
      "Epoch 15/25, Train MSE: 0.5511, Val MSE: 0.8017\n",
      "Epoch 16/25, Train MSE: 0.5355, Val MSE: 0.8040\n",
      "Epoch 17/25, Train MSE: 0.5185, Val MSE: 0.8040\n",
      "Epoch 18/25, Train MSE: 0.5012, Val MSE: 0.8063\n",
      "Epoch 19/25, Train MSE: 0.4833, Val MSE: 0.8049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:54:54,645] Trial 6 finished with value: 0.803030232453534 and parameters: {'latent_dim': 29, 'learning_rate': 0.008425347072925658, 'regularization': 1.1149060035905202e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Train MSE: 0.4650, Val MSE: 0.8030\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.6598, Val MSE: 0.8629\n",
      "Epoch 2/25, Train MSE: 0.5793, Val MSE: 0.8456\n",
      "Epoch 3/25, Train MSE: 0.4497, Val MSE: 0.8324\n",
      "Epoch 4/25, Train MSE: 0.2918, Val MSE: 0.8278\n",
      "Epoch 5/25, Train MSE: 0.1848, Val MSE: 0.8317\n",
      "Epoch 6/25, Train MSE: 0.1204, Val MSE: 0.8482\n",
      "Epoch 7/25, Train MSE: 0.0851, Val MSE: 0.8548\n",
      "Epoch 8/25, Train MSE: 0.0650, Val MSE: 0.8697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 19:56:20,915] Trial 7 finished with value: 0.8792038208009065 and parameters: {'latent_dim': 38, 'learning_rate': 0.08184702253869122, 'regularization': 0.009077600049022558}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Train MSE: 0.0512, Val MSE: 0.8792\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.9823, Val MSE: 1.0041\n",
      "Epoch 2/25, Train MSE: 0.9370, Val MSE: 0.9655\n",
      "Epoch 3/25, Train MSE: 0.9071, Val MSE: 0.9420\n",
      "Epoch 4/25, Train MSE: 0.8851, Val MSE: 0.9258\n",
      "Epoch 5/25, Train MSE: 0.8678, Val MSE: 0.9129\n",
      "Epoch 6/25, Train MSE: 0.8536, Val MSE: 0.9026\n",
      "Epoch 7/25, Train MSE: 0.8417, Val MSE: 0.8947\n",
      "Epoch 8/25, Train MSE: 0.8314, Val MSE: 0.8877\n",
      "Epoch 9/25, Train MSE: 0.8224, Val MSE: 0.8816\n",
      "Epoch 10/25, Train MSE: 0.8144, Val MSE: 0.8766\n",
      "Epoch 11/25, Train MSE: 0.8072, Val MSE: 0.8716\n",
      "Epoch 12/25, Train MSE: 0.8007, Val MSE: 0.8678\n",
      "Epoch 13/25, Train MSE: 0.7947, Val MSE: 0.8643\n",
      "Epoch 14/25, Train MSE: 0.7891, Val MSE: 0.8609\n",
      "Epoch 15/25, Train MSE: 0.7839, Val MSE: 0.8580\n",
      "Epoch 16/25, Train MSE: 0.7791, Val MSE: 0.8551\n",
      "Epoch 17/25, Train MSE: 0.7745, Val MSE: 0.8525\n",
      "Epoch 18/25, Train MSE: 0.7702, Val MSE: 0.8502\n",
      "Epoch 19/25, Train MSE: 0.7661, Val MSE: 0.8483\n",
      "Epoch 20/25, Train MSE: 0.7622, Val MSE: 0.8464\n",
      "Epoch 21/25, Train MSE: 0.7585, Val MSE: 0.8443\n",
      "Epoch 22/25, Train MSE: 0.7550, Val MSE: 0.8427\n",
      "Epoch 23/25, Train MSE: 0.7516, Val MSE: 0.8409\n",
      "Epoch 24/25, Train MSE: 0.7483, Val MSE: 0.8396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:00:17,911] Trial 8 finished with value: 0.8382263229985745 and parameters: {'latent_dim': 8, 'learning_rate': 0.0007346183927711892, 'regularization': 0.003853731670468113}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.7452, Val MSE: 0.8382\n",
      "Epoch 1/25, Train MSE: 0.9927, Val MSE: 1.0142\n",
      "Epoch 2/25, Train MSE: 0.9500, Val MSE: 0.9769\n",
      "Epoch 3/25, Train MSE: 0.9210, Val MSE: 0.9537\n",
      "Epoch 4/25, Train MSE: 0.8993, Val MSE: 0.9370\n",
      "Epoch 5/25, Train MSE: 0.8821, Val MSE: 0.9245\n",
      "Epoch 6/25, Train MSE: 0.8679, Val MSE: 0.9139\n",
      "Epoch 7/25, Train MSE: 0.8559, Val MSE: 0.9054\n",
      "Epoch 8/25, Train MSE: 0.8456, Val MSE: 0.8982\n",
      "Epoch 9/25, Train MSE: 0.8365, Val MSE: 0.8919\n",
      "Epoch 10/25, Train MSE: 0.8284, Val MSE: 0.8862\n",
      "Epoch 11/25, Train MSE: 0.8211, Val MSE: 0.8812\n",
      "Epoch 12/25, Train MSE: 0.8145, Val MSE: 0.8770\n",
      "Epoch 13/25, Train MSE: 0.8085, Val MSE: 0.8731\n",
      "Epoch 14/25, Train MSE: 0.8029, Val MSE: 0.8698\n",
      "Epoch 15/25, Train MSE: 0.7977, Val MSE: 0.8667\n",
      "Epoch 16/25, Train MSE: 0.7928, Val MSE: 0.8638\n",
      "Epoch 17/25, Train MSE: 0.7883, Val MSE: 0.8611\n",
      "Epoch 18/25, Train MSE: 0.7840, Val MSE: 0.8585\n",
      "Epoch 19/25, Train MSE: 0.7799, Val MSE: 0.8562\n",
      "Epoch 20/25, Train MSE: 0.7760, Val MSE: 0.8542\n",
      "Epoch 21/25, Train MSE: 0.7723, Val MSE: 0.8522\n",
      "Epoch 22/25, Train MSE: 0.7688, Val MSE: 0.8503\n",
      "Epoch 23/25, Train MSE: 0.7654, Val MSE: 0.8483\n",
      "Epoch 24/25, Train MSE: 0.7622, Val MSE: 0.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:04:15,116] Trial 9 finished with value: 0.845200437473686 and parameters: {'latent_dim': 11, 'learning_rate': 0.0006088071805980583, 'regularization': 1.3950149819846142e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.7591, Val MSE: 0.8452\n",
      "Epoch 1/25, Train MSE: 0.7592, Val MSE: 0.8507\n",
      "Epoch 2/25, Train MSE: 0.7039, Val MSE: 0.8286\n",
      "Epoch 3/25, Train MSE: 0.6684, Val MSE: 0.8126\n",
      "Epoch 4/25, Train MSE: 0.6436, Val MSE: 0.8103\n",
      "Epoch 5/25, Train MSE: 0.6205, Val MSE: 0.8096\n",
      "Epoch 6/25, Train MSE: 0.6012, Val MSE: 0.8068\n",
      "Epoch 7/25, Train MSE: 0.5823, Val MSE: 0.8110\n",
      "Epoch 8/25, Train MSE: 0.5598, Val MSE: 0.8063\n",
      "Epoch 9/25, Train MSE: 0.5339, Val MSE: 0.8098\n",
      "Epoch 10/25, Train MSE: 0.5046, Val MSE: 0.8068\n",
      "Epoch 11/25, Train MSE: 0.4734, Val MSE: 0.8047\n",
      "Epoch 12/25, Train MSE: 0.4399, Val MSE: 0.8069\n",
      "Epoch 13/25, Train MSE: 0.4047, Val MSE: 0.8057\n",
      "Epoch 14/25, Train MSE: 0.3698, Val MSE: 0.8036\n",
      "Epoch 15/25, Train MSE: 0.3371, Val MSE: 0.8038\n",
      "Epoch 16/25, Train MSE: 0.3068, Val MSE: 0.8067\n",
      "Epoch 17/25, Train MSE: 0.2796, Val MSE: 0.8112\n",
      "Epoch 18/25, Train MSE: 0.2555, Val MSE: 0.8140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:07:13,876] Trial 10 finished with value: 0.8191905892597536 and parameters: {'latent_dim': 23, 'learning_rate': 0.015730564470881538, 'regularization': 7.782764776812118e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Train MSE: 0.2344, Val MSE: 0.8192\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7935, Val MSE: 0.8593\n",
      "Epoch 2/25, Train MSE: 0.7406, Val MSE: 0.8341\n",
      "Epoch 3/25, Train MSE: 0.7078, Val MSE: 0.8269\n",
      "Epoch 4/25, Train MSE: 0.6840, Val MSE: 0.8204\n",
      "Epoch 5/25, Train MSE: 0.6648, Val MSE: 0.8198\n",
      "Epoch 6/25, Train MSE: 0.6481, Val MSE: 0.8136\n",
      "Epoch 7/25, Train MSE: 0.6335, Val MSE: 0.8105\n",
      "Epoch 8/25, Train MSE: 0.6208, Val MSE: 0.8139\n",
      "Epoch 9/25, Train MSE: 0.6077, Val MSE: 0.8078\n",
      "Epoch 10/25, Train MSE: 0.5955, Val MSE: 0.8059\n",
      "Epoch 11/25, Train MSE: 0.5831, Val MSE: 0.8069\n",
      "Epoch 12/25, Train MSE: 0.5696, Val MSE: 0.8092\n",
      "Epoch 13/25, Train MSE: 0.5547, Val MSE: 0.8105\n",
      "Epoch 14/25, Train MSE: 0.5367, Val MSE: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:09:32,591] Trial 11 finished with value: 0.810213931555411 and parameters: {'latent_dim': 28, 'learning_rate': 0.009786529863124538, 'regularization': 1.0338129895477671e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Train MSE: 0.5170, Val MSE: 0.8102\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7952, Val MSE: 0.8658\n",
      "Epoch 2/25, Train MSE: 0.7428, Val MSE: 0.8407\n",
      "Epoch 3/25, Train MSE: 0.7106, Val MSE: 0.8281\n",
      "Epoch 4/25, Train MSE: 0.6861, Val MSE: 0.8238\n",
      "Epoch 5/25, Train MSE: 0.6669, Val MSE: 0.8161\n",
      "Epoch 6/25, Train MSE: 0.6509, Val MSE: 0.8135\n",
      "Epoch 7/25, Train MSE: 0.6358, Val MSE: 0.8119\n",
      "Epoch 8/25, Train MSE: 0.6225, Val MSE: 0.8091\n",
      "Epoch 9/25, Train MSE: 0.6094, Val MSE: 0.8063\n",
      "Epoch 10/25, Train MSE: 0.5971, Val MSE: 0.8031\n",
      "Epoch 11/25, Train MSE: 0.5841, Val MSE: 0.8091\n",
      "Epoch 12/25, Train MSE: 0.5699, Val MSE: 0.8044\n",
      "Epoch 13/25, Train MSE: 0.5531, Val MSE: 0.8094\n",
      "Epoch 14/25, Train MSE: 0.5360, Val MSE: 0.8068\n",
      "Epoch 15/25, Train MSE: 0.5169, Val MSE: 0.8030\n",
      "Epoch 16/25, Train MSE: 0.4975, Val MSE: 0.8034\n",
      "Epoch 17/25, Train MSE: 0.4771, Val MSE: 0.8049\n",
      "Epoch 18/25, Train MSE: 0.4562, Val MSE: 0.8039\n",
      "Epoch 19/25, Train MSE: 0.4344, Val MSE: 0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:12:39,773] Trial 12 finished with value: 0.8042096430050315 and parameters: {'latent_dim': 29, 'learning_rate': 0.009522140875606162, 'regularization': 5.105172242413599e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/25, Train MSE: 0.4125, Val MSE: 0.8042\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7250, Val MSE: 0.8400\n",
      "Epoch 2/25, Train MSE: 0.6684, Val MSE: 0.8274\n",
      "Epoch 3/25, Train MSE: 0.6288, Val MSE: 0.8196\n",
      "Epoch 4/25, Train MSE: 0.5970, Val MSE: 0.8197\n",
      "Epoch 5/25, Train MSE: 0.5650, Val MSE: 0.8192\n",
      "Epoch 6/25, Train MSE: 0.5245, Val MSE: 0.8137\n",
      "Epoch 7/25, Train MSE: 0.4772, Val MSE: 0.8022\n",
      "Epoch 8/25, Train MSE: 0.4253, Val MSE: 0.8099\n",
      "Epoch 9/25, Train MSE: 0.3722, Val MSE: 0.8035\n",
      "Epoch 10/25, Train MSE: 0.3219, Val MSE: 0.8145\n",
      "Epoch 11/25, Train MSE: 0.2792, Val MSE: 0.8196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:14:31,856] Trial 13 finished with value: 0.8272396471859628 and parameters: {'latent_dim': 22, 'learning_rate': 0.02499567606991659, 'regularization': 9.865533504646962e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25, Train MSE: 0.2436, Val MSE: 0.8272\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.8467, Val MSE: 0.9004\n",
      "Epoch 2/25, Train MSE: 0.7944, Val MSE: 0.8617\n",
      "Epoch 3/25, Train MSE: 0.7635, Val MSE: 0.8473\n",
      "Epoch 4/25, Train MSE: 0.7416, Val MSE: 0.8401\n",
      "Epoch 5/25, Train MSE: 0.7239, Val MSE: 0.8305\n",
      "Epoch 6/25, Train MSE: 0.7094, Val MSE: 0.8248\n",
      "Epoch 7/25, Train MSE: 0.6966, Val MSE: 0.8216\n",
      "Epoch 8/25, Train MSE: 0.6854, Val MSE: 0.8205\n",
      "Epoch 9/25, Train MSE: 0.6754, Val MSE: 0.8156\n",
      "Epoch 10/25, Train MSE: 0.6662, Val MSE: 0.8143\n",
      "Epoch 11/25, Train MSE: 0.6578, Val MSE: 0.8127\n",
      "Epoch 12/25, Train MSE: 0.6499, Val MSE: 0.8117\n",
      "Epoch 13/25, Train MSE: 0.6424, Val MSE: 0.8121\n",
      "Epoch 14/25, Train MSE: 0.6352, Val MSE: 0.8063\n",
      "Epoch 15/25, Train MSE: 0.6285, Val MSE: 0.8088\n",
      "Epoch 16/25, Train MSE: 0.6219, Val MSE: 0.8107\n",
      "Epoch 17/25, Train MSE: 0.6156, Val MSE: 0.8095\n",
      "Epoch 18/25, Train MSE: 0.6094, Val MSE: 0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:17:30,396] Trial 14 finished with value: 0.8064851650754148 and parameters: {'latent_dim': 35, 'learning_rate': 0.004785642879822109, 'regularization': 0.0009269262848247135}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/25, Train MSE: 0.6031, Val MSE: 0.8065\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 1.0534, Val MSE: 1.0733\n",
      "Epoch 2/25, Train MSE: 1.0354, Val MSE: 1.0553\n",
      "Epoch 3/25, Train MSE: 1.0202, Val MSE: 1.0404\n",
      "Epoch 4/25, Train MSE: 1.0071, Val MSE: 1.0279\n",
      "Epoch 5/25, Train MSE: 0.9957, Val MSE: 1.0172\n",
      "Epoch 6/25, Train MSE: 0.9855, Val MSE: 1.0078\n",
      "Epoch 7/25, Train MSE: 0.9763, Val MSE: 0.9996\n",
      "Epoch 8/25, Train MSE: 0.9679, Val MSE: 0.9923\n",
      "Epoch 9/25, Train MSE: 0.9603, Val MSE: 0.9857\n",
      "Epoch 10/25, Train MSE: 0.9532, Val MSE: 0.9798\n",
      "Epoch 11/25, Train MSE: 0.9467, Val MSE: 0.9744\n",
      "Epoch 12/25, Train MSE: 0.9406, Val MSE: 0.9694\n",
      "Epoch 13/25, Train MSE: 0.9349, Val MSE: 0.9647\n",
      "Epoch 14/25, Train MSE: 0.9295, Val MSE: 0.9605\n",
      "Epoch 15/25, Train MSE: 0.9245, Val MSE: 0.9565\n",
      "Epoch 16/25, Train MSE: 0.9197, Val MSE: 0.9527\n",
      "Epoch 17/25, Train MSE: 0.9152, Val MSE: 0.9492\n",
      "Epoch 18/25, Train MSE: 0.9109, Val MSE: 0.9459\n",
      "Epoch 19/25, Train MSE: 0.9068, Val MSE: 0.9427\n",
      "Epoch 20/25, Train MSE: 0.9029, Val MSE: 0.9398\n",
      "Epoch 21/25, Train MSE: 0.8991, Val MSE: 0.9369\n",
      "Epoch 22/25, Train MSE: 0.8956, Val MSE: 0.9343\n",
      "Epoch 23/25, Train MSE: 0.8921, Val MSE: 0.9317\n",
      "Epoch 24/25, Train MSE: 0.8888, Val MSE: 0.9292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:21:25,210] Trial 15 finished with value: 0.9268850476793126 and parameters: {'latent_dim': 19, 'learning_rate': 0.0001165723794724678, 'regularization': 3.494141225407038e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.8857, Val MSE: 0.9269\n",
      "Epoch 1/25, Train MSE: 0.6692, Val MSE: 0.8365\n",
      "Epoch 2/25, Train MSE: 0.5684, Val MSE: 0.8393\n",
      "Epoch 3/25, Train MSE: 0.4337, Val MSE: 0.8296\n",
      "Epoch 4/25, Train MSE: 0.2878, Val MSE: 0.8198\n",
      "Epoch 5/25, Train MSE: 0.1838, Val MSE: 0.8317\n",
      "Epoch 6/25, Train MSE: 0.1232, Val MSE: 0.8523\n",
      "Epoch 7/25, Train MSE: 0.0853, Val MSE: 0.8717\n",
      "Epoch 8/25, Train MSE: 0.0622, Val MSE: 0.8890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:22:49,521] Trial 16 finished with value: 0.9109075853853101 and parameters: {'latent_dim': 34, 'learning_rate': 0.07118383144680686, 'regularization': 1.0518529155194852e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Train MSE: 0.0464, Val MSE: 0.9109\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.8122, Val MSE: 0.8771\n",
      "Epoch 2/25, Train MSE: 0.7599, Val MSE: 0.8476\n",
      "Epoch 3/25, Train MSE: 0.7286, Val MSE: 0.8303\n",
      "Epoch 4/25, Train MSE: 0.7054, Val MSE: 0.8230\n",
      "Epoch 5/25, Train MSE: 0.6869, Val MSE: 0.8220\n",
      "Epoch 6/25, Train MSE: 0.6711, Val MSE: 0.8177\n",
      "Epoch 7/25, Train MSE: 0.6580, Val MSE: 0.8138\n",
      "Epoch 8/25, Train MSE: 0.6457, Val MSE: 0.8136\n",
      "Epoch 9/25, Train MSE: 0.6348, Val MSE: 0.8096\n",
      "Epoch 10/25, Train MSE: 0.6246, Val MSE: 0.8103\n",
      "Epoch 11/25, Train MSE: 0.6150, Val MSE: 0.8111\n",
      "Epoch 12/25, Train MSE: 0.6055, Val MSE: 0.8092\n",
      "Epoch 13/25, Train MSE: 0.5966, Val MSE: 0.8117\n",
      "Epoch 14/25, Train MSE: 0.5871, Val MSE: 0.8082\n",
      "Epoch 15/25, Train MSE: 0.5776, Val MSE: 0.8095\n",
      "Epoch 16/25, Train MSE: 0.5674, Val MSE: 0.8049\n",
      "Epoch 17/25, Train MSE: 0.5563, Val MSE: 0.8084\n",
      "Epoch 18/25, Train MSE: 0.5443, Val MSE: 0.8054\n",
      "Epoch 19/25, Train MSE: 0.5315, Val MSE: 0.8092\n",
      "Epoch 20/25, Train MSE: 0.5181, Val MSE: 0.8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:26:06,047] Trial 17 finished with value: 0.8061654499922987 and parameters: {'latent_dim': 17, 'learning_rate': 0.007550015391444936, 'regularization': 0.00028463727782866786}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/25, Train MSE: 0.5043, Val MSE: 0.8062\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.9071, Val MSE: 0.9432\n",
      "Epoch 2/25, Train MSE: 0.8538, Val MSE: 0.9050\n",
      "Epoch 3/25, Train MSE: 0.8225, Val MSE: 0.8831\n",
      "Epoch 4/25, Train MSE: 0.8008, Val MSE: 0.8675\n",
      "Epoch 5/25, Train MSE: 0.7841, Val MSE: 0.8587\n",
      "Epoch 6/25, Train MSE: 0.7702, Val MSE: 0.8508\n",
      "Epoch 7/25, Train MSE: 0.7587, Val MSE: 0.8444\n",
      "Epoch 8/25, Train MSE: 0.7484, Val MSE: 0.8408\n",
      "Epoch 9/25, Train MSE: 0.7392, Val MSE: 0.8357\n",
      "Epoch 10/25, Train MSE: 0.7310, Val MSE: 0.8346\n",
      "Epoch 11/25, Train MSE: 0.7235, Val MSE: 0.8295\n",
      "Epoch 12/25, Train MSE: 0.7166, Val MSE: 0.8270\n",
      "Epoch 13/25, Train MSE: 0.7102, Val MSE: 0.8256\n",
      "Epoch 14/25, Train MSE: 0.7042, Val MSE: 0.8244\n",
      "Epoch 15/25, Train MSE: 0.6986, Val MSE: 0.8216\n",
      "Epoch 16/25, Train MSE: 0.6932, Val MSE: 0.8196\n",
      "Epoch 17/25, Train MSE: 0.6881, Val MSE: 0.8189\n",
      "Epoch 18/25, Train MSE: 0.6834, Val MSE: 0.8180\n",
      "Epoch 19/25, Train MSE: 0.6787, Val MSE: 0.8163\n",
      "Epoch 20/25, Train MSE: 0.6744, Val MSE: 0.8152\n",
      "Epoch 21/25, Train MSE: 0.6702, Val MSE: 0.8141\n",
      "Epoch 22/25, Train MSE: 0.6661, Val MSE: 0.8144\n",
      "Epoch 23/25, Train MSE: 0.6622, Val MSE: 0.8119\n",
      "Epoch 24/25, Train MSE: 0.6584, Val MSE: 0.8117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:29:59,082] Trial 18 finished with value: 0.8103232219348379 and parameters: {'latent_dim': 40, 'learning_rate': 0.0021940670010980987, 'regularization': 0.0034063238651260964}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.6548, Val MSE: 0.8103\n",
      "Epoch 1/25, Train MSE: 0.7122, Val MSE: 0.8382\n",
      "Epoch 2/25, Train MSE: 0.6555, Val MSE: 0.8118\n",
      "Epoch 3/25, Train MSE: 0.6210, Val MSE: 0.8171\n",
      "Epoch 4/25, Train MSE: 0.5936, Val MSE: 0.8099\n",
      "Epoch 5/25, Train MSE: 0.5657, Val MSE: 0.8045\n",
      "Epoch 6/25, Train MSE: 0.5311, Val MSE: 0.7996\n",
      "Epoch 7/25, Train MSE: 0.4858, Val MSE: 0.7948\n",
      "Epoch 8/25, Train MSE: 0.4298, Val MSE: 0.7972\n",
      "Epoch 9/25, Train MSE: 0.3732, Val MSE: 0.7945\n",
      "Epoch 10/25, Train MSE: 0.3197, Val MSE: 0.7995\n",
      "Epoch 11/25, Train MSE: 0.2721, Val MSE: 0.8001\n",
      "Epoch 12/25, Train MSE: 0.2336, Val MSE: 0.8000\n",
      "Epoch 13/25, Train MSE: 0.2023, Val MSE: 0.8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:32:12,024] Trial 19 finished with value: 0.8137382528465514 and parameters: {'latent_dim': 31, 'learning_rate': 0.032795916950287135, 'regularization': 0.01987612327608112}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Train MSE: 0.1759, Val MSE: 0.8137\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.9421, Val MSE: 0.9698\n",
      "Epoch 2/25, Train MSE: 0.8906, Val MSE: 0.9305\n",
      "Epoch 3/25, Train MSE: 0.8592, Val MSE: 0.9074\n",
      "Epoch 4/25, Train MSE: 0.8369, Val MSE: 0.8924\n",
      "Epoch 5/25, Train MSE: 0.8199, Val MSE: 0.8800\n",
      "Epoch 6/25, Train MSE: 0.8060, Val MSE: 0.8707\n",
      "Epoch 7/25, Train MSE: 0.7944, Val MSE: 0.8645\n",
      "Epoch 8/25, Train MSE: 0.7843, Val MSE: 0.8581\n",
      "Epoch 9/25, Train MSE: 0.7754, Val MSE: 0.8531\n",
      "Epoch 10/25, Train MSE: 0.7675, Val MSE: 0.8486\n",
      "Epoch 11/25, Train MSE: 0.7602, Val MSE: 0.8458\n",
      "Epoch 12/25, Train MSE: 0.7536, Val MSE: 0.8426\n",
      "Epoch 13/25, Train MSE: 0.7474, Val MSE: 0.8399\n",
      "Epoch 14/25, Train MSE: 0.7417, Val MSE: 0.8372\n",
      "Epoch 15/25, Train MSE: 0.7363, Val MSE: 0.8348\n",
      "Epoch 16/25, Train MSE: 0.7312, Val MSE: 0.8329\n",
      "Epoch 17/25, Train MSE: 0.7265, Val MSE: 0.8307\n",
      "Epoch 18/25, Train MSE: 0.7219, Val MSE: 0.8291\n",
      "Epoch 19/25, Train MSE: 0.7176, Val MSE: 0.8280\n",
      "Epoch 20/25, Train MSE: 0.7135, Val MSE: 0.8258\n",
      "Epoch 21/25, Train MSE: 0.7095, Val MSE: 0.8249\n",
      "Epoch 22/25, Train MSE: 0.7057, Val MSE: 0.8239\n",
      "Epoch 23/25, Train MSE: 0.7021, Val MSE: 0.8228\n",
      "Epoch 24/25, Train MSE: 0.6985, Val MSE: 0.8218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:36:06,713] Trial 20 finished with value: 0.8212480716888857 and parameters: {'latent_dim': 24, 'learning_rate': 0.0013626506961453655, 'regularization': 2.7741186232506962e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.6952, Val MSE: 0.8212\n",
      "Epoch 1/25, Train MSE: 0.7909, Val MSE: 0.8601\n",
      "Epoch 2/25, Train MSE: 0.7384, Val MSE: 0.8334\n",
      "Epoch 3/25, Train MSE: 0.7062, Val MSE: 0.8303\n",
      "Epoch 4/25, Train MSE: 0.6823, Val MSE: 0.8193\n",
      "Epoch 5/25, Train MSE: 0.6625, Val MSE: 0.8181\n",
      "Epoch 6/25, Train MSE: 0.6454, Val MSE: 0.8132\n",
      "Epoch 7/25, Train MSE: 0.6308, Val MSE: 0.8071\n",
      "Epoch 8/25, Train MSE: 0.6171, Val MSE: 0.8096\n",
      "Epoch 9/25, Train MSE: 0.6043, Val MSE: 0.8070\n",
      "Epoch 10/25, Train MSE: 0.5910, Val MSE: 0.8044\n",
      "Epoch 11/25, Train MSE: 0.5769, Val MSE: 0.8067\n",
      "Epoch 12/25, Train MSE: 0.5616, Val MSE: 0.8068\n",
      "Epoch 13/25, Train MSE: 0.5440, Val MSE: 0.8074\n",
      "Epoch 14/25, Train MSE: 0.5246, Val MSE: 0.8049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:38:28,317] Trial 21 finished with value: 0.8048383961159468 and parameters: {'latent_dim': 29, 'learning_rate': 0.010051568086820725, 'regularization': 6.977777443370939e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25, Train MSE: 0.5038, Val MSE: 0.8048\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7691, Val MSE: 0.8442\n",
      "Epoch 2/25, Train MSE: 0.7143, Val MSE: 0.8312\n",
      "Epoch 3/25, Train MSE: 0.6804, Val MSE: 0.8123\n",
      "Epoch 4/25, Train MSE: 0.6545, Val MSE: 0.8139\n",
      "Epoch 5/25, Train MSE: 0.6337, Val MSE: 0.8088\n",
      "Epoch 6/25, Train MSE: 0.6148, Val MSE: 0.8110\n",
      "Epoch 7/25, Train MSE: 0.5970, Val MSE: 0.8070\n",
      "Epoch 8/25, Train MSE: 0.5783, Val MSE: 0.8108\n",
      "Epoch 9/25, Train MSE: 0.5580, Val MSE: 0.8017\n",
      "Epoch 10/25, Train MSE: 0.5329, Val MSE: 0.8075\n",
      "Epoch 11/25, Train MSE: 0.5049, Val MSE: 0.8034\n",
      "Epoch 12/25, Train MSE: 0.4752, Val MSE: 0.8085\n",
      "Epoch 13/25, Train MSE: 0.4435, Val MSE: 0.8047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:40:39,182] Trial 22 finished with value: 0.8030409887372694 and parameters: {'latent_dim': 32, 'learning_rate': 0.013760816418327613, 'regularization': 3.818983867405986e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Train MSE: 0.4101, Val MSE: 0.8030\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7477, Val MSE: 0.8403\n",
      "Epoch 2/25, Train MSE: 0.6936, Val MSE: 0.8209\n",
      "Epoch 3/25, Train MSE: 0.6577, Val MSE: 0.8182\n",
      "Epoch 4/25, Train MSE: 0.6291, Val MSE: 0.8125\n",
      "Epoch 5/25, Train MSE: 0.6050, Val MSE: 0.8118\n",
      "Epoch 6/25, Train MSE: 0.5809, Val MSE: 0.8165\n",
      "Epoch 7/25, Train MSE: 0.5519, Val MSE: 0.8090\n",
      "Epoch 8/25, Train MSE: 0.5168, Val MSE: 0.8088\n",
      "Epoch 9/25, Train MSE: 0.4773, Val MSE: 0.8026\n",
      "Epoch 10/25, Train MSE: 0.4348, Val MSE: 0.8103\n",
      "Epoch 11/25, Train MSE: 0.3906, Val MSE: 0.8025\n",
      "Epoch 12/25, Train MSE: 0.3495, Val MSE: 0.8064\n",
      "Epoch 13/25, Train MSE: 0.3111, Val MSE: 0.8076\n",
      "Epoch 14/25, Train MSE: 0.2771, Val MSE: 0.8103\n",
      "Epoch 15/25, Train MSE: 0.2470, Val MSE: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:43:08,967] Trial 23 finished with value: 0.8194814121717171 and parameters: {'latent_dim': 33, 'learning_rate': 0.018167765926627312, 'regularization': 2.80527909155703e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25, Train MSE: 0.2204, Val MSE: 0.8195\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.8347, Val MSE: 0.8900\n",
      "Epoch 2/25, Train MSE: 0.7819, Val MSE: 0.8586\n",
      "Epoch 3/25, Train MSE: 0.7513, Val MSE: 0.8433\n",
      "Epoch 4/25, Train MSE: 0.7288, Val MSE: 0.8321\n",
      "Epoch 5/25, Train MSE: 0.7109, Val MSE: 0.8268\n",
      "Epoch 6/25, Train MSE: 0.6959, Val MSE: 0.8175\n",
      "Epoch 7/25, Train MSE: 0.6827, Val MSE: 0.8150\n",
      "Epoch 8/25, Train MSE: 0.6712, Val MSE: 0.8160\n",
      "Epoch 9/25, Train MSE: 0.6609, Val MSE: 0.8156\n",
      "Epoch 10/25, Train MSE: 0.6511, Val MSE: 0.8115\n",
      "Epoch 11/25, Train MSE: 0.6420, Val MSE: 0.8114\n",
      "Epoch 12/25, Train MSE: 0.6336, Val MSE: 0.8098\n",
      "Epoch 13/25, Train MSE: 0.6255, Val MSE: 0.8092\n",
      "Epoch 14/25, Train MSE: 0.6176, Val MSE: 0.8086\n",
      "Epoch 15/25, Train MSE: 0.6099, Val MSE: 0.8089\n",
      "Epoch 16/25, Train MSE: 0.6022, Val MSE: 0.8075\n",
      "Epoch 17/25, Train MSE: 0.5942, Val MSE: 0.8071\n",
      "Epoch 18/25, Train MSE: 0.5857, Val MSE: 0.8069\n",
      "Epoch 19/25, Train MSE: 0.5769, Val MSE: 0.8058\n",
      "Epoch 20/25, Train MSE: 0.5676, Val MSE: 0.8054\n",
      "Epoch 21/25, Train MSE: 0.5574, Val MSE: 0.8032\n",
      "Epoch 22/25, Train MSE: 0.5466, Val MSE: 0.8037\n",
      "Epoch 23/25, Train MSE: 0.5353, Val MSE: 0.8024\n",
      "Epoch 24/25, Train MSE: 0.5234, Val MSE: 0.8054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:47:02,146] Trial 24 finished with value: 0.803979635359419 and parameters: {'latent_dim': 38, 'learning_rate': 0.005626641101542891, 'regularization': 0.00015512389276424682}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.5112, Val MSE: 0.8040\n",
      "Epoch 1/25, Train MSE: 0.7587, Val MSE: 0.8415\n",
      "Epoch 2/25, Train MSE: 0.7028, Val MSE: 0.8262\n",
      "Epoch 3/25, Train MSE: 0.6684, Val MSE: 0.8158\n",
      "Epoch 4/25, Train MSE: 0.6418, Val MSE: 0.8139\n",
      "Epoch 5/25, Train MSE: 0.6202, Val MSE: 0.8159\n",
      "Epoch 6/25, Train MSE: 0.6016, Val MSE: 0.8130\n",
      "Epoch 7/25, Train MSE: 0.5837, Val MSE: 0.8121\n",
      "Epoch 8/25, Train MSE: 0.5658, Val MSE: 0.8143\n",
      "Epoch 9/25, Train MSE: 0.5452, Val MSE: 0.8047\n",
      "Epoch 10/25, Train MSE: 0.5184, Val MSE: 0.8096\n",
      "Epoch 11/25, Train MSE: 0.4877, Val MSE: 0.8126\n",
      "Epoch 12/25, Train MSE: 0.4549, Val MSE: 0.8114\n",
      "Epoch 13/25, Train MSE: 0.4205, Val MSE: 0.8135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:49:12,771] Trial 25 finished with value: 0.8077089298794584 and parameters: {'latent_dim': 17, 'learning_rate': 0.015957128119986147, 'regularization': 1.8509618583573706e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25, Train MSE: 0.3873, Val MSE: 0.8077\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.7094, Val MSE: 0.8338\n",
      "Epoch 2/25, Train MSE: 0.6462, Val MSE: 0.8100\n",
      "Epoch 3/25, Train MSE: 0.6072, Val MSE: 0.8174\n",
      "Epoch 4/25, Train MSE: 0.5633, Val MSE: 0.8179\n",
      "Epoch 5/25, Train MSE: 0.5087, Val MSE: 0.8115\n",
      "Epoch 6/25, Train MSE: 0.4403, Val MSE: 0.8060\n",
      "Epoch 7/25, Train MSE: 0.3691, Val MSE: 0.8148\n",
      "Epoch 8/25, Train MSE: 0.3041, Val MSE: 0.8104\n",
      "Epoch 9/25, Train MSE: 0.2519, Val MSE: 0.8174\n",
      "Epoch 10/25, Train MSE: 0.2098, Val MSE: 0.8298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:50:51,922] Trial 26 finished with value: 0.8420574493357069 and parameters: {'latent_dim': 25, 'learning_rate': 0.032420965854927784, 'regularization': 0.0005751861652980218}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25, Train MSE: 0.1771, Val MSE: 0.8421\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.6822, Val MSE: 0.8393\n",
      "Epoch 2/25, Train MSE: 0.6090, Val MSE: 0.8238\n",
      "Epoch 3/25, Train MSE: 0.5361, Val MSE: 0.8238\n",
      "Epoch 4/25, Train MSE: 0.4228, Val MSE: 0.8209\n",
      "Epoch 5/25, Train MSE: 0.3065, Val MSE: 0.8216\n",
      "Epoch 6/25, Train MSE: 0.2159, Val MSE: 0.8215\n",
      "Epoch 7/25, Train MSE: 0.1508, Val MSE: 0.8321\n",
      "Epoch 8/25, Train MSE: 0.1087, Val MSE: 0.8493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:52:17,058] Trial 27 finished with value: 0.8630876903529715 and parameters: {'latent_dim': 43, 'learning_rate': 0.050248268974003016, 'regularization': 4.82734028509788e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25, Train MSE: 0.0781, Val MSE: 0.8631\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.8300, Val MSE: 0.8853\n",
      "Epoch 2/25, Train MSE: 0.7770, Val MSE: 0.8526\n",
      "Epoch 3/25, Train MSE: 0.7460, Val MSE: 0.8391\n",
      "Epoch 4/25, Train MSE: 0.7237, Val MSE: 0.8255\n",
      "Epoch 5/25, Train MSE: 0.7055, Val MSE: 0.8248\n",
      "Epoch 6/25, Train MSE: 0.6902, Val MSE: 0.8165\n",
      "Epoch 7/25, Train MSE: 0.6772, Val MSE: 0.8180\n",
      "Epoch 8/25, Train MSE: 0.6654, Val MSE: 0.8142\n",
      "Epoch 9/25, Train MSE: 0.6549, Val MSE: 0.8125\n",
      "Epoch 10/25, Train MSE: 0.6452, Val MSE: 0.8107\n",
      "Epoch 11/25, Train MSE: 0.6361, Val MSE: 0.8101\n",
      "Epoch 12/25, Train MSE: 0.6277, Val MSE: 0.8092\n",
      "Epoch 13/25, Train MSE: 0.6195, Val MSE: 0.8059\n",
      "Epoch 14/25, Train MSE: 0.6117, Val MSE: 0.8103\n",
      "Epoch 15/25, Train MSE: 0.6037, Val MSE: 0.8075\n",
      "Epoch 16/25, Train MSE: 0.5960, Val MSE: 0.8109\n",
      "Epoch 17/25, Train MSE: 0.5879, Val MSE: 0.8063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:55:06,573] Trial 28 finished with value: 0.8086963823842489 and parameters: {'latent_dim': 26, 'learning_rate': 0.00602255252575581, 'regularization': 1.889410286726043e-05}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/25, Train MSE: 0.5796, Val MSE: 0.8087\n",
      "Early stopping triggered.\n",
      "Epoch 1/25, Train MSE: 0.9207, Val MSE: 0.9543\n",
      "Epoch 2/25, Train MSE: 0.8676, Val MSE: 0.9142\n",
      "Epoch 3/25, Train MSE: 0.8362, Val MSE: 0.8912\n",
      "Epoch 4/25, Train MSE: 0.8142, Val MSE: 0.8762\n",
      "Epoch 5/25, Train MSE: 0.7974, Val MSE: 0.8660\n",
      "Epoch 6/25, Train MSE: 0.7836, Val MSE: 0.8583\n",
      "Epoch 7/25, Train MSE: 0.7720, Val MSE: 0.8522\n",
      "Epoch 8/25, Train MSE: 0.7619, Val MSE: 0.8469\n",
      "Epoch 9/25, Train MSE: 0.7529, Val MSE: 0.8412\n",
      "Epoch 10/25, Train MSE: 0.7448, Val MSE: 0.8389\n",
      "Epoch 11/25, Train MSE: 0.7374, Val MSE: 0.8355\n",
      "Epoch 12/25, Train MSE: 0.7306, Val MSE: 0.8335\n",
      "Epoch 13/25, Train MSE: 0.7243, Val MSE: 0.8317\n",
      "Epoch 14/25, Train MSE: 0.7184, Val MSE: 0.8288\n",
      "Epoch 15/25, Train MSE: 0.7128, Val MSE: 0.8265\n",
      "Epoch 16/25, Train MSE: 0.7076, Val MSE: 0.8249\n",
      "Epoch 17/25, Train MSE: 0.7026, Val MSE: 0.8230\n",
      "Epoch 18/25, Train MSE: 0.6979, Val MSE: 0.8213\n",
      "Epoch 19/25, Train MSE: 0.6935, Val MSE: 0.8205\n",
      "Epoch 20/25, Train MSE: 0.6892, Val MSE: 0.8206\n",
      "Epoch 21/25, Train MSE: 0.6850, Val MSE: 0.8191\n",
      "Epoch 22/25, Train MSE: 0.6810, Val MSE: 0.8169\n",
      "Epoch 23/25, Train MSE: 0.6772, Val MSE: 0.8172\n",
      "Epoch 24/25, Train MSE: 0.6736, Val MSE: 0.8146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-16 20:59:02,664] Trial 29 finished with value: 0.8144858486495758 and parameters: {'latent_dim': 20, 'learning_rate': 0.0018307852371425963, 'regularization': 0.00011136113953042562}. Best is trial 3 with value: 0.8027304317478758.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/25, Train MSE: 0.6700, Val MSE: 0.8145\n",
      "Best hyperparameters:\n",
      "{'latent_dim': 28, 'learning_rate': 0.008089574860460297, 'regularization': 1.6073221934120177e-05}\n",
      "Best validation loss: 0.8027\n",
      "Best trial: FrozenTrial(number=3, state=TrialState.COMPLETE, values=[0.8027304317478758], datetime_start=datetime.datetime(2024, 12, 16, 19, 39, 56, 96262), datetime_complete=datetime.datetime(2024, 12, 16, 19, 43, 53, 318659), params={'latent_dim': 28, 'learning_rate': 0.008089574860460297, 'regularization': 1.6073221934120177e-05}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'latent_dim': IntDistribution(high=50, log=False, low=5, step=1), 'learning_rate': FloatDistribution(high=0.1, log=True, low=0.0001, step=None), 'regularization': FloatDistribution(high=0.1, log=True, low=1e-05, step=None)}, trial_id=3, value=None)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "\n",
    "    Parameters:\n",
    "        trial (optuna.Trial): A trial object for hyperparameter suggestions.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation loss for the best set of hyperparameters.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 5, 50)  # Latent dimensions\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True)  # Learning rate\n",
    "    regularization = trial.suggest_float(\"regularization\", 1e-5, 1e-1, log=True)  # Regularization parameter\n",
    "    patience = 5  # Fixed patience for early stopping\n",
    "    epochs = 25  # Fixed number of epochs per trial\n",
    "\n",
    "    # Train the model with the suggested hyperparameters\n",
    "    _, _, _, val_losses = matrix_factorization_with_content(\n",
    "        train_sparse=train_sparse,\n",
    "        val_sparse=val_sparse,\n",
    "        content_matrix=content_matrix,\n",
    "        num_users=len(user_mapping),\n",
    "        num_items=len(movie_mapping),\n",
    "        latent_dim=latent_dim,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        regularization=regularization,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    # Return the last validation loss as the objective value\n",
    "    return val_losses[-1]\n",
    "\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # Run for 30 trials\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Best validation loss\n",
    "print(f\"Best validation loss: {study.best_value:.4f}\")\n",
    "\n",
    "# Best trial details\n",
    "print(f\"Best trial: {study.best_trial}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate the model\n",
    "\n",
    "## Model Performance Evaluation Concepts\n",
    "\n",
    "When evaluating a recommendation system, it is crucial to choose appropriate metrics that reflect the quality of recommendations and their relevance to the end user. Two commonly used metrics are **Root Mean Square Error (RMSE)** and **Precision@K**. Below is an overview of these metrics and their relevance:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Root Mean Square Error (RMSE)**\n",
    "- **Purpose:** RMSE is a regression-based metric that measures how well a model predicts numerical ratings.\n",
    "- **Formula:** \n",
    "  $$\n",
    "  RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "  where:\n",
    "  - $y_i$: True rating for item $i$.\n",
    "  - $\\hat{y}_i$: Predicted rating for item $i$.\n",
    "  - $n$: Total number of ratings in the test set.\n",
    "- **Significance:** RMSE quantifies the average prediction error. Lower RMSE values indicate that the model's predictions are closer to the actual ratings.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Precision@K**\n",
    "- **Purpose:** Precision@K evaluates the quality of top-$K$ recommendations by measuring how many of the recommended items are relevant.\n",
    "- **Formula:** \n",
    "  $$\n",
    "  Precision@K = \\frac{|Relevant \\cap Recommended@K|}{K}\n",
    "  $$\n",
    "  where:\n",
    "  - $Relevant$: Set of items relevant to the user.\n",
    "  - $Recommended@K$: Top-$K$ items recommended to the user.\n",
    "  - $K$: Number of items considered in the evaluation.\n",
    "- **Significance:** Precision@K focuses on the relevance of the most prominent recommendations. High Precision@K values indicate that the top recommendations align well with user preferences.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Both Metrics?**\n",
    "- **RMSE** evaluates the model's overall prediction accuracy, making it suitable for assessing rating prediction tasks.\n",
    "- **Precision@K** focuses on ranking performance, which is critical for recommendation tasks where the goal is to present the most relevant items to users.\n",
    "\n",
    "By combining these metrics, we can comprehensively evaluate the recommendation system's ability to predict ratings and prioritize relevant items in recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def calculate_rmse_with_content(Q, P, test_sparse, content_matrix, b_u, b_i, mu):\n",
    "    \"\"\"\n",
    "    Calculate the RMSE for the test data using the trained Q and P matrices with content blending.\n",
    "\n",
    "    Args:\n",
    "    - Q (np.ndarray): Trained item matrix (num_items x latent_dim) with only collaborative features.\n",
    "    - P (np.ndarray): Trained user matrix (num_users x (latent_dim + content_dim)).\n",
    "    - test_sparse (csr_matrix): Sparse matrix of test data.\n",
    "    - content_matrix (csr_matrix): Sparse matrix of content-based features (num_items x content_dim).\n",
    "    - b_u (np.ndarray): User biases.\n",
    "    - b_i (np.ndarray): Item biases.\n",
    "    - mu (float): Global bias.\n",
    "\n",
    "    Returns:\n",
    "    - rmse (float): Root Mean Square Error for the predictions.\n",
    "    \"\"\"\n",
    "    # Ensure correct dimensions for P and Q + content_matrix\n",
    "    latent_dim = Q.shape[1]\n",
    "    content_dim = content_matrix.shape[1]\n",
    "    if P.shape[1] != latent_dim + content_dim:\n",
    "        raise ValueError(f\"Shape mismatch: P has {P.shape[1]} dimensions, but latent_dim + content_dim is {latent_dim + content_dim}.\")\n",
    "\n",
    "    # Convert test_sparse to COO format for efficient row-column iteration\n",
    "    test_coo = test_sparse.tocoo()\n",
    "\n",
    "    # Prepare lists for true ratings and predicted ratings\n",
    "    true_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    # Iterate through each non-zero entry in the test sparse matrix\n",
    "    for row, col, true_rating in zip(test_coo.row, test_coo.col, test_coo.data):\n",
    "        # Extract the content-based features for the current item\n",
    "        content_features = content_matrix.getrow(col).toarray().flatten()\n",
    "\n",
    "        # Augment Q[col, :] with the content-based features\n",
    "        augmented_Q_col = np.hstack([Q[col, :], content_features])\n",
    "\n",
    "        # Predict the rating using biases and dot product\n",
    "        predicted_rating = mu + b_u[row] + b_i[col] + np.dot(P[row, :], augmented_Q_col)\n",
    "        \n",
    "        # Store the results\n",
    "        true_ratings.append(true_rating)\n",
    "        predicted_ratings.append(predicted_rating)\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, predicted_ratings))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded P matrix from saved_weights/P_matrix.npy with shape (610, 1703)\n",
      "Loaded Q matrix from saved_weights/Q_matrix.npy with shape (9724, 28)\n",
      "Loaded user biases from saved_weights/b_u.npy with shape (610,)\n",
      "Loaded item biases from saved_weights/b_i.npy with shape (9724,)\n",
      "Loaded global bias (mu) from saved_weights/mu.npy with value 3.5216\n",
      "\n",
      "Test RMSE: 0.9171\n"
     ]
    }
   ],
   "source": [
    "# Load the trained matrices\n",
    "P, Q, b_u, b_i, mu = load_matrices('saved_weights/P_matrix.npy', 'saved_weights/Q_matrix.npy', \n",
    "                                   'saved_weights/b_u.npy', 'saved_weights/b_i.npy', 'saved_weights/mu.npy')\n",
    "\n",
    "# Example Usage\n",
    "# Assuming Q, P, test_sparse, and content_matrix are defined with correct shapes\n",
    "rmse = calculate_rmse_with_content(Q, P, test_sparse, content_matrix, b_u, b_i, mu)\n",
    "print()\n",
    "print(f\"Test RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This average error is quite **good**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision@K metric measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.5636\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def precision_at_k(Q, P, test_sparse, content_matrix, b_u, b_i, mu, K=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K for the test set.\n",
    "\n",
    "    Args:\n",
    "    - Q (np.ndarray): Trained item matrix (num_items x latent_dim).\n",
    "    - P (np.ndarray): Trained user matrix (num_users x (latent_dim + content_dim)).\n",
    "    - test_sparse (csr_matrix): Sparse matrix of test data.\n",
    "    - content_matrix (csr_matrix): Sparse matrix of content-based features (num_items x content_dim).\n",
    "    - b_u (np.ndarray): User biases.\n",
    "    - b_i (np.ndarray): Item biases.\n",
    "    - mu (float): Global bias.\n",
    "    - K (int): Number of top items to consider for Precision@K.\n",
    "    - threshold (float): Rating threshold to consider an item as relevant.\n",
    "\n",
    "    Returns:\n",
    "    - precision (float): Average Precision@K across all users.\n",
    "    \"\"\"\n",
    "    # Initialize precision sum and user count\n",
    "    precision_sum = 0\n",
    "    user_count = 0\n",
    "\n",
    "    # Convert test_sparse to COO for easy row iteration\n",
    "    test_coo = test_sparse.tocoo()\n",
    "\n",
    "    # Build user-item rating dictionary from the test set\n",
    "    test_ratings = defaultdict(list)\n",
    "    for row, col, rating in zip(test_coo.row, test_coo.col, test_coo.data):\n",
    "        test_ratings[row].append((col, rating))\n",
    "\n",
    "    # Iterate over each user in the test set\n",
    "    for user_id, items_ratings in test_ratings.items():\n",
    "        # Predict ratings for all items for this user\n",
    "        predicted_ratings = []\n",
    "        for item_id, _ in items_ratings:\n",
    "            # Extract content-based features for the item\n",
    "            content_features = content_matrix.getrow(item_id).toarray().flatten()\n",
    "            augmented_Q = np.hstack([Q[item_id, :], content_features])\n",
    "\n",
    "            # Predict rating\n",
    "            prediction = mu + b_u[user_id] + b_i[item_id] + np.dot(P[user_id, :], augmented_Q)\n",
    "            predicted_ratings.append((item_id, prediction))\n",
    "\n",
    "        # Sort predictions by rating in descending order\n",
    "        predicted_ratings = sorted(predicted_ratings, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top-K items\n",
    "        top_k_items = [item_id for item_id, _ in predicted_ratings[:K]]\n",
    "\n",
    "        # Determine relevance based on threshold\n",
    "        relevant_items = [item_id for item_id, rating in items_ratings if rating >= threshold]\n",
    "\n",
    "        # Calculate Precision@K for this user\n",
    "        hits = len(set(top_k_items) & set(relevant_items))\n",
    "        precision_sum += hits / K\n",
    "        user_count += 1\n",
    "\n",
    "    # Average Precision@K across all users\n",
    "    precision = precision_sum / user_count if user_count > 0 else 0\n",
    "    return precision\n",
    "\n",
    "# Usage\n",
    "precision_k = precision_at_k(Q, P, test_sparse, content_matrix, b_u, b_i, mu, K=10, threshold=3.5)\n",
    "print(f\"Precision@10: {precision_k:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain Context\n",
    "Interpret the results in the **context** of your application:\n",
    "\n",
    "**E-commerce:** A Precision@10 of 0.56 means that about 5-6 out of 10 recommended products are relevant, which is generally considered good.\n",
    "**Movie Recommendation:** If users see 5-6 relevant movies out of 10 recommendations, the performance is decent, especially in a large catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models\n",
    "To determine how well your model performs, compare it against simpler baselines:\n",
    "\n",
    "**Global Popularity Baseline**:\n",
    "- Recommend the most popular items (e.g., movies with the highest average rating) to all users.\n",
    "- Random Recommendations:\n",
    "- Recommend random items to users and calculate Precision@K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Popularity Baseline Precision@10: 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Simple popularity-based Precision@K baseline\n",
    "def precision_at_k_baseline(test_ratings, K=10, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Calculate Precision@K using a global popularity baseline.\n",
    "    \"\"\"\n",
    "    # Rank items by their global average rating\n",
    "    item_avg_ratings = test_data.groupby('movieId')['rating'].mean().sort_values(ascending=False)\n",
    "    popular_items = item_avg_ratings.index[:K]  # Top-K most popular items\n",
    "\n",
    "    precision_sum = 0\n",
    "    user_count = 0\n",
    "\n",
    "    for user, group in test_ratings.groupby('userId'):\n",
    "        relevant_items = group[group['rating'] >= threshold]['movieId'].tolist()\n",
    "        hits = len(set(popular_items) & set(relevant_items))\n",
    "        precision_sum += hits / K\n",
    "        user_count += 1\n",
    "\n",
    "    return precision_sum / user_count if user_count > 0 else 0\n",
    "\n",
    "baseline_precision = precision_at_k_baseline(test_data, K=10, threshold=3.5)\n",
    "print(f\"Global Popularity Baseline Precision@10: {baseline_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Sparsity\n",
    "If your dataset is **sparse** (i.e., many items have few ratings), achieving a high Precision@K is difficult. In sparse datasets, a Precision@K of 0.5+ is often quite strong.\n",
    "\n",
    "To check dataset sparsity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sparsity: 98.8146%\n"
     ]
    }
   ],
   "source": [
    "sparsity = 1.0 - (len(train_data) / (len(user_mapping) * len(movie_mapping)))\n",
    "print(f\"Dataset Sparsity: {sparsity:.4%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it! **Sparsity** is more than **98%**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall at K\n",
    "And for the **fun** of it, here is a recall metric evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.6994\n"
     ]
    }
   ],
   "source": [
    "# Recall@K\n",
    "def recall_at_k(Q, P, test_sparse, content_matrix, b_u, b_i, mu, K=10, threshold=3.5):\n",
    "    recall_sum = 0\n",
    "    user_count = 0\n",
    "\n",
    "    test_coo = test_sparse.tocoo()\n",
    "    test_ratings = defaultdict(list)\n",
    "    for row, col, rating in zip(test_coo.row, test_coo.col, test_coo.data):\n",
    "        test_ratings[row].append((col, rating))\n",
    "\n",
    "    for user_id, items_ratings in test_ratings.items():\n",
    "        predicted_ratings = []\n",
    "        for item_id, _ in items_ratings:\n",
    "            content_features = content_matrix.getrow(item_id).toarray().flatten()\n",
    "            augmented_Q = np.hstack([Q[item_id, :], content_features])\n",
    "            prediction = mu + b_u[user_id] + b_i[item_id] + np.dot(P[user_id, :], augmented_Q)\n",
    "            predicted_ratings.append((item_id, prediction))\n",
    "\n",
    "        predicted_ratings = sorted(predicted_ratings, key=lambda x: x[1], reverse=True)[:K]\n",
    "        top_k_items = [item_id for item_id, _ in predicted_ratings]\n",
    "        relevant_items = [item_id for item_id, rating in items_ratings if rating >= threshold]\n",
    "\n",
    "        hits = len(set(top_k_items) & set(relevant_items))\n",
    "        recall_sum += hits / len(relevant_items) if relevant_items else 0\n",
    "        user_count += 1\n",
    "\n",
    "    return recall_sum / user_count if user_count > 0 else 0\n",
    "\n",
    "recall_k = recall_at_k(Q, P, test_sparse, content_matrix, b_u, b_i, mu, K=10, threshold=3.5)\n",
    "print(f\"Recall@10: {recall_k:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recall@10 of 0.6994 means that, on average, your model is able to retrieve 69.94% of all relevant items for a user when recommending the top-10 items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Your Recall@10\n",
    "High Recall@K:\n",
    "\n",
    "**Strength**: Your model retrieves most of the relevant items, meaning the user is likely to see the movies (or items) they care about.\n",
    "If Recall@10 is close to 1.0, your model is performing exceptionally well in identifying relevant items.\n",
    "Comparing Precision@K and Recall@K:\n",
    "\n",
    "You reported Precision@10 = 0.56 and Recall@10 = 0.6994:\n",
    "Precision@10 of 0.56 means 56% of the recommended top-10 items are relevant.\n",
    "Recall@10 of 0.6994 means you are covering almost 70% of all relevant items for users.\n",
    "The balance indicates that while your model retrieves many relevant items (high recall), some of the top-\n",
    "𝐾\n",
    "K recommendations are still not relevant, which lowers precision.\n",
    "Dataset Sparsity Impact:\n",
    "\n",
    "In sparse datasets (common in recommendation systems), achieving a Recall@10 near 0.7 is impressive because there are so many items that a user may not have interacted with.\n",
    "High recall suggests the model effectively prioritizes relevant items despite sparse data.\n",
    "Domain Context:\n",
    "\n",
    "Movie Recommendations: A Recall@10 of 0.7 is strong. If a user sees 70% of all the movies they would enjoy in the top 10, they’re likely to be satisfied.\n",
    "E-commerce: A similar result would indicate that most relevant products are included in the recommendations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
