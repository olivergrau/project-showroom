{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based Collaborative Filtering: Matrix Factorization with SVD\n",
    "\n",
    "In this notebook, we implement a **model-based collaborative filtering** approach using **Matrix Factorization**. Unlike memory-based approaches, model-based methods involve a training phase to learn representations for users and items, enabling more scalable and generalizable recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Memory-Based vs. Model-Based Collaborative Filtering\n",
    "\n",
    "### Memory-Based Collaborative Filtering\n",
    "- **Core Idea**: Relies on direct relationships in the data, such as user-user or item-item similarity.\n",
    "- **How It Works**:\n",
    "  - User-Based CF: Recommends items by finding similar users and aggregating their preferences.\n",
    "  - Item-Based CF: Recommends items by finding similar items based on user interactions.\n",
    "- **Key Characteristics**:\n",
    "  - No explicit training phase.\n",
    "  - Predictions are made on-the-fly by aggregating observed data.\n",
    "\n",
    "### Model-Based Collaborative Filtering\n",
    "- **Core Idea**: Builds a predictive model by learning latent representations of users and items.\n",
    "- **How It Works**:\n",
    "  - Decomposes the User-Item Interaction Matrix into low-dimensional representations (embeddings) for users and items.\n",
    "  - Uses these embeddings to predict interactions between users and items.\n",
    "- **Key Characteristics**:\n",
    "  - Requires a training phase to learn the model parameters.\n",
    "  - Generalizes to unobserved user-item pairs by capturing latent patterns.\n",
    "\n",
    "### Key Differences Between Memory-Based and Model-Based Approaches\n",
    "\n",
    "| **Aspect**            | **Memory-Based CF**                              | **Model-Based CF**                              |\n",
    "|------------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| **Computation**        | On-the-fly; computes similarities dynamically.   | Pre-computes embeddings; predictions are faster. |\n",
    "| **Training**           | No explicit training phase.                      | Explicit training phase to learn latent factors. |\n",
    "| **Generalization**     | Limited to observed interactions.                | Generalizes well to unobserved interactions.     |\n",
    "| **Sparsity Handling**  | Struggles with sparse data.                      | Learns latent patterns to handle sparsity better. |\n",
    "| **Scalability**        | Limited scalability with large datasets.         | More scalable, though training can be costly.    |\n",
    "| **Inference**          | Requires entire dataset for predictions.         | Can make predictions using pre-trained embeddings. |\n",
    "\n",
    "---\n",
    "\n",
    "## Matrix Factorization: The Core of Model-Based CF\n",
    "\n",
    "### What is Matrix Factorization?\n",
    "Matrix Factorization (MF) is a model-based approach that approximates the **User-Item Interaction Matrix** as the product of two smaller matrices:\n",
    "- **User Latent Matrix** ($U$)): Represents users in a latent feature space.\n",
    "- **Item Latent Matrix** ($V$): Represents items in the same latent feature space.\n",
    "\n",
    "### Mathematical Explanation\n",
    "Given a User-Item Interaction Matrix $ R $, we approximate it as:\n",
    "\n",
    "$$\n",
    "[R \\approx U \\cdot V^T]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ R $: Original interaction matrix ($n_{users} \\times n_{items}$).\n",
    "- $ U $: User latent feature matrix ($n_{users} \\times k$).\n",
    "- $ V $: Item latent feature matrix ($n_{items} \\times k$).\n",
    "- $ k $: Number of latent features (hyperparameter).\n",
    "\n",
    "The goal is to minimize the difference between the observed interactions in $ R $ and the predicted interactions from $ U \\cdot V^T $.\n",
    "\n",
    "### Loss Function\n",
    "The optimization problem is defined as:\n",
    "\n",
    "$$\n",
    "[\\text{Loss} = \\sum_{(u,i) \\in R} \\left( R_{u,i} - U_u \\cdot V_i^T \\right)^2 + \\lambda \\left( ||U||^2 + ||V||^2 \\right)]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- First term: Squared error between actual and predicted interactions.\n",
    "- Second term: Regularization term to prevent overfitting.\n",
    "- $ \\lambda $: Regularization hyperparameter.\n",
    "\n",
    "### Optimization Techniques\n",
    "Matrix Factorization models are trained using optimization techniques like:\n",
    "1. **Stochastic Gradient Descent (SGD)**: Iteratively updates $ U $ and $ V $ to minimize the loss function.\n",
    "2. **Alternating Least Squares (ALS)**: Alternates between fixing $ U $ and $ V $, solving one matrix at a time.\n",
    "\n",
    "---\n",
    "\n",
    "## SVD (Singular Value Decomposition) for Matrix Factorization\n",
    "\n",
    "### What is SVD?\n",
    "SVD is a variant of Matrix Factorization that decomposes the User-Item Interaction Matrix into three matrices:\n",
    "\n",
    "$$\n",
    "[R = U \\cdot \\Sigma \\cdot V^T]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ U $: Orthogonal matrix representing users.\n",
    "- $ \\Sigma $: Diagonal matrix of singular values, representing the strength of latent features.\n",
    "- $ V $: Orthogonal matrix representing items.\n",
    "\n",
    "### Simplification for Recommendations\n",
    "In practice, we truncate $ \\Sigma $ to retain only the top $ k $ singular values (dimensionality reduction):\n",
    "\n",
    "$$\n",
    "[R \\approx U_k \\cdot \\Sigma_k \\cdot V_k^T]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ U_k $: Top $ k $ user features.\n",
    "- $ \\Sigma_k $: Top $ k $ singular values.\n",
    "- $ V_k $: Top $ k $ item features.\n",
    "\n",
    "### Why SVD?\n",
    "- Captures latent user-item relationships effectively.\n",
    "- Handles sparsity by focusing on the most significant latent features.\n",
    "- Provides a mathematically robust way to factorize matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Use Matrix Factorization?\n",
    "\n",
    "### Advantages\n",
    "1. **Handles Sparse Data**: Learns latent patterns, making it robust to missing interactions.\n",
    "2. **Scalable**: Efficient for large datasets when implemented with optimized libraries.\n",
    "3. **Better Generalization**: Predicts unobserved interactions effectively.\n",
    "4. **Customizable**: Can be extended with biases, side information, or implicit feedback.\n",
    "\n",
    "### Challenges\n",
    "1. **Cold-Start Problem**: Still struggles with new users or items.\n",
    "2. **Hyperparameter Tuning**: Number of latent features (\\(k\\)) and regularization parameter (\\(\\lambda\\)) must be carefully tuned.\n",
    "3. **Interpretability**: Latent features are not always interpretable.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In this notebook, we will:\n",
    "1. Implement **Matrix Factorization** for recommendation using the **SVD variant**.\n",
    "2. Train the model to learn user and item latent features.\n",
    "3. Evaluate its performance using metrics like **Precision@K** and **Recall@K**.\n",
    "\n",
    "By the end, we will have a scalable and generalizable recommendation system ready for further enhancements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Matrix Factorization: Learning vs. Calculation\n",
    "\n",
    "In Matrix Factorization for recommendation systems, the factors (user and item latent matrices) are typically **learned through an iterative optimization process** rather than being calculated deterministically. This distinction arises from the nature of real-world data and the goals of recommendation systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Are Factors Learned Instead of Calculated?\n",
    "\n",
    "### 1. Traditional SVD: Deterministic Factorization\n",
    "- **Singular Value Decomposition (SVD)** is a **deterministic mathematical method** used for matrix factorization.\n",
    "- Given a complete and dense matrix $R$, SVD decomposes it into three matrices:\n",
    "  $$\n",
    "  R = U \\cdot \\Sigma \\cdot V^T\n",
    "  $$\n",
    "  - $U$: Orthogonal matrix representing users.\n",
    "  - $\\Sigma$: Diagonal matrix of singular values.\n",
    "  - $V$: Orthogonal matrix representing items.\n",
    "- SVD provides a precise decomposition but requires $R$ to be fully populated (no missing entries).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Challenges with Real-World Data\n",
    "- In recommendation systems, the User-Item Interaction Matrix $R$ is typically **sparse** and **incomplete**:\n",
    "  - Most entries are missing because users interact with only a small subset of items.\n",
    "  - Deterministic SVD cannot handle missing entries directly.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Learning the Factors\n",
    "To handle sparsity and incompleteness, **Matrix Factorization** uses an **iterative learning process** to approximate the factors:\n",
    "$$\n",
    "R \\approx U \\cdot V^T\n",
    "$$\n",
    "- $U$: User latent feature matrix.\n",
    "- $V$: Item latent feature matrix.\n",
    "- The goal is to minimize the error between the observed interactions in $R$ and the predicted interactions from $U \\cdot V^T$.\n",
    "\n",
    "---\n",
    "\n",
    "## How Factors Are Learned: Iterative Optimization\n",
    "\n",
    "### 1. Objective Function\n",
    "The factors are learned by minimizing an objective function that measures the error between the observed and predicted values:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{(u,i) \\in R} \\left( R_{u,i} - U_u \\cdot V_i^T \\right)^2 + \\lambda \\left( ||U||^2 + ||V||^2 \\right)\n",
    "$$\n",
    "- First term: Squared error for observed entries in $R$.\n",
    "- Second term: Regularization to prevent overfitting.\n",
    "- $\\lambda$: Regularization parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Optimization Techniques\n",
    "To minimize this loss function, the following iterative optimization algorithms are commonly used:\n",
    "\n",
    "#### **Stochastic Gradient Descent (SGD)**\n",
    "- Iteratively updates the latent factors $U$ and $V$ to reduce the loss.\n",
    "- Gradient updates:\n",
    "  $$\n",
    "  U_u \\gets U_u + \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial U_u}\n",
    "  $$\n",
    "  $$\n",
    "  V_i \\gets V_i + \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial V_i}\n",
    "  $$\n",
    "  - $\\eta$: Learning rate.\n",
    "\n",
    "#### **Alternating Least Squares (ALS)**\n",
    "- Alternates between fixing $U$ and optimizing $V$, then fixing $V$ and optimizing $U$.\n",
    "- Solves least-squares problems iteratively until convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Iterative Nature of Learning\n",
    "- The optimization process starts with random initial values for $U$ and $V$.\n",
    "- Each iteration adjusts the values to reduce the loss.\n",
    "- The process stops when the error converges below a threshold or after a fixed number of iterations.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Differences Between Deterministic and Learned Factorization\n",
    "\n",
    "| **Aspect**                | **Traditional SVD (Deterministic)** | **Matrix Factorization for Recommendations (Learned)** |\n",
    "|---------------------------|------------------------------------|--------------------------------------------------------|\n",
    "| **Input Matrix**           | Complete and dense.               | Sparse and incomplete (missing entries).               |\n",
    "| **Output**                 | Exact decomposition.              | Approximation of the matrix.                           |\n",
    "| **Method**                 | Mathematical, closed-form.        | Iterative optimization (e.g., SGD, ALS).               |\n",
    "| **Handling Missing Data**  | Cannot handle missing entries.    | Explicitly designed to work with missing entries.       |\n",
    "| **Scalability**            | Limited for very large datasets.  | Scalable with distributed algorithms like ALS.          |\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of Learning Factors\n",
    "1. **Handles Sparse Data**: Works effectively with incomplete matrices, focusing only on observed values during training.\n",
    "2. **Flexibility**: Allows for the inclusion of constraints (e.g., regularization) and side information (e.g., implicit feedback).\n",
    "3. **Scalability**: Can handle very large datasets using distributed or parallelized optimization techniques (e.g., ALS in Spark).\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "In recommendation systems, **factorization is learned** through iterative optimization because real-world interaction data is sparse and incomplete. This approach approximates the User-Item Interaction Matrix $R$ and uncovers latent relationships between users and items. Deterministic methods like traditional SVD are unsuitable for sparse data, making learning-based Matrix Factorization the method of choice.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Steps: Matrix Factorization for Recommendation\n",
    "\n",
    "## Step 2: Import Libraries and Load Dataset\n",
    "1. Import necessary libraries for data manipulation and matrix operations (e.g., Pandas, NumPy, etc.).\n",
    "2. Load the dataset (e.g., MovieLens) containing user-item interactions (ratings).\n",
    "3. Perform initial inspection of the dataset to understand its structure (e.g., columns for `userId`, `movieId`, `rating`, and potentially `timestamp`).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Data Preprocessing\n",
    "1. Prepare the **User-Item Interaction Matrix**:\n",
    "   - Create a pivot table with users as rows, items as columns, and ratings as values.\n",
    "2. Handle missing values:\n",
    "   - Fill missing entries with `0` (explicit feedback) or `NaN` (implicit feedback, to be handled differently in training).\n",
    "3. Normalize the data:\n",
    "   - Optionally subtract the mean rating of each user to center the data.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Model Implementation (Matrix Factorization)\n",
    "1. Define the model parameters:\n",
    "   - Number of latent features ($k$).\n",
    "   - Learning rate ($\\eta$) for optimization.\n",
    "   - Regularization parameter ($\\lambda$) to prevent overfitting.\n",
    "2. Initialize latent matrices:\n",
    "   - Randomly initialize the **User Latent Matrix** ($U$) and **Item Latent Matrix** ($V$) with appropriate dimensions.\n",
    "3. Define the loss function:\n",
    "   - Mean Squared Error (MSE) between observed and predicted ratings.\n",
    "   - Add regularization terms to penalize large values in $U$ and $V$.\n",
    "4. Implement the optimization algorithm:\n",
    "   - **Stochastic Gradient Descent (SGD)**:\n",
    "     - Update $U$ and $V$ iteratively based on observed ratings.\n",
    "   - **Alternating Least Squares (ALS)** (optional):\n",
    "     - Alternate between optimizing $U$ and $V$.\n",
    "5. Train the model:\n",
    "   - Iterate over the data for a fixed number of epochs.\n",
    "   - Compute and track the loss at each epoch to monitor convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Make Predictions\n",
    "1. Compute the **Predicted Ratings Matrix**:\n",
    "   - Use the dot product of the trained matrices $U$ and $V^T$ to predict ratings for all user-item pairs.\n",
    "2. Convert the predicted ratings matrix into a user-friendly format (e.g., Pandas DataFrame).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Evaluate the Model\n",
    "1. Define evaluation metrics:\n",
    "   - **Precision@K**: Fraction of top-$K$ recommendations that are relevant.\n",
    "   - **Recall@K**: Fraction of relevant items included in the top-$K$ recommendations.\n",
    "2. Split the dataset into training and test sets:\n",
    "   - Use the training set to build the model.\n",
    "   - Use the test set to evaluate the recommendations.\n",
    "3. Compute the evaluation metrics:\n",
    "   - Compare the predicted ratings with the actual ratings in the test set.\n",
    "   - Calculate Precision@K and Recall@K to assess the quality of recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Conclusion and Next Steps\n",
    "1. Summarize the performance of the Matrix Factorization model.\n",
    "2. Discuss the limitations:\n",
    "   - Cold-start problems for new users or items.\n",
    "   - Sensitivity to hyperparameters (e.g., $k$, $\\lambda$, $\\eta$).\n",
    "3. Suggest potential improvements:\n",
    "   - Incorporate user/item biases into the model.\n",
    "   - Use implicit feedback for recommendations.\n",
    "   - Experiment with alternative optimization methods (e.g., ALS).\n",
    "4. Provide an outlook to more advanced methods (e.g., deep learning-based collaborative filtering).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import Libraries and Load Dataset\n",
    "1. Import necessary libraries for data manipulation and matrix operations (e.g., Pandas, NumPy, etc.).\n",
    "2. Load the dataset (e.g., MovieLens) containing user-item interactions (ratings).\n",
    "3. Perform initial inspection of the dataset to understand its structure (e.g., columns for `userId`, `movieId`, `rating`, and potentially `timestamp`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data: Purpose and Approach\n",
    "\n",
    "In a recommendation system, splitting the data into **training**, **validation**, and **test** sets is essential for evaluating the model's ability to generalize to unseen data. This ensures that the model is not simply memorizing historical interactions but is learning patterns that can predict future user-item interactions effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Split the Data?**\n",
    "\n",
    "1. **Generalization**:\n",
    "   - The training set is used to learn the latent factors for users and items.\n",
    "   - The validation and test sets simulate future interactions, allowing us to evaluate the model's ability to generalize beyond the data it was trained on.\n",
    "\n",
    "2. **Avoiding Data Leakage**:\n",
    "   - By splitting chronologically, we ensure that the model only uses past interactions to predict future ones. This avoids \"peeking\" into future data during training.\n",
    "\n",
    "3. **Model Evaluation**:\n",
    "   - Splitting the data provides a robust framework to measure the model's performance on:\n",
    "     - **Validation Set**: Used to tune hyperparameters (e.g., learning rate, regularization).\n",
    "     - **Test Set**: Provides an unbiased estimate of the model’s real-world performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Our Splitting Approach**\n",
    "\n",
    "1. **Temporal Splitting**:\n",
    "   - The dataset is split chronologically based on the `timestamp` of user interactions.\n",
    "   - This ensures that:\n",
    "     - Older interactions are in the training set.\n",
    "     - Newer interactions are in the validation and test sets.\n",
    "\n",
    "2. **User Presence**:\n",
    "   - Every user is represented in the training set to ensure the model can generate predictions for all users in the validation and test sets.\n",
    "   - Interactions for each user are split so that:\n",
    "     - Early interactions go to the training set.\n",
    "     - Middle interactions go to the validation set.\n",
    "     - Recent interactions go to the test set.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Techniques**\n",
    "\n",
    "In real-world systems, new users, items, or updated ratings often appear dynamically. While these scenarios are not directly addressed in this notebook, they can be handled with advanced techniques such as:\n",
    "- **Online Updates** for user and item latent factors.\n",
    "- **Pretrained Embeddings** for cold-start scenarios.\n",
    "- **Hybrid Models** that combine collaborative filtering with content-based or neural approaches.\n",
    "\n",
    "These methods allow recommendation systems to adapt dynamically to changes in data without full retraining.\n",
    "\n",
    "---\n",
    "\n",
    "By splitting the data appropriately and understanding its temporal structure, we ensure a realistic evaluation of our model's predictive capabilities while laying the foundation for extending it to more advanced dynamic scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 70312 interactions\n",
      "Validation set: 15102 interactions\n",
      "Test set: 15422 interactions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "ratings = pd.read_csv('data/ml-latest-small/ratings.csv')\n",
    "\n",
    "# Sort the dataset by userId and timestamp\n",
    "ratings = ratings.sort_values(by=['userId', 'timestamp'])\n",
    "\n",
    "# Create empty lists to store the split data\n",
    "train_data = []\n",
    "val_data = []\n",
    "test_data = []\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7  # 70% for training\n",
    "val_ratio = 0.15  # 15% for validation\n",
    "test_ratio = 0.15  # 15% for testing\n",
    "\n",
    "# Split the data for each user\n",
    "for user, group in ratings.groupby('userId'):\n",
    "    n = len(group)\n",
    "    \n",
    "    # Ensure at least one interaction per set\n",
    "    if n < 3:\n",
    "        # If the user has fewer than 3 interactions, assign all to training\n",
    "        train_data.append(group)\n",
    "    else:\n",
    "        train_end = int(train_ratio * n)\n",
    "        val_end = int((train_ratio + val_ratio) * n)\n",
    "        \n",
    "        # Assign splits\n",
    "        train_data.append(group.iloc[:train_end])  # Early interactions for training\n",
    "        val_data.append(group.iloc[train_end:val_end])  # Middle interactions for validation\n",
    "        test_data.append(group.iloc[val_end:])  # Recent interactions for testing\n",
    "\n",
    "# Combine the split data into separate DataFrames\n",
    "train_data = pd.concat(train_data)\n",
    "val_data = pd.concat(val_data)\n",
    "test_data = pd.concat(test_data)\n",
    "\n",
    "# Output the sizes of the splits\n",
    "print(f\"Training set: {len(train_data)} interactions\")\n",
    "print(f\"Validation set: {len(val_data)} interactions\")\n",
    "print(f\"Test set: {len(test_data)} interactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Preprocessing\n",
    "Because we potentially work with large datasets (not with small practicing sets like ml-latest-small) we use **scipy** for using efficient data structures for sparse matrices instead of pandas.\n",
    "\n",
    "\n",
    "1. Prepare the **User-Item Interaction Matrix**:\n",
    "   - Create a pivot table with users as rows, items as columns, and ratings as values.\n",
    "2. Handle missing values:\n",
    "   - Fill missing entries with `0` (explicit feedback) or `NaN` (implicit feedback, to be handled differently in training).\n",
    "3. Normalize the data:\n",
    "   - Optionally subtract the mean rating of each user to center the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse User-Item Interaction Matrices created and normalized.\n",
      "Train Matrix Shape: (610, 7525), Non-Zero: 70312\n",
      "Validation Matrix Shape: (610, 5129), Non-Zero: 15102\n",
      "Test Matrix Shape: (610, 5556), Non-Zero: 15422\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to create a sparse User-Item Interaction Matrix\n",
    "def create_sparse_matrix(data, user_col, item_col, value_col):\n",
    "    users = data[user_col].unique()\n",
    "    items = data[item_col].unique()\n",
    "    user_map = {user: i for i, user in enumerate(users)}\n",
    "    item_map = {item: i for i, item in enumerate(items)}\n",
    "    \n",
    "    # Map users and items to indices\n",
    "    rows = data[user_col].map(user_map)\n",
    "    cols = data[item_col].map(item_map)\n",
    "    values = data[value_col]\n",
    "\n",
    "    # Create a sparse matrix (compressed sparse row matrix)\n",
    "    sparse_matrix = csr_matrix((values, (rows, cols)), shape=(len(users), len(items)))\n",
    "    return sparse_matrix, user_map, item_map\n",
    "\n",
    "# Normalize the sparse matrix by subtracting mean ratings per user\n",
    "def normalize_sparse_matrix(sparse_matrix):\n",
    "    # Compute mean rating per user\n",
    "    user_means = np.array(sparse_matrix.mean(axis=1)).flatten()\n",
    "    \n",
    "    # Subtract mean from non-zero entries\n",
    "    row_indices, col_indices = sparse_matrix.nonzero()\n",
    "    normalized_data = sparse_matrix.copy()\n",
    "    normalized_data.data -= user_means[row_indices]\n",
    "    \n",
    "    return normalized_data, user_means\n",
    "\n",
    "# Step 1: Prepare Sparse Matrices for Train, Validation, and Test Sets\n",
    "# Train\n",
    "train_sparse, train_user_map, train_item_map = create_sparse_matrix(\n",
    "    train_data, user_col='userId', item_col='movieId', value_col='rating'\n",
    ")\n",
    "train_normalized, train_user_means = normalize_sparse_matrix(train_sparse)\n",
    "\n",
    "# Validation\n",
    "val_sparse, val_user_map, val_item_map = create_sparse_matrix(\n",
    "    val_data, user_col='userId', item_col='movieId', value_col='rating'\n",
    ")\n",
    "val_normalized, val_user_means = normalize_sparse_matrix(val_sparse)\n",
    "\n",
    "# Test\n",
    "test_sparse, test_user_map, test_item_map = create_sparse_matrix(\n",
    "    test_data, user_col='userId', item_col='movieId', value_col='rating'\n",
    ")\n",
    "test_normalized, test_user_means = normalize_sparse_matrix(test_sparse)\n",
    "\n",
    "# Step 2: Print Summary of the Matrices\n",
    "print(\"Sparse User-Item Interaction Matrices created and normalized.\")\n",
    "print(f\"Train Matrix Shape: {train_normalized.shape}, Non-Zero: {train_sparse.nnz}\")\n",
    "print(f\"Validation Matrix Shape: {val_normalized.shape}, Non-Zero: {val_sparse.nnz}\")\n",
    "print(f\"Test Matrix Shape: {test_normalized.shape}, Non-Zero: {test_sparse.nnz}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model Implementation (Matrix Factorization)\n",
    "1. Define the model parameters:\n",
    "   - Number of latent features ($k$).\n",
    "   - Learning rate ($\\eta$) for optimization.\n",
    "   - Regularization parameter ($\\lambda$) to prevent overfitting.\n",
    "2. Initialize latent matrices:\n",
    "   - Randomly initialize the **User Latent Matrix** ($U$) and **Item Latent Matrix** ($V$) with appropriate dimensions.\n",
    "3. Define the loss function:\n",
    "   - Mean Squared Error (MSE) between observed and predicted ratings.\n",
    "   - Add regularization terms to penalize large values in $U$ and $V$.\n",
    "4. Implement the optimization algorithm:\n",
    "   - **Stochastic Gradient Descent (SGD)**:\n",
    "     - Update $U$ and $V$ iteratively based on observed ratings.\n",
    "   - **Alternating Least Squares (ALS)** (optional):\n",
    "     - Alternate between optimizing $U$ and $V$.\n",
    "5. Train the model:\n",
    "   - Iterate over the data for a fixed number of epochs.\n",
    "   - Compute and track the loss at each epoch to monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 12.2973, Validation Loss: 12.6040\n",
      "Epoch 2/50, Training Loss: 12.0734, Validation Loss: 12.5131\n",
      "Epoch 3/50, Training Loss: 9.3553, Validation Loss: 11.2151\n",
      "Epoch 4/50, Training Loss: 5.3561, Validation Loss: 8.3608\n",
      "Epoch 5/50, Training Loss: 3.7644, Validation Loss: 6.5978\n",
      "Epoch 6/50, Training Loss: 3.0361, Validation Loss: 5.5776\n",
      "Epoch 7/50, Training Loss: 2.6448, Validation Loss: 4.9306\n",
      "Epoch 8/50, Training Loss: 2.4071, Validation Loss: 4.4885\n",
      "Epoch 9/50, Training Loss: 2.2508, Validation Loss: 4.1695\n",
      "Epoch 10/50, Training Loss: 2.1424, Validation Loss: 3.9302\n",
      "Epoch 11/50, Training Loss: 2.0645, Validation Loss: 3.7455\n",
      "Epoch 12/50, Training Loss: 2.0071, Validation Loss: 3.5998\n",
      "Epoch 13/50, Training Loss: 1.9639, Validation Loss: 3.4834\n",
      "Epoch 14/50, Training Loss: 1.9309, Validation Loss: 3.3893\n",
      "Epoch 15/50, Training Loss: 1.9055, Validation Loss: 3.3129\n",
      "Epoch 16/50, Training Loss: 1.8857, Validation Loss: 3.2507\n",
      "Epoch 17/50, Training Loss: 1.8703, Validation Loss: 3.2002\n",
      "Epoch 18/50, Training Loss: 1.8582, Validation Loss: 3.1592\n",
      "Epoch 19/50, Training Loss: 1.8488, Validation Loss: 3.1263\n",
      "Epoch 20/50, Training Loss: 1.8414, Validation Loss: 3.1001\n",
      "Epoch 21/50, Training Loss: 1.8356, Validation Loss: 3.0793\n",
      "Epoch 22/50, Training Loss: 1.8310, Validation Loss: 3.0632\n",
      "Epoch 23/50, Training Loss: 1.8273, Validation Loss: 3.0510\n",
      "Epoch 24/50, Training Loss: 1.8244, Validation Loss: 3.0419\n",
      "Epoch 25/50, Training Loss: 1.8219, Validation Loss: 3.0354\n",
      "Epoch 26/50, Training Loss: 1.8199, Validation Loss: 3.0312\n",
      "Epoch 27/50, Training Loss: 1.8183, Validation Loss: 3.0290\n",
      "Epoch 28/50, Training Loss: 1.8169, Validation Loss: 3.0283\n",
      "Epoch 29/50, Training Loss: 1.8159, Validation Loss: 3.0291\n",
      "Epoch 30/50, Training Loss: 1.8151, Validation Loss: 3.0312\n",
      "Epoch 31/50, Training Loss: 1.8145, Validation Loss: 3.0343\n",
      "Epoch 32/50, Training Loss: 1.8142, Validation Loss: 3.0383\n",
      "Epoch 33/50, Training Loss: 1.8141, Validation Loss: 3.0433\n",
      "Early stopping triggered at epoch 33. Best Validation Loss: 3.0283\n",
      "Matrix Factorization model trained successfully.\n",
      "Test Loss: 3.1734\n"
     ]
    }
   ],
   "source": [
    "# we found a setting for hyperparameters that works well for this dataset\n",
    "# Best Parameters Found:\n",
    "# Learning Rate: 0.004099151484173524\n",
    "# Regularization: 0.00010453688227089353\n",
    "# Latent Factors: 27\n",
    "# Best Validation Loss: 3.0988\n",
    "\n",
    "# Define model parameters\n",
    "num_users, num_items = train_normalized.shape\n",
    "k = 27  # Number of latent features\n",
    "learning_rate = 0.004099151484173524\n",
    "lambda_reg = 0.00010453688227089353\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize latent factor matrices\n",
    "np.random.seed(42)\n",
    "U = np.random.normal(scale=1./k, size=(num_users, k))\n",
    "V = np.random.normal(scale=1./k, size=(num_items, k))\n",
    "\n",
    "# Define the loss function\n",
    "def compute_mse_loss(sparse_matrix, U, V, lambda_reg):\n",
    "    row_indices, col_indices = sparse_matrix.nonzero()\n",
    "    predictions = np.sum(U[row_indices] * V[col_indices], axis=1)\n",
    "    errors = sparse_matrix.data - predictions\n",
    "    mse = np.mean(errors ** 2)\n",
    "    regularization = lambda_reg * (np.sum(U**2) + np.sum(V**2))\n",
    "    return mse + regularization\n",
    "\n",
    "# Train the model using Stochastic Gradient Descent (SGD)\n",
    "def train_mf_sgd(train_matrix, val_matrix, U, V, num_epochs, learning_rate, lambda_reg, patience=5):\n",
    "    train_row_indices, train_col_indices = train_matrix.nonzero()\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    best_U, best_V = U.copy(), V.copy()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training loop\n",
    "        for idx in range(len(train_row_indices)):\n",
    "            user = train_row_indices[idx]\n",
    "            item = train_col_indices[idx]\n",
    "            rating = train_matrix.data[idx]\n",
    "\n",
    "            # Compute prediction and error\n",
    "            prediction = np.dot(U[user], V[item])\n",
    "            error = rating - prediction\n",
    "\n",
    "            # Update user and item latent factors\n",
    "            U[user] += learning_rate * (error * V[item] - lambda_reg * U[user])\n",
    "            V[item] += learning_rate * (error * U[user] - lambda_reg * V[item])\n",
    "\n",
    "            # Clip gradients to prevent overflow\n",
    "            U[user] = np.clip(U[user], -10, 10)\n",
    "            V[item] = np.clip(V[item], -10, 10)\n",
    "\n",
    "        # Compute losses after each epoch\n",
    "        train_loss = compute_mse_loss(train_matrix, U, V, lambda_reg)\n",
    "        val_loss = compute_mse_loss(val_matrix, U, V, lambda_reg)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            # Save the best U and V\n",
    "            best_U, best_V = U.copy(), V.copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    # Return the best U and V, along with the best losses\n",
    "    return best_U, best_V, train_loss, best_val_loss, best_epoch\n",
    "\n",
    "# Train the model with the new train and validation sets\n",
    "U, V, _, _, _ = train_mf_sgd(train_normalized, val_normalized, U, V, num_epochs, learning_rate, lambda_reg)\n",
    "\n",
    "print(\"Matrix Factorization model trained successfully.\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss = compute_mse_loss(test_normalized, U, V, lambda_reg)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search (Grid Search)\n",
    "When you try the basic training loop you may notice that it is pure luck to get a good loss. The loss depends (of course) heavily on the hyperparameters and where you start.\n",
    "\n",
    "So it would be advisable to use tools like Grid Search or better Optuna to automate the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parameters: learning_rate=0.001, lambda_reg=0.01, k=10\n",
      "Epoch 1/20, Training Loss: 20.3762, Validation Loss: 20.6772\n",
      "Epoch 2/20, Training Loss: 20.3743, Validation Loss: 20.6797\n",
      "Epoch 3/20, Training Loss: 20.3789, Validation Loss: 20.6893\n",
      "Epoch 4/20, Training Loss: 20.3908, Validation Loss: 20.7075\n",
      "Epoch 5/20, Training Loss: 20.4118, Validation Loss: 20.7378\n",
      "Epoch 6/20, Training Loss: 20.4464, Validation Loss: 20.7876\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 20.6772\n",
      "New best params: (0.001, 0.01, 10) with Validation Loss: 20.6772\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=0.01, k=20\n",
      "Epoch 1/20, Training Loss: 16.3513, Validation Loss: 16.6480\n",
      "Epoch 2/20, Training Loss: 16.3517, Validation Loss: 16.6507\n",
      "Epoch 3/20, Training Loss: 16.3554, Validation Loss: 16.6571\n",
      "Epoch 4/20, Training Loss: 16.3629, Validation Loss: 16.6681\n",
      "Epoch 5/20, Training Loss: 16.3755, Validation Loss: 16.6859\n",
      "Epoch 6/20, Training Loss: 16.3960, Validation Loss: 16.7151\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 16.6480\n",
      "New best params: (0.001, 0.01, 20) with Validation Loss: 16.6480\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=0.01, k=50\n",
      "Epoch 1/20, Training Loss: 13.9082, Validation Loss: 14.2053\n",
      "Epoch 2/20, Training Loss: 13.9086, Validation Loss: 14.2067\n",
      "Epoch 3/20, Training Loss: 13.9105, Validation Loss: 14.2098\n",
      "Epoch 4/20, Training Loss: 13.9142, Validation Loss: 14.2152\n",
      "Epoch 5/20, Training Loss: 13.9203, Validation Loss: 14.2242\n",
      "Epoch 6/20, Training Loss: 13.9307, Validation Loss: 14.2394\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 14.2053\n",
      "New best params: (0.001, 0.01, 50) with Validation Loss: 14.2053\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=0.1, k=10\n",
      "Epoch 1/20, Training Loss: 93.2660, Validation Loss: 93.5673\n",
      "Epoch 2/20, Training Loss: 93.0960, Validation Loss: 93.4017\n",
      "Epoch 3/20, Training Loss: 93.0041, Validation Loss: 93.3150\n",
      "Epoch 4/20, Training Loss: 93.0073, Validation Loss: 93.3247\n",
      "Epoch 5/20, Training Loss: 93.1454, Validation Loss: 93.4722\n",
      "Epoch 6/20, Training Loss: 93.5025, Validation Loss: 93.8447\n",
      "Epoch 7/20, Training Loss: 94.2504, Validation Loss: 94.6198\n",
      "Epoch 8/20, Training Loss: 95.7334, Validation Loss: 96.1532\n",
      "Early stopping triggered at epoch 8. Best Validation Loss: 93.3150\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=0.1, k=20\n",
      "Epoch 1/20, Training Loss: 52.8623, Validation Loss: 53.1601\n",
      "Epoch 2/20, Training Loss: 52.7805, Validation Loss: 53.0806\n",
      "Epoch 3/20, Training Loss: 52.7439, Validation Loss: 53.0468\n",
      "Epoch 4/20, Training Loss: 52.7666, Validation Loss: 53.0733\n",
      "Epoch 5/20, Training Loss: 52.8808, Validation Loss: 53.1936\n",
      "Epoch 6/20, Training Loss: 53.1553, Validation Loss: 53.4788\n",
      "Epoch 7/20, Training Loss: 53.7323, Validation Loss: 54.0760\n",
      "Epoch 8/20, Training Loss: 54.9008, Validation Loss: 55.2839\n",
      "Early stopping triggered at epoch 8. Best Validation Loss: 53.0468\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=0.1, k=50\n",
      "Epoch 1/20, Training Loss: 28.5579, Validation Loss: 28.8554\n",
      "Epoch 2/20, Training Loss: 28.5251, Validation Loss: 28.8236\n",
      "Epoch 3/20, Training Loss: 28.5118, Validation Loss: 28.8115\n",
      "Epoch 4/20, Training Loss: 28.5252, Validation Loss: 28.8265\n",
      "Epoch 5/20, Training Loss: 28.5814, Validation Loss: 28.8856\n",
      "Epoch 6/20, Training Loss: 28.7148, Validation Loss: 29.0241\n",
      "Epoch 7/20, Training Loss: 28.9970, Validation Loss: 29.3161\n",
      "Epoch 8/20, Training Loss: 29.5747, Validation Loss: 29.9133\n",
      "Early stopping triggered at epoch 8. Best Validation Loss: 28.8115\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=1.0, k=10\n",
      "Epoch 1/20, Training Loss: 805.1357, Validation Loss: 805.4348\n",
      "Epoch 2/20, Training Loss: 786.8408, Validation Loss: 787.1426\n",
      "Epoch 3/20, Training Loss: 771.4118, Validation Loss: 771.7158\n",
      "Epoch 4/20, Training Loss: 757.9596, Validation Loss: 758.2656\n",
      "Epoch 5/20, Training Loss: 746.0485, Validation Loss: 746.3567\n",
      "Epoch 6/20, Training Loss: 735.4566, Validation Loss: 735.7675\n",
      "Epoch 7/20, Training Loss: 726.0995, Validation Loss: 726.4139\n",
      "Epoch 8/20, Training Loss: 718.0105, Validation Loss: 718.3298\n",
      "Epoch 9/20, Training Loss: 711.3488, Validation Loss: 711.6752\n",
      "Epoch 10/20, Training Loss: 706.4318, Validation Loss: 706.7686\n",
      "Epoch 11/20, Training Loss: 703.7902, Validation Loss: 704.1421\n",
      "Epoch 12/20, Training Loss: 704.2516, Validation Loss: 704.6259\n",
      "Epoch 13/20, Training Loss: 709.0574, Validation Loss: 709.4647\n",
      "Epoch 14/20, Training Loss: 720.0115, Validation Loss: 720.4670\n",
      "Epoch 15/20, Training Loss: 739.6461, Validation Loss: 740.1711\n",
      "Epoch 16/20, Training Loss: 771.3567, Validation Loss: 771.9799\n",
      "Early stopping triggered at epoch 16. Best Validation Loss: 704.1421\n",
      "Testing parameters: learning_rate=0.001, lambda_reg=1.0, k=20\n",
      "Epoch 1/20, Training Loss: 407.8984, Validation Loss: 408.1972\n",
      "Epoch 2/20, Training Loss: 398.7495, Validation Loss: 399.0492\n",
      "Epoch 3/20, Training Loss: 391.0272, Validation Loss: 391.3278\n",
      "Epoch 4/20, Training Loss: 384.2789, Validation Loss: 384.5802\n",
      "Epoch 5/20, Training Loss: 378.2734, Validation Loss: 378.5754\n",
      "Epoch 6/20, Training Loss: 372.8811, Validation Loss: 373.1840\n",
      "Epoch 7/20, Training Loss: 368.0334, Validation Loss: 368.3373\n",
      "Epoch 8/20, Training Loss: 363.7082, Validation Loss: 364.0137\n",
      "Epoch 9/20, Training Loss: 359.9287, Validation Loss: 360.2363\n",
      "Epoch 10/20, Training Loss: 356.7701, Validation Loss: 357.0808\n",
      "Epoch 11/20, Training Loss: 354.3762, Validation Loss: 354.6915\n",
      "Epoch 12/20, Training Loss: 352.9867, Validation Loss: 353.3087\n",
      "Epoch 13/20, Training Loss: 352.9783, Validation Loss: 353.3104\n",
      "Epoch 14/20, Training Loss: 354.9274, Validation Loss: 355.2743\n",
      "Epoch 15/20, Training Loss: 359.6971, Validation Loss: 360.0661\n",
      "Epoch 16/20, Training Loss: 368.5574, Validation Loss: 368.9592\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mk, size\u001b[38;5;241m=\u001b[39m(train_normalized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], k))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Train the model (using the updated function with early stopping)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m U, V, train_loss, val_loss, best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mf_sgd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_normalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_matrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_normalized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mU\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Maximum epochs\u001b[39;49;00m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Early stopping patience\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[1;32m     50\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: learning_rate,\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlambda_reg\u001b[39m\u001b[38;5;124m'\u001b[39m: lambda_reg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m: best_epoch\n\u001b[1;32m     57\u001b[0m })\n",
      "Cell \u001b[0;32mIn[23], line 43\u001b[0m, in \u001b[0;36mtrain_mf_sgd\u001b[0;34m(train_matrix, val_matrix, U, V, num_epochs, learning_rate, lambda_reg, patience)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Update user and item latent factors\u001b[39;00m\n\u001b[1;32m     42\u001b[0m U[user] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (error \u001b[38;5;241m*\u001b[39m V[item] \u001b[38;5;241m-\u001b[39m lambda_reg \u001b[38;5;241m*\u001b[39m U[user])\n\u001b[0;32m---> 43\u001b[0m V[item] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (\u001b[43merror\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mU\u001b[49m\u001b[43m[\u001b[49m\u001b[43muser\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m lambda_reg \u001b[38;5;241m*\u001b[39m V[item])\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Clip gradients to prevent overflow\u001b[39;00m\n\u001b[1;32m     46\u001b[0m U[user] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(U[user], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "regularization_params = [0.01, 0.1, 1.0]\n",
    "latent_factors = [10, 20, 50]\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "param_grid = list(product(learning_rates, regularization_params, latent_factors))\n",
    "\n",
    "# Function to evaluate a model\n",
    "def evaluate_model(U, V, test_matrix):\n",
    "    # Compute predictions\n",
    "    predictions = np.dot(U, V.T)\n",
    "    \n",
    "    # Compute RMSE for the test set\n",
    "    row_indices, col_indices = test_matrix.nonzero()\n",
    "    errors = [\n",
    "        (test_matrix[row, col] - predictions[row, col])**2\n",
    "        for row, col in zip(row_indices, col_indices)\n",
    "    ]\n",
    "    return np.sqrt(np.mean(errors))\n",
    "\n",
    "# Perform Grid Search\n",
    "best_params = None\n",
    "best_val_loss = float('inf')\n",
    "results = []\n",
    "\n",
    "for learning_rate, lambda_reg, k in param_grid:\n",
    "    print(f\"Testing parameters: learning_rate={learning_rate}, lambda_reg={lambda_reg}, k={k}\")\n",
    "    \n",
    "    # Initialize latent factor matrices\n",
    "    U = np.random.normal(scale=1./k, size=(train_normalized.shape[0], k))\n",
    "    V = np.random.normal(scale=1./k, size=(train_normalized.shape[1], k))\n",
    "    \n",
    "    # Train the model (using the updated function with early stopping)\n",
    "    U, V, train_loss, val_loss, best_epoch = train_mf_sgd(\n",
    "        train_matrix=train_normalized,\n",
    "        val_matrix=val_normalized,\n",
    "        U=U,\n",
    "        V=V,\n",
    "        num_epochs=20,  # Maximum epochs\n",
    "        learning_rate=learning_rate,\n",
    "        lambda_reg=lambda_reg,\n",
    "        patience=5  # Early stopping patience\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'learning_rate': learning_rate,\n",
    "        'lambda_reg': lambda_reg,\n",
    "        'k': k,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'best_epoch': best_epoch\n",
    "    })\n",
    "    \n",
    "    # Update best parameters if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_params = (learning_rate, lambda_reg, k)\n",
    "        print(f\"New best params: {best_params} with Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(f\"Learning Rate: {best_params[0]}, Regularization: {best_params[1]}, Latent Factors: {best_params[2]}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Optional: Convert results to DataFrame for analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nGrid Search Results:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Search (Optuna)\n",
    "Grid Search takes a lot of time and is tedious. Let's try Optuna for hyperparameter search. And let's add an early stopping test to the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:06:27,527] A new study created in memory with name: no-name-16dfab37-a54f-4256-a4a1-356b6f574dff\n",
      "/tmp/ipykernel_7796/403730401.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
      "/tmp/ipykernel_7796/403730401.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_reg = trial.suggest_loguniform('lambda_reg', 1e-4, 1e-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Training Loss: 45.1992, Validation Loss: 45.5459\n",
      "Epoch 2/20, Training Loss: 117.0074, Validation Loss: 118.8239\n",
      "Epoch 3/20, Training Loss: 348.2541, Validation Loss: 351.4071\n",
      "Epoch 4/20, Training Loss: 509.7894, Validation Loss: 512.5627\n",
      "Epoch 5/20, Training Loss: 622.0148, Validation Loss: 624.4169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:06:34,109] Trial 0 finished with value: 45.545910732771986 and parameters: {'learning_rate': 0.005958998949583688, 'lambda_reg': 0.09643064217921256, 'k': 25}. Best is trial 0 with value: 45.545910732771986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 707.8056, Validation Loss: 709.9302\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 45.5459\n",
      "Epoch 1/20, Training Loss: 13.6899, Validation Loss: 13.9876\n",
      "Epoch 2/20, Training Loss: 13.6898, Validation Loss: 13.9884\n",
      "Epoch 3/20, Training Loss: 13.6903, Validation Loss: 13.9901\n",
      "Epoch 4/20, Training Loss: 13.6914, Validation Loss: 13.9926\n",
      "Epoch 5/20, Training Loss: 13.6932, Validation Loss: 13.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:06:40,704] Trial 1 finished with value: 13.987573344188096 and parameters: {'learning_rate': 0.0007425281450583539, 'lambda_reg': 0.006592702653314613, 'k': 38}. Best is trial 1 with value: 13.987573344188096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 13.6959, Validation Loss: 14.0016\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 13.9876\n",
      "Epoch 1/20, Training Loss: 12.3067, Validation Loss: 12.6077\n",
      "Epoch 2/20, Training Loss: 12.2926, Validation Loss: 12.6041\n",
      "Epoch 3/20, Training Loss: 12.2114, Validation Loss: 12.5744\n",
      "Epoch 4/20, Training Loss: 11.6890, Validation Loss: 12.3611\n",
      "Epoch 5/20, Training Loss: 9.5179, Validation Loss: 11.3615\n",
      "Epoch 6/20, Training Loss: 6.5708, Validation Loss: 9.5387\n",
      "Epoch 7/20, Training Loss: 4.8926, Validation Loss: 8.0240\n",
      "Epoch 8/20, Training Loss: 3.9347, Validation Loss: 6.9570\n",
      "Epoch 9/20, Training Loss: 3.3537, Validation Loss: 6.1955\n",
      "Epoch 10/20, Training Loss: 2.9781, Validation Loss: 5.6347\n",
      "Epoch 11/20, Training Loss: 2.7204, Validation Loss: 5.2081\n",
      "Epoch 12/20, Training Loss: 2.5353, Validation Loss: 4.8744\n",
      "Epoch 13/20, Training Loss: 2.3978, Validation Loss: 4.6075\n",
      "Epoch 14/20, Training Loss: 2.2928, Validation Loss: 4.3899\n",
      "Epoch 15/20, Training Loss: 2.2112, Validation Loss: 4.2097\n",
      "Epoch 16/20, Training Loss: 2.1469, Validation Loss: 4.0588\n",
      "Epoch 17/20, Training Loss: 2.0956, Validation Loss: 3.9310\n",
      "Epoch 18/20, Training Loss: 2.0544, Validation Loss: 3.8218\n",
      "Epoch 19/20, Training Loss: 2.0211, Validation Loss: 3.7279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:07:02,413] Trial 2 finished with value: 3.646738813706412 and parameters: {'learning_rate': 0.0025439998636188027, 'lambda_reg': 0.00010980672787850688, 'k': 28}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 1.9939, Validation Loss: 3.6467\n",
      "Epoch 1/20, Training Loss: 12.4245, Validation Loss: 12.7212\n",
      "Epoch 2/20, Training Loss: 12.4239, Validation Loss: 12.7213\n",
      "Epoch 3/20, Training Loss: 12.4232, Validation Loss: 12.7213\n",
      "Epoch 4/20, Training Loss: 12.4226, Validation Loss: 12.7214\n",
      "Epoch 5/20, Training Loss: 12.4217, Validation Loss: 12.7215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:07:09,144] Trial 3 finished with value: 12.721222337493453 and parameters: {'learning_rate': 0.0007043759616598867, 'lambda_reg': 0.0008942074345371845, 'k': 50}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 12.4206, Validation Loss: 12.7216\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 12.7212\n",
      "Epoch 1/20, Training Loss: 12.3142, Validation Loss: 12.6270\n",
      "Epoch 2/20, Training Loss: 11.2440, Validation Loss: 12.1792\n",
      "Epoch 3/20, Training Loss: 6.3757, Validation Loss: 9.2536\n",
      "Epoch 4/20, Training Loss: 4.6840, Validation Loss: 7.3331\n",
      "Epoch 5/20, Training Loss: 4.1949, Validation Loss: 6.4897\n",
      "Epoch 6/20, Training Loss: 4.0377, Validation Loss: 6.0592\n",
      "Epoch 7/20, Training Loss: 3.9970, Validation Loss: 5.8190\n",
      "Epoch 8/20, Training Loss: 4.0069, Validation Loss: 5.6794\n",
      "Epoch 9/20, Training Loss: 4.0415, Validation Loss: 5.5981\n",
      "Epoch 10/20, Training Loss: 4.0884, Validation Loss: 5.5531\n",
      "Epoch 11/20, Training Loss: 4.1412, Validation Loss: 5.5320\n",
      "Epoch 12/20, Training Loss: 4.1964, Validation Loss: 5.5276\n",
      "Epoch 13/20, Training Loss: 4.2520, Validation Loss: 5.5356\n",
      "Epoch 14/20, Training Loss: 4.3069, Validation Loss: 5.5529\n",
      "Epoch 15/20, Training Loss: 4.3607, Validation Loss: 5.5777\n",
      "Epoch 16/20, Training Loss: 4.4130, Validation Loss: 5.6083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:07:27,888] Trial 4 finished with value: 5.527625878980612 and parameters: {'learning_rate': 0.005404971391194554, 'lambda_reg': 0.00033182427843668906, 'k': 47}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Training Loss: 4.4636, Validation Loss: 5.6435\n",
      "Early stopping triggered at epoch 17. Best Validation Loss: 5.5276\n",
      "Epoch 1/20, Training Loss: 14.1096, Validation Loss: 14.4057\n",
      "Epoch 2/20, Training Loss: 14.1095, Validation Loss: 14.4057\n",
      "Epoch 3/20, Training Loss: 14.1094, Validation Loss: 14.4056\n",
      "Epoch 4/20, Training Loss: 14.1093, Validation Loss: 14.4056\n",
      "Epoch 5/20, Training Loss: 14.1091, Validation Loss: 14.4056\n",
      "Epoch 6/20, Training Loss: 14.1090, Validation Loss: 14.4056\n",
      "Epoch 7/20, Training Loss: 14.1089, Validation Loss: 14.4056\n",
      "Epoch 8/20, Training Loss: 14.1088, Validation Loss: 14.4055\n",
      "Epoch 9/20, Training Loss: 14.1087, Validation Loss: 14.4055\n",
      "Epoch 10/20, Training Loss: 14.1086, Validation Loss: 14.4055\n",
      "Epoch 11/20, Training Loss: 14.1085, Validation Loss: 14.4055\n",
      "Epoch 12/20, Training Loss: 14.1084, Validation Loss: 14.4055\n",
      "Epoch 13/20, Training Loss: 14.1083, Validation Loss: 14.4056\n",
      "Epoch 14/20, Training Loss: 14.1082, Validation Loss: 14.4056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:07:43,898] Trial 5 finished with value: 14.405538331737848 and parameters: {'learning_rate': 4.967083922015576e-05, 'lambda_reg': 0.005194231445336169, 'k': 23}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Training Loss: 14.1081, Validation Loss: 14.4056\n",
      "Early stopping triggered at epoch 15. Best Validation Loss: 14.4055\n",
      "Epoch 1/20, Training Loss: 12.8859, Validation Loss: 13.1820\n",
      "Epoch 2/20, Training Loss: 12.8858, Validation Loss: 13.1820\n",
      "Epoch 3/20, Training Loss: 12.8857, Validation Loss: 13.1820\n",
      "Epoch 4/20, Training Loss: 12.8855, Validation Loss: 13.1820\n",
      "Epoch 5/20, Training Loss: 12.8854, Validation Loss: 13.1820\n",
      "Epoch 6/20, Training Loss: 12.8852, Validation Loss: 13.1820\n",
      "Epoch 7/20, Training Loss: 12.8851, Validation Loss: 13.1820\n",
      "Epoch 8/20, Training Loss: 12.8849, Validation Loss: 13.1820\n",
      "Epoch 9/20, Training Loss: 12.8848, Validation Loss: 13.1820\n",
      "Epoch 10/20, Training Loss: 12.8847, Validation Loss: 13.1820\n",
      "Epoch 11/20, Training Loss: 12.8845, Validation Loss: 13.1819\n",
      "Epoch 12/20, Training Loss: 12.8844, Validation Loss: 13.1819\n",
      "Epoch 13/20, Training Loss: 12.8843, Validation Loss: 13.1819\n",
      "Epoch 14/20, Training Loss: 12.8841, Validation Loss: 13.1819\n",
      "Epoch 15/20, Training Loss: 12.8840, Validation Loss: 13.1819\n",
      "Epoch 16/20, Training Loss: 12.8838, Validation Loss: 13.1819\n",
      "Epoch 17/20, Training Loss: 12.8837, Validation Loss: 13.1820\n",
      "Epoch 18/20, Training Loss: 12.8836, Validation Loss: 13.1820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:08:04,745] Trial 6 finished with value: 13.181947038082702 and parameters: {'learning_rate': 4.5079875421490405e-05, 'lambda_reg': 0.0011227403130258822, 'k': 15}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Training Loss: 12.8834, Validation Loss: 13.1820\n",
      "Early stopping triggered at epoch 19. Best Validation Loss: 13.1819\n",
      "Epoch 1/20, Training Loss: 12.7391, Validation Loss: 13.0423\n",
      "Epoch 2/20, Training Loss: 12.7248, Validation Loss: 13.0429\n",
      "Epoch 3/20, Training Loss: 12.6270, Validation Loss: 13.0256\n",
      "Epoch 4/20, Training Loss: 11.9797, Validation Loss: 12.8724\n",
      "Epoch 5/20, Training Loss: 10.0046, Validation Loss: 12.3276\n",
      "Epoch 6/20, Training Loss: 8.7261, Validation Loss: 11.8143\n",
      "Epoch 7/20, Training Loss: 8.5072, Validation Loss: 11.6079\n",
      "Epoch 8/20, Training Loss: 8.6781, Validation Loss: 11.6192\n",
      "Epoch 9/20, Training Loss: 9.0098, Validation Loss: 11.7560\n",
      "Epoch 10/20, Training Loss: 9.3935, Validation Loss: 11.9548\n",
      "Epoch 11/20, Training Loss: 9.7857, Validation Loss: 12.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:08:17,733] Trial 7 finished with value: 11.607918401822484 and parameters: {'learning_rate': 0.002666908268608185, 'lambda_reg': 0.0012607130924920953, 'k': 22}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Training Loss: 10.1693, Validation Loss: 12.4252\n",
      "Early stopping triggered at epoch 12. Best Validation Loss: 11.6079\n",
      "Epoch 1/20, Training Loss: 12.4107, Validation Loss: 12.7073\n",
      "Epoch 2/20, Training Loss: 12.4104, Validation Loss: 12.7073\n",
      "Epoch 3/20, Training Loss: 12.4101, Validation Loss: 12.7072\n",
      "Epoch 4/20, Training Loss: 12.4099, Validation Loss: 12.7072\n",
      "Epoch 5/20, Training Loss: 12.4096, Validation Loss: 12.7072\n",
      "Epoch 6/20, Training Loss: 12.4093, Validation Loss: 12.7072\n",
      "Epoch 7/20, Training Loss: 12.4090, Validation Loss: 12.7072\n",
      "Epoch 8/20, Training Loss: 12.4088, Validation Loss: 12.7072\n",
      "Epoch 9/20, Training Loss: 12.4084, Validation Loss: 12.7072\n",
      "Epoch 10/20, Training Loss: 12.4081, Validation Loss: 12.7072\n",
      "Epoch 11/20, Training Loss: 12.4077, Validation Loss: 12.7072\n",
      "Epoch 12/20, Training Loss: 12.4074, Validation Loss: 12.7072\n",
      "Epoch 13/20, Training Loss: 12.4069, Validation Loss: 12.7071\n",
      "Epoch 14/20, Training Loss: 12.4064, Validation Loss: 12.7071\n",
      "Epoch 15/20, Training Loss: 12.4059, Validation Loss: 12.7070\n",
      "Epoch 16/20, Training Loss: 12.4052, Validation Loss: 12.7070\n",
      "Epoch 17/20, Training Loss: 12.4045, Validation Loss: 12.7069\n",
      "Epoch 18/20, Training Loss: 12.4036, Validation Loss: 12.7067\n",
      "Epoch 19/20, Training Loss: 12.4026, Validation Loss: 12.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:08:39,870] Trial 8 finished with value: 12.706324404617176 and parameters: {'learning_rate': 0.00025021852706087327, 'lambda_reg': 0.0007418691194253386, 'k': 46}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 12.4013, Validation Loss: 12.7063\n",
      "Epoch 1/20, Training Loss: 15.8736, Validation Loss: 16.1716\n",
      "Epoch 2/20, Training Loss: 15.8731, Validation Loss: 16.1716\n",
      "Epoch 3/20, Training Loss: 15.8727, Validation Loss: 16.1715\n",
      "Epoch 4/20, Training Loss: 15.8723, Validation Loss: 16.1715\n",
      "Epoch 5/20, Training Loss: 15.8719, Validation Loss: 16.1716\n",
      "Epoch 6/20, Training Loss: 15.8716, Validation Loss: 16.1717\n",
      "Epoch 7/20, Training Loss: 15.8712, Validation Loss: 16.1718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:08:48,497] Trial 9 finished with value: 16.171538402931937 and parameters: {'learning_rate': 0.00010365130657710538, 'lambda_reg': 0.005297210190985668, 'k': 12}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Training Loss: 15.8710, Validation Loss: 16.1719\n",
      "Early stopping triggered at epoch 8. Best Validation Loss: 16.1715\n",
      "Epoch 1/20, Training Loss: 12.3088, Validation Loss: 12.6044\n",
      "Epoch 2/20, Training Loss: 12.3088, Validation Loss: 12.6044\n",
      "Epoch 3/20, Training Loss: 12.3088, Validation Loss: 12.6044\n",
      "Epoch 4/20, Training Loss: 12.3088, Validation Loss: 12.6044\n",
      "Epoch 5/20, Training Loss: 12.3088, Validation Loss: 12.6044\n",
      "Epoch 6/20, Training Loss: 12.3088, Validation Loss: 12.6044\n",
      "Epoch 7/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 8/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 9/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 10/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 11/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 12/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 13/20, Training Loss: 12.3087, Validation Loss: 12.6044\n",
      "Epoch 14/20, Training Loss: 12.3086, Validation Loss: 12.6044\n",
      "Epoch 15/20, Training Loss: 12.3086, Validation Loss: 12.6044\n",
      "Epoch 16/20, Training Loss: 12.3086, Validation Loss: 12.6044\n",
      "Epoch 17/20, Training Loss: 12.3086, Validation Loss: 12.6044\n",
      "Epoch 18/20, Training Loss: 12.3086, Validation Loss: 12.6044\n",
      "Epoch 19/20, Training Loss: 12.3086, Validation Loss: 12.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:09:10,377] Trial 10 finished with value: 12.604360148259428 and parameters: {'learning_rate': 1.057847323785517e-05, 'lambda_reg': 0.00012191505617007474, 'k': 34}. Best is trial 2 with value: 3.646738813706412.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 12.3086, Validation Loss: 12.6044\n",
      "Epoch 1/20, Training Loss: 12.1276, Validation Loss: 12.5302\n",
      "Epoch 2/20, Training Loss: 7.2018, Validation Loss: 9.7258\n",
      "Epoch 3/20, Training Loss: 3.9779, Validation Loss: 6.4168\n",
      "Epoch 4/20, Training Loss: 3.0929, Validation Loss: 5.1477\n",
      "Epoch 5/20, Training Loss: 2.7349, Validation Loss: 4.5147\n",
      "Epoch 6/20, Training Loss: 2.5482, Validation Loss: 4.1434\n",
      "Epoch 7/20, Training Loss: 2.4399, Validation Loss: 3.9042\n",
      "Epoch 8/20, Training Loss: 2.3748, Validation Loss: 3.7419\n",
      "Epoch 9/20, Training Loss: 2.3356, Validation Loss: 3.6287\n",
      "Epoch 10/20, Training Loss: 2.3127, Validation Loss: 3.5496\n",
      "Epoch 11/20, Training Loss: 2.3003, Validation Loss: 3.4956\n",
      "Epoch 12/20, Training Loss: 2.2949, Validation Loss: 3.4608\n",
      "Epoch 13/20, Training Loss: 2.2942, Validation Loss: 3.4411\n",
      "Epoch 14/20, Training Loss: 2.2966, Validation Loss: 3.4331\n",
      "Epoch 15/20, Training Loss: 2.3011, Validation Loss: 3.4341\n",
      "Epoch 16/20, Training Loss: 2.3068, Validation Loss: 3.4418\n",
      "Epoch 17/20, Training Loss: 2.3131, Validation Loss: 3.4543\n",
      "Epoch 18/20, Training Loss: 2.3196, Validation Loss: 3.4701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:09:31,078] Trial 11 finished with value: 3.433062764286082 and parameters: {'learning_rate': 0.006966260272668645, 'lambda_reg': 0.00014130419810859835, 'k': 39}. Best is trial 11 with value: 3.433062764286082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Training Loss: 2.3260, Validation Loss: 3.4881\n",
      "Early stopping triggered at epoch 19. Best Validation Loss: 3.4331\n",
      "Epoch 1/20, Training Loss: 12.3007, Validation Loss: 12.5991\n",
      "Epoch 2/20, Training Loss: 12.2968, Validation Loss: 12.5984\n",
      "Epoch 3/20, Training Loss: 12.2864, Validation Loss: 12.5952\n",
      "Epoch 4/20, Training Loss: 12.2519, Validation Loss: 12.5824\n",
      "Epoch 5/20, Training Loss: 12.1273, Validation Loss: 12.5330\n",
      "Epoch 6/20, Training Loss: 11.6933, Validation Loss: 12.3541\n",
      "Epoch 7/20, Training Loss: 10.4659, Validation Loss: 11.8152\n",
      "Epoch 8/20, Training Loss: 8.3686, Validation Loss: 10.7451\n",
      "Epoch 9/20, Training Loss: 6.5120, Validation Loss: 9.5025\n",
      "Epoch 10/20, Training Loss: 5.2958, Validation Loss: 8.4463\n",
      "Epoch 11/20, Training Loss: 4.4714, Validation Loss: 7.6067\n",
      "Epoch 12/20, Training Loss: 3.8957, Validation Loss: 6.9417\n",
      "Epoch 13/20, Training Loss: 3.4841, Validation Loss: 6.4104\n",
      "Epoch 14/20, Training Loss: 3.1813, Validation Loss: 5.9801\n",
      "Epoch 15/20, Training Loss: 2.9522, Validation Loss: 5.6263\n",
      "Epoch 16/20, Training Loss: 2.7744, Validation Loss: 5.3314\n",
      "Epoch 17/20, Training Loss: 2.6337, Validation Loss: 5.0825\n",
      "Epoch 18/20, Training Loss: 2.5204, Validation Loss: 4.8700\n",
      "Epoch 19/20, Training Loss: 2.4278, Validation Loss: 4.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:09:52,683] Trial 12 finished with value: 4.527515783666918 and parameters: {'learning_rate': 0.0017225450345013371, 'lambda_reg': 0.00011237970343571529, 'k': 39}. Best is trial 11 with value: 3.433062764286082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.3511, Validation Loss: 4.5275\n",
      "Epoch 1/20, Training Loss: 9.8949, Validation Loss: 11.4025\n",
      "Epoch 2/20, Training Loss: 5.0620, Validation Loss: 7.1301\n",
      "Epoch 3/20, Training Loss: 4.1272, Validation Loss: 5.8446\n",
      "Epoch 4/20, Training Loss: 3.8860, Validation Loss: 5.3772\n",
      "Epoch 5/20, Training Loss: 3.8042, Validation Loss: 5.1621\n",
      "Epoch 6/20, Training Loss: 3.7898, Validation Loss: 5.0564\n",
      "Epoch 7/20, Training Loss: 3.8090, Validation Loss: 5.0098\n",
      "Epoch 8/20, Training Loss: 3.8454, Validation Loss: 5.0009\n",
      "Epoch 9/20, Training Loss: 3.8908, Validation Loss: 5.0189\n",
      "Epoch 10/20, Training Loss: 3.9409, Validation Loss: 5.0570\n",
      "Epoch 11/20, Training Loss: 3.9934, Validation Loss: 5.1099\n",
      "Epoch 12/20, Training Loss: 4.0464, Validation Loss: 5.1727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:10:06,850] Trial 13 finished with value: 5.00087514886319 and parameters: {'learning_rate': 0.009610512779414496, 'lambda_reg': 0.00027274530142059965, 'k': 30}. Best is trial 11 with value: 3.433062764286082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Training Loss: 4.0986, Validation Loss: 5.2409\n",
      "Early stopping triggered at epoch 13. Best Validation Loss: 5.0009\n",
      "Epoch 1/20, Training Loss: 20.0683, Validation Loss: 20.3667\n",
      "Epoch 2/20, Training Loss: 20.0888, Validation Loss: 20.3906\n",
      "Epoch 3/20, Training Loss: 20.1742, Validation Loss: 20.4838\n",
      "Epoch 4/20, Training Loss: 20.4766, Validation Loss: 20.8134\n",
      "Epoch 5/20, Training Loss: 21.6486, Validation Loss: 22.0919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:10:13,477] Trial 14 finished with value: 20.366681294810427 and parameters: {'learning_rate': 0.0019623519277194214, 'lambda_reg': 0.038177877812858675, 'k': 40}. Best is trial 11 with value: 3.433062764286082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 26.1965, Validation Loss: 27.0326\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 20.3667\n",
      "Epoch 1/20, Training Loss: 12.3562, Validation Loss: 12.6539\n",
      "Epoch 2/20, Training Loss: 12.3547, Validation Loss: 12.6537\n",
      "Epoch 3/20, Training Loss: 12.3529, Validation Loss: 12.6535\n",
      "Epoch 4/20, Training Loss: 12.3504, Validation Loss: 12.6530\n",
      "Epoch 5/20, Training Loss: 12.3466, Validation Loss: 12.6521\n",
      "Epoch 6/20, Training Loss: 12.3406, Validation Loss: 12.6503\n",
      "Epoch 7/20, Training Loss: 12.3302, Validation Loss: 12.6469\n",
      "Epoch 8/20, Training Loss: 12.3120, Validation Loss: 12.6405\n",
      "Epoch 9/20, Training Loss: 12.2793, Validation Loss: 12.6285\n",
      "Epoch 10/20, Training Loss: 12.2204, Validation Loss: 12.6065\n",
      "Epoch 11/20, Training Loss: 12.1147, Validation Loss: 12.5662\n",
      "Epoch 12/20, Training Loss: 11.9287, Validation Loss: 12.4940\n",
      "Epoch 13/20, Training Loss: 11.6128, Validation Loss: 12.3692\n",
      "Epoch 14/20, Training Loss: 11.1098, Validation Loss: 12.1644\n",
      "Epoch 15/20, Training Loss: 10.3867, Validation Loss: 11.8555\n",
      "Epoch 16/20, Training Loss: 9.4863, Validation Loss: 11.4403\n",
      "Epoch 17/20, Training Loss: 8.5387, Validation Loss: 10.9513\n",
      "Epoch 18/20, Training Loss: 7.6808, Validation Loss: 10.4406\n",
      "Epoch 19/20, Training Loss: 6.9701, Validation Loss: 9.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:10:34,941] Trial 15 finished with value: 9.496581801473422 and parameters: {'learning_rate': 0.0007923863283841693, 'lambda_reg': 0.0002878072774885186, 'k': 30}. Best is trial 11 with value: 3.433062764286082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 6.3943, Validation Loss: 9.4966\n",
      "Epoch 1/20, Training Loss: 18.9234, Validation Loss: 19.2259\n",
      "Epoch 2/20, Training Loss: 18.9869, Validation Loss: 19.3033\n",
      "Epoch 3/20, Training Loss: 19.2886, Validation Loss: 19.6770\n",
      "Epoch 4/20, Training Loss: 21.1940, Validation Loss: 22.0707\n",
      "Epoch 5/20, Training Loss: 30.1299, Validation Loss: 32.5249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:10:41,487] Trial 16 finished with value: 19.225856916804823 and parameters: {'learning_rate': 0.002873770363543256, 'lambda_reg': 0.014694262692562864, 'k': 18}. Best is trial 11 with value: 3.433062764286082.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 46.0447, Validation Loss: 49.1897\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 19.2259\n",
      "Epoch 1/20, Training Loss: 10.0091, Validation Loss: 11.4017\n",
      "Epoch 2/20, Training Loss: 4.3482, Validation Loss: 6.4250\n",
      "Epoch 3/20, Training Loss: 3.0262, Validation Loss: 4.7575\n",
      "Epoch 4/20, Training Loss: 2.5508, Validation Loss: 4.0613\n",
      "Epoch 5/20, Training Loss: 2.3060, Validation Loss: 3.6860\n",
      "Epoch 6/20, Training Loss: 2.1731, Validation Loss: 3.4596\n",
      "Epoch 7/20, Training Loss: 2.1011, Validation Loss: 3.3175\n",
      "Epoch 8/20, Training Loss: 2.0624, Validation Loss: 3.2294\n",
      "Epoch 9/20, Training Loss: 2.0424, Validation Loss: 3.1782\n",
      "Epoch 10/20, Training Loss: 2.0330, Validation Loss: 3.1535\n",
      "Epoch 11/20, Training Loss: 2.0293, Validation Loss: 3.1477\n",
      "Epoch 12/20, Training Loss: 2.0286, Validation Loss: 3.1549\n",
      "Epoch 13/20, Training Loss: 2.0291, Validation Loss: 3.1704\n",
      "Epoch 14/20, Training Loss: 2.0299, Validation Loss: 3.1906\n",
      "Epoch 15/20, Training Loss: 2.0306, Validation Loss: 3.2134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:10:58,664] Trial 17 finished with value: 3.1477262233085597 and parameters: {'learning_rate': 0.00964212381604277, 'lambda_reg': 0.00011623361275907739, 'k': 35}. Best is trial 17 with value: 3.1477262233085597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Training Loss: 2.0314, Validation Loss: 3.2376\n",
      "Early stopping triggered at epoch 16. Best Validation Loss: 3.1477\n",
      "Epoch 1/20, Training Loss: 12.2471, Validation Loss: 13.5198\n",
      "Epoch 2/20, Training Loss: 13.4880, Validation Loss: 15.6373\n",
      "Epoch 3/20, Training Loss: 16.0447, Validation Loss: 17.8135\n",
      "Epoch 4/20, Training Loss: 18.1208, Validation Loss: 19.6502\n",
      "Epoch 5/20, Training Loss: 19.7327, Validation Loss: 21.1206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:11:05,207] Trial 18 finished with value: 13.51976300214832 and parameters: {'learning_rate': 0.009558431031003633, 'lambda_reg': 0.0019932077202119915, 'k': 35}. Best is trial 17 with value: 3.1477262233085597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 21.0353, Validation Loss: 22.3244\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 13.5198\n",
      "Epoch 1/20, Training Loss: 12.3604, Validation Loss: 12.6569\n",
      "Epoch 2/20, Training Loss: 12.3600, Validation Loss: 12.6568\n",
      "Epoch 3/20, Training Loss: 12.3597, Validation Loss: 12.6568\n",
      "Epoch 4/20, Training Loss: 12.3594, Validation Loss: 12.6568\n",
      "Epoch 5/20, Training Loss: 12.3590, Validation Loss: 12.6568\n",
      "Epoch 6/20, Training Loss: 12.3586, Validation Loss: 12.6568\n",
      "Epoch 7/20, Training Loss: 12.3583, Validation Loss: 12.6567\n",
      "Epoch 8/20, Training Loss: 12.3578, Validation Loss: 12.6567\n",
      "Epoch 9/20, Training Loss: 12.3574, Validation Loss: 12.6567\n",
      "Epoch 10/20, Training Loss: 12.3569, Validation Loss: 12.6566\n",
      "Epoch 11/20, Training Loss: 12.3563, Validation Loss: 12.6565\n",
      "Epoch 12/20, Training Loss: 12.3556, Validation Loss: 12.6564\n",
      "Epoch 13/20, Training Loss: 12.3548, Validation Loss: 12.6563\n",
      "Epoch 14/20, Training Loss: 12.3539, Validation Loss: 12.6561\n",
      "Epoch 15/20, Training Loss: 12.3528, Validation Loss: 12.6559\n",
      "Epoch 16/20, Training Loss: 12.3514, Validation Loss: 12.6555\n",
      "Epoch 17/20, Training Loss: 12.3498, Validation Loss: 12.6551\n",
      "Epoch 18/20, Training Loss: 12.3478, Validation Loss: 12.6545\n",
      "Epoch 19/20, Training Loss: 12.3453, Validation Loss: 12.6538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:11:27,131] Trial 19 finished with value: 12.652866972573117 and parameters: {'learning_rate': 0.00030710566881568654, 'lambda_reg': 0.00043721398941386447, 'k': 44}. Best is trial 17 with value: 3.1477262233085597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 12.3422, Validation Loss: 12.6529\n",
      "Epoch 1/20, Training Loss: 12.3012, Validation Loss: 12.6150\n",
      "Epoch 2/20, Training Loss: 11.4919, Validation Loss: 12.2643\n",
      "Epoch 3/20, Training Loss: 6.7528, Validation Loss: 9.5678\n",
      "Epoch 4/20, Training Loss: 4.4057, Validation Loss: 7.2208\n",
      "Epoch 5/20, Training Loss: 3.5761, Validation Loss: 6.0610\n",
      "Epoch 6/20, Training Loss: 3.2126, Validation Loss: 5.4090\n",
      "Epoch 7/20, Training Loss: 3.0266, Validation Loss: 5.0028\n",
      "Epoch 8/20, Training Loss: 2.9238, Validation Loss: 4.7317\n",
      "Epoch 9/20, Training Loss: 2.8660, Validation Loss: 4.5423\n",
      "Epoch 10/20, Training Loss: 2.8348, Validation Loss: 4.4058\n",
      "Epoch 11/20, Training Loss: 2.8204, Validation Loss: 4.3056\n",
      "Epoch 12/20, Training Loss: 2.8169, Validation Loss: 4.2314\n",
      "Epoch 13/20, Training Loss: 2.8205, Validation Loss: 4.1765\n",
      "Epoch 14/20, Training Loss: 2.8286, Validation Loss: 4.1365\n",
      "Epoch 15/20, Training Loss: 2.8398, Validation Loss: 4.1083\n",
      "Epoch 16/20, Training Loss: 2.8528, Validation Loss: 4.0896\n",
      "Epoch 17/20, Training Loss: 2.8669, Validation Loss: 4.0788\n",
      "Epoch 18/20, Training Loss: 2.8818, Validation Loss: 4.0745\n",
      "Epoch 19/20, Training Loss: 2.8969, Validation Loss: 4.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:11:49,217] Trial 20 finished with value: 4.074508815577245 and parameters: {'learning_rate': 0.004806387053894252, 'lambda_reg': 0.00019540284819336205, 'k': 34}. Best is trial 17 with value: 3.1477262233085597.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.9121, Validation Loss: 4.0810\n",
      "Epoch 1/20, Training Loss: 12.2975, Validation Loss: 12.6046\n",
      "Epoch 2/20, Training Loss: 12.0788, Validation Loss: 12.5143\n",
      "Epoch 3/20, Training Loss: 9.4000, Validation Loss: 11.2309\n",
      "Epoch 4/20, Training Loss: 5.3699, Validation Loss: 8.3762\n",
      "Epoch 5/20, Training Loss: 3.7670, Validation Loss: 6.6069\n",
      "Epoch 6/20, Training Loss: 3.0344, Validation Loss: 5.5828\n",
      "Epoch 7/20, Training Loss: 2.6405, Validation Loss: 4.9337\n",
      "Epoch 8/20, Training Loss: 2.4008, Validation Loss: 4.4902\n",
      "Epoch 9/20, Training Loss: 2.2428, Validation Loss: 4.1703\n",
      "Epoch 10/20, Training Loss: 2.1334, Validation Loss: 3.9304\n",
      "Epoch 11/20, Training Loss: 2.0552, Validation Loss: 3.7453\n",
      "Epoch 12/20, Training Loss: 1.9981, Validation Loss: 3.5994\n",
      "Epoch 13/20, Training Loss: 1.9556, Validation Loss: 3.4828\n",
      "Epoch 14/20, Training Loss: 1.9235, Validation Loss: 3.3887\n",
      "Epoch 15/20, Training Loss: 1.8990, Validation Loss: 3.3122\n",
      "Epoch 16/20, Training Loss: 1.8800, Validation Loss: 3.2499\n",
      "Epoch 17/20, Training Loss: 1.8653, Validation Loss: 3.1992\n",
      "Epoch 18/20, Training Loss: 1.8536, Validation Loss: 3.1581\n",
      "Epoch 19/20, Training Loss: 1.8445, Validation Loss: 3.1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:12:10,738] Trial 21 finished with value: 3.0987913564505427 and parameters: {'learning_rate': 0.004099151484173524, 'lambda_reg': 0.00010453688227089353, 'k': 27}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 1.8372, Validation Loss: 3.0988\n",
      "Epoch 1/20, Training Loss: 12.3689, Validation Loss: 12.6664\n",
      "Epoch 2/20, Training Loss: 12.3673, Validation Loss: 12.6663\n",
      "Epoch 3/20, Training Loss: 12.3647, Validation Loss: 12.6660\n",
      "Epoch 4/20, Training Loss: 12.3596, Validation Loss: 12.6650\n",
      "Epoch 5/20, Training Loss: 12.3475, Validation Loss: 12.6615\n",
      "Epoch 6/20, Training Loss: 12.3167, Validation Loss: 12.6515\n",
      "Epoch 7/20, Training Loss: 12.2369, Validation Loss: 12.6239\n",
      "Epoch 8/20, Training Loss: 12.0335, Validation Loss: 12.5509\n",
      "Epoch 9/20, Training Loss: 11.5516, Validation Loss: 12.3720\n",
      "Epoch 10/20, Training Loss: 10.5933, Validation Loss: 11.9944\n",
      "Epoch 11/20, Training Loss: 9.2126, Validation Loss: 11.3826\n",
      "Epoch 12/20, Training Loss: 7.8928, Validation Loss: 10.6649\n",
      "Epoch 13/20, Training Loss: 6.9378, Validation Loss: 9.9980\n",
      "Epoch 14/20, Training Loss: 6.2749, Validation Loss: 9.4322\n",
      "Epoch 15/20, Training Loss: 5.8038, Validation Loss: 8.9648\n",
      "Epoch 16/20, Training Loss: 5.4689, Validation Loss: 8.5830\n",
      "Epoch 17/20, Training Loss: 5.2327, Validation Loss: 8.2720\n",
      "Epoch 18/20, Training Loss: 5.0673, Validation Loss: 8.0182\n",
      "Epoch 19/20, Training Loss: 4.9525, Validation Loss: 7.8101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:12:32,599] Trial 22 finished with value: 7.638617234012541 and parameters: {'learning_rate': 0.0012661850826462058, 'lambda_reg': 0.0004789542250192062, 'k': 43}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 4.8745, Validation Loss: 7.6386\n",
      "Epoch 1/20, Training Loss: 12.3109, Validation Loss: 12.6149\n",
      "Epoch 2/20, Training Loss: 12.1787, Validation Loss: 12.5612\n",
      "Epoch 3/20, Training Loss: 10.2185, Validation Loss: 11.6651\n",
      "Epoch 4/20, Training Loss: 5.9858, Validation Loss: 8.9496\n",
      "Epoch 5/20, Training Loss: 4.2416, Validation Loss: 7.1121\n",
      "Epoch 6/20, Training Loss: 3.5025, Validation Loss: 6.0874\n",
      "Epoch 7/20, Training Loss: 3.1374, Validation Loss: 5.4603\n",
      "Epoch 8/20, Training Loss: 2.9334, Validation Loss: 5.0458\n",
      "Epoch 9/20, Training Loss: 2.8110, Validation Loss: 4.7565\n",
      "Epoch 10/20, Training Loss: 2.7352, Validation Loss: 4.5463\n",
      "Epoch 11/20, Training Loss: 2.6880, Validation Loss: 4.3895\n",
      "Epoch 12/20, Training Loss: 2.6594, Validation Loss: 4.2701\n",
      "Epoch 13/20, Training Loss: 2.6433, Validation Loss: 4.1782\n",
      "Epoch 14/20, Training Loss: 2.6357, Validation Loss: 4.1069\n",
      "Epoch 15/20, Training Loss: 2.6339, Validation Loss: 4.0517\n",
      "Epoch 16/20, Training Loss: 2.6361, Validation Loss: 4.0092\n",
      "Epoch 17/20, Training Loss: 2.6413, Validation Loss: 3.9769\n",
      "Epoch 18/20, Training Loss: 2.6484, Validation Loss: 3.9532\n",
      "Epoch 19/20, Training Loss: 2.6568, Validation Loss: 3.9364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:12:54,328] Trial 23 finished with value: 3.92536395049727 and parameters: {'learning_rate': 0.0040943212020290295, 'lambda_reg': 0.00017963339382078876, 'k': 36}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.6663, Validation Loss: 3.9254\n",
      "Epoch 1/20, Training Loss: 9.4148, Validation Loss: 11.0440\n",
      "Epoch 2/20, Training Loss: 4.4849, Validation Loss: 6.4494\n",
      "Epoch 3/20, Training Loss: 3.3985, Validation Loss: 5.0553\n",
      "Epoch 4/20, Training Loss: 3.0354, Validation Loss: 4.4936\n",
      "Epoch 5/20, Training Loss: 2.8631, Validation Loss: 4.2021\n",
      "Epoch 6/20, Training Loss: 2.7814, Validation Loss: 4.0358\n",
      "Epoch 7/20, Training Loss: 2.7468, Validation Loss: 3.9407\n",
      "Epoch 8/20, Training Loss: 2.7377, Validation Loss: 3.8920\n",
      "Epoch 9/20, Training Loss: 2.7429, Validation Loss: 3.8757\n",
      "Epoch 10/20, Training Loss: 2.7566, Validation Loss: 3.8826\n",
      "Epoch 11/20, Training Loss: 2.7749, Validation Loss: 3.9054\n",
      "Epoch 12/20, Training Loss: 2.7953, Validation Loss: 3.9379\n",
      "Epoch 13/20, Training Loss: 2.8157, Validation Loss: 3.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:13:09,428] Trial 24 finished with value: 3.8756627165865725 and parameters: {'learning_rate': 0.009971529359620148, 'lambda_reg': 0.000174119365113973, 'k': 27}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Training Loss: 2.8353, Validation Loss: 4.0153\n",
      "Early stopping triggered at epoch 14. Best Validation Loss: 3.8757\n",
      "Epoch 1/20, Training Loss: 12.3921, Validation Loss: 12.7139\n",
      "Epoch 2/20, Training Loss: 11.2100, Validation Loss: 12.2740\n",
      "Epoch 3/20, Training Loss: 7.0219, Validation Loss: 9.9461\n",
      "Epoch 4/20, Training Loss: 5.8643, Validation Loss: 8.5741\n",
      "Epoch 5/20, Training Loss: 5.6948, Validation Loss: 8.0558\n",
      "Epoch 6/20, Training Loss: 5.7754, Validation Loss: 7.8615\n",
      "Epoch 7/20, Training Loss: 5.9265, Validation Loss: 7.8090\n",
      "Epoch 8/20, Training Loss: 6.0977, Validation Loss: 7.8263\n",
      "Epoch 9/20, Training Loss: 6.2711, Validation Loss: 7.8799\n",
      "Epoch 10/20, Training Loss: 6.4392, Validation Loss: 7.9527\n",
      "Epoch 11/20, Training Loss: 6.5991, Validation Loss: 8.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:13:22,594] Trial 25 finished with value: 7.809017800063553 and parameters: {'learning_rate': 0.005123024539454447, 'lambda_reg': 0.0005889116771972554, 'k': 33}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Training Loss: 6.7499, Validation Loss: 8.1245\n",
      "Early stopping triggered at epoch 12. Best Validation Loss: 7.8090\n",
      "Epoch 1/20, Training Loss: 12.2985, Validation Loss: 12.5964\n",
      "Epoch 2/20, Training Loss: 12.2962, Validation Loss: 12.5961\n",
      "Epoch 3/20, Training Loss: 12.2922, Validation Loss: 12.5953\n",
      "Epoch 4/20, Training Loss: 12.2830, Validation Loss: 12.5926\n",
      "Epoch 5/20, Training Loss: 12.2584, Validation Loss: 12.5839\n",
      "Epoch 6/20, Training Loss: 12.1878, Validation Loss: 12.5570\n",
      "Epoch 7/20, Training Loss: 11.9850, Validation Loss: 12.4763\n",
      "Epoch 8/20, Training Loss: 11.4423, Validation Loss: 12.2513\n",
      "Epoch 9/20, Training Loss: 10.2531, Validation Loss: 11.7235\n",
      "Epoch 10/20, Training Loss: 8.4942, Validation Loss: 10.8261\n",
      "Epoch 11/20, Training Loss: 6.8769, Validation Loss: 9.7888\n",
      "Epoch 12/20, Training Loss: 5.7218, Validation Loss: 8.8531\n",
      "Epoch 13/20, Training Loss: 4.8953, Validation Loss: 8.0708\n",
      "Epoch 14/20, Training Loss: 4.2867, Validation Loss: 7.4247\n",
      "Epoch 15/20, Training Loss: 3.8316, Validation Loss: 6.8905\n",
      "Epoch 16/20, Training Loss: 3.4855, Validation Loss: 6.4457\n",
      "Epoch 17/20, Training Loss: 3.2169, Validation Loss: 6.0720\n",
      "Epoch 18/20, Training Loss: 3.0044, Validation Loss: 5.7547\n",
      "Epoch 19/20, Training Loss: 2.8333, Validation Loss: 5.4827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:13:44,399] Trial 26 finished with value: 5.247330840498527 and parameters: {'learning_rate': 0.0014241481137006037, 'lambda_reg': 0.00010385960337157155, 'k': 41}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.6935, Validation Loss: 5.2473\n",
      "Epoch 1/20, Training Loss: 12.7596, Validation Loss: 13.0634\n",
      "Epoch 2/20, Training Loss: 12.7213, Validation Loss: 13.0682\n",
      "Epoch 3/20, Training Loss: 12.2127, Validation Loss: 13.0461\n",
      "Epoch 4/20, Training Loss: 10.4247, Validation Loss: 13.0268\n",
      "Epoch 5/20, Training Loss: 10.4601, Validation Loss: 13.5117\n",
      "Epoch 6/20, Training Loss: 11.3509, Validation Loss: 14.2210\n",
      "Epoch 7/20, Training Loss: 12.4007, Validation Loss: 15.0157\n",
      "Epoch 8/20, Training Loss: 13.4075, Validation Loss: 15.7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:13:54,143] Trial 27 finished with value: 13.02680001277477 and parameters: {'learning_rate': 0.003536766682235287, 'lambda_reg': 0.0019030550517473268, 'k': 32}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Training Loss: 14.3298, Validation Loss: 16.5249\n",
      "Early stopping triggered at epoch 9. Best Validation Loss: 13.0268\n",
      "Epoch 1/20, Training Loss: 12.0967, Validation Loss: 12.5535\n",
      "Epoch 2/20, Training Loss: 7.0184, Validation Loss: 9.6790\n",
      "Epoch 3/20, Training Loss: 4.2904, Validation Loss: 6.7884\n",
      "Epoch 4/20, Training Loss: 3.5606, Validation Loss: 5.6722\n",
      "Epoch 5/20, Training Loss: 3.2862, Validation Loss: 5.1239\n",
      "Epoch 6/20, Training Loss: 3.1600, Validation Loss: 4.8126\n",
      "Epoch 7/20, Training Loss: 3.1002, Validation Loss: 4.6215\n",
      "Epoch 8/20, Training Loss: 3.0759, Validation Loss: 4.4997\n",
      "Epoch 9/20, Training Loss: 3.0720, Validation Loss: 4.4219\n",
      "Epoch 10/20, Training Loss: 3.0802, Validation Loss: 4.3743\n",
      "Epoch 11/20, Training Loss: 3.0958, Validation Loss: 4.3486\n",
      "Epoch 12/20, Training Loss: 3.1159, Validation Loss: 4.3395\n",
      "Epoch 13/20, Training Loss: 3.1388, Validation Loss: 4.3429\n",
      "Epoch 14/20, Training Loss: 3.1632, Validation Loss: 4.3558\n",
      "Epoch 15/20, Training Loss: 3.1882, Validation Loss: 4.3757\n",
      "Epoch 16/20, Training Loss: 3.2132, Validation Loss: 4.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:14:12,408] Trial 28 finished with value: 4.3394792947246845 and parameters: {'learning_rate': 0.006692754039244508, 'lambda_reg': 0.00021208458761990052, 'k': 19}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Training Loss: 3.2378, Validation Loss: 4.4284\n",
      "Early stopping triggered at epoch 17. Best Validation Loss: 4.3395\n",
      "Epoch 1/20, Training Loss: 28.2376, Validation Loss: 28.6577\n",
      "Epoch 2/20, Training Loss: 99.1822, Validation Loss: 101.8612\n",
      "Epoch 3/20, Training Loss: 214.9684, Validation Loss: 217.6757\n",
      "Epoch 4/20, Training Loss: 283.6638, Validation Loss: 285.9377\n",
      "Epoch 5/20, Training Loss: 332.6586, Validation Loss: 334.6149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:14:18,960] Trial 29 finished with value: 28.657709880542797 and parameters: {'learning_rate': 0.006817864539411394, 'lambda_reg': 0.04505229873679308, 'k': 27}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 370.5970, Validation Loss: 372.3309\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 28.6577\n",
      "Epoch 1/20, Training Loss: 12.3615, Validation Loss: 12.6582\n",
      "Epoch 2/20, Training Loss: 12.3609, Validation Loss: 12.6581\n",
      "Epoch 3/20, Training Loss: 12.3603, Validation Loss: 12.6581\n",
      "Epoch 4/20, Training Loss: 12.3597, Validation Loss: 12.6581\n",
      "Epoch 5/20, Training Loss: 12.3590, Validation Loss: 12.6580\n",
      "Epoch 6/20, Training Loss: 12.3582, Validation Loss: 12.6580\n",
      "Epoch 7/20, Training Loss: 12.3572, Validation Loss: 12.6579\n",
      "Epoch 8/20, Training Loss: 12.3561, Validation Loss: 12.6577\n",
      "Epoch 9/20, Training Loss: 12.3546, Validation Loss: 12.6575\n",
      "Epoch 10/20, Training Loss: 12.3528, Validation Loss: 12.6571\n",
      "Epoch 11/20, Training Loss: 12.3504, Validation Loss: 12.6565\n",
      "Epoch 12/20, Training Loss: 12.3471, Validation Loss: 12.6557\n",
      "Epoch 13/20, Training Loss: 12.3428, Validation Loss: 12.6544\n",
      "Epoch 14/20, Training Loss: 12.3369, Validation Loss: 12.6527\n",
      "Epoch 15/20, Training Loss: 12.3289, Validation Loss: 12.6502\n",
      "Epoch 16/20, Training Loss: 12.3178, Validation Loss: 12.6465\n",
      "Epoch 17/20, Training Loss: 12.3024, Validation Loss: 12.6414\n",
      "Epoch 18/20, Training Loss: 12.2812, Validation Loss: 12.6342\n",
      "Epoch 19/20, Training Loss: 12.2519, Validation Loss: 12.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:14:41,230] Trial 30 finished with value: 12.609613081315453 and parameters: {'learning_rate': 0.0004326908362536612, 'lambda_reg': 0.00037558701101824787, 'k': 37}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 12.2114, Validation Loss: 12.6096\n",
      "Epoch 1/20, Training Loss: 12.3070, Validation Loss: 12.6078\n",
      "Epoch 2/20, Training Loss: 12.2828, Validation Loss: 12.5997\n",
      "Epoch 3/20, Training Loss: 12.0878, Validation Loss: 12.5195\n",
      "Epoch 4/20, Training Loss: 10.6900, Validation Loss: 11.8971\n",
      "Epoch 5/20, Training Loss: 7.2328, Validation Loss: 9.9736\n",
      "Epoch 6/20, Training Loss: 5.0219, Validation Loss: 8.1041\n",
      "Epoch 7/20, Training Loss: 3.8934, Validation Loss: 6.8601\n",
      "Epoch 8/20, Training Loss: 3.2591, Validation Loss: 6.0196\n",
      "Epoch 9/20, Training Loss: 2.8708, Validation Loss: 5.4259\n",
      "Epoch 10/20, Training Loss: 2.6145, Validation Loss: 4.9883\n",
      "Epoch 11/20, Training Loss: 2.4355, Validation Loss: 4.6542\n",
      "Epoch 12/20, Training Loss: 2.3054, Validation Loss: 4.3921\n",
      "Epoch 13/20, Training Loss: 2.2080, Validation Loss: 4.1819\n",
      "Epoch 14/20, Training Loss: 2.1336, Validation Loss: 4.0103\n",
      "Epoch 15/20, Training Loss: 2.0759, Validation Loss: 3.8683\n",
      "Epoch 16/20, Training Loss: 2.0306, Validation Loss: 3.7495\n",
      "Epoch 17/20, Training Loss: 1.9947, Validation Loss: 3.6493\n",
      "Epoch 18/20, Training Loss: 1.9660, Validation Loss: 3.5641\n",
      "Epoch 19/20, Training Loss: 1.9428, Validation Loss: 3.4913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:15:02,769] Trial 31 finished with value: 3.4288600061966146 and parameters: {'learning_rate': 0.002932749435887332, 'lambda_reg': 0.00010760196034193446, 'k': 26}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 1.9241, Validation Loss: 3.4289\n",
      "Epoch 1/20, Training Loss: 12.3170, Validation Loss: 12.6257\n",
      "Epoch 2/20, Training Loss: 12.1106, Validation Loss: 12.5463\n",
      "Epoch 3/20, Training Loss: 9.7676, Validation Loss: 11.4750\n",
      "Epoch 4/20, Training Loss: 5.8239, Validation Loss: 8.8497\n",
      "Epoch 5/20, Training Loss: 4.1924, Validation Loss: 7.1138\n",
      "Epoch 6/20, Training Loss: 3.4563, Validation Loss: 6.1012\n",
      "Epoch 7/20, Training Loss: 3.0750, Validation Loss: 5.4620\n",
      "Epoch 8/20, Training Loss: 2.8532, Validation Loss: 5.0295\n",
      "Epoch 9/20, Training Loss: 2.7144, Validation Loss: 4.7215\n",
      "Epoch 10/20, Training Loss: 2.6238, Validation Loss: 4.4937\n",
      "Epoch 11/20, Training Loss: 2.5633, Validation Loss: 4.3206\n",
      "Epoch 12/20, Training Loss: 2.5228, Validation Loss: 4.1864\n",
      "Epoch 13/20, Training Loss: 2.4959, Validation Loss: 4.0810\n",
      "Epoch 14/20, Training Loss: 2.4786, Validation Loss: 3.9974\n",
      "Epoch 15/20, Training Loss: 2.4682, Validation Loss: 3.9308\n",
      "Epoch 16/20, Training Loss: 2.4628, Validation Loss: 3.8778\n",
      "Epoch 17/20, Training Loss: 2.4609, Validation Loss: 3.8357\n",
      "Epoch 18/20, Training Loss: 2.4617, Validation Loss: 3.8027\n",
      "Epoch 19/20, Training Loss: 2.4644, Validation Loss: 3.7771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:15:23,691] Trial 32 finished with value: 3.7578196144632683 and parameters: {'learning_rate': 0.0038487717066418266, 'lambda_reg': 0.00016106918150976174, 'k': 25}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.4685, Validation Loss: 3.7578\n",
      "Epoch 1/20, Training Loss: 12.3455, Validation Loss: 12.6432\n",
      "Epoch 2/20, Training Loss: 12.3432, Validation Loss: 12.6429\n",
      "Epoch 3/20, Training Loss: 12.3399, Validation Loss: 12.6423\n",
      "Epoch 4/20, Training Loss: 12.3341, Validation Loss: 12.6407\n",
      "Epoch 5/20, Training Loss: 12.3228, Validation Loss: 12.6370\n",
      "Epoch 6/20, Training Loss: 12.2985, Validation Loss: 12.6283\n",
      "Epoch 7/20, Training Loss: 12.2451, Validation Loss: 12.6083\n",
      "Epoch 8/20, Training Loss: 12.1265, Validation Loss: 12.5628\n",
      "Epoch 9/20, Training Loss: 11.8701, Validation Loss: 12.4621\n",
      "Epoch 10/20, Training Loss: 11.3531, Validation Loss: 12.2529\n",
      "Epoch 11/20, Training Loss: 10.4497, Validation Loss: 11.8669\n",
      "Epoch 12/20, Training Loss: 9.2058, Validation Loss: 11.2792\n",
      "Epoch 13/20, Training Loss: 7.9321, Validation Loss: 10.5717\n",
      "Epoch 14/20, Training Loss: 6.8963, Validation Loss: 9.8690\n",
      "Epoch 15/20, Training Loss: 6.1136, Validation Loss: 9.2365\n",
      "Epoch 16/20, Training Loss: 5.5132, Validation Loss: 8.6861\n",
      "Epoch 17/20, Training Loss: 5.0453, Validation Loss: 8.2119\n",
      "Epoch 18/20, Training Loss: 4.6784, Validation Loss: 7.8046\n",
      "Epoch 19/20, Training Loss: 4.3889, Validation Loss: 7.4545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:15:45,442] Trial 33 finished with value: 7.152406674282025 and parameters: {'learning_rate': 0.0010735169410610427, 'lambda_reg': 0.0002406073366907951, 'k': 29}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 4.1587, Validation Loss: 7.1524\n",
      "Epoch 1/20, Training Loss: 12.3223, Validation Loss: 12.6226\n",
      "Epoch 2/20, Training Loss: 12.3102, Validation Loss: 12.6196\n",
      "Epoch 3/20, Training Loss: 12.2603, Validation Loss: 12.6017\n",
      "Epoch 4/20, Training Loss: 12.0168, Validation Loss: 12.5048\n",
      "Epoch 5/20, Training Loss: 10.9800, Validation Loss: 12.0637\n",
      "Epoch 6/20, Training Loss: 8.5366, Validation Loss: 10.8536\n",
      "Epoch 7/20, Training Loss: 6.2935, Validation Loss: 9.3290\n",
      "Epoch 8/20, Training Loss: 4.9745, Validation Loss: 8.1133\n",
      "Epoch 9/20, Training Loss: 4.1560, Validation Loss: 7.2119\n",
      "Epoch 10/20, Training Loss: 3.6291, Validation Loss: 6.5394\n",
      "Epoch 11/20, Training Loss: 3.2752, Validation Loss: 6.0272\n",
      "Epoch 12/20, Training Loss: 3.0269, Validation Loss: 5.6275\n",
      "Epoch 13/20, Training Loss: 2.8462, Validation Loss: 5.3089\n",
      "Epoch 14/20, Training Loss: 2.7108, Validation Loss: 5.0500\n",
      "Epoch 15/20, Training Loss: 2.6071, Validation Loss: 4.8364\n",
      "Epoch 16/20, Training Loss: 2.5263, Validation Loss: 4.6577\n",
      "Epoch 17/20, Training Loss: 2.4625, Validation Loss: 4.5066\n",
      "Epoch 18/20, Training Loss: 2.4117, Validation Loss: 4.3776\n",
      "Epoch 19/20, Training Loss: 2.3709, Validation Loss: 4.2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:16:07,033] Trial 34 finished with value: 4.170464070234884 and parameters: {'learning_rate': 0.002160988356950365, 'lambda_reg': 0.00014010688838985203, 'k': 24}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.3380, Validation Loss: 4.1705\n",
      "Epoch 1/20, Training Loss: 11.8009, Validation Loss: 12.4489\n",
      "Epoch 2/20, Training Loss: 6.2226, Validation Loss: 8.9106\n",
      "Epoch 3/20, Training Loss: 4.5715, Validation Loss: 6.8031\n",
      "Epoch 4/20, Training Loss: 4.1827, Validation Loss: 6.0593\n",
      "Epoch 5/20, Training Loss: 4.0804, Validation Loss: 5.7278\n",
      "Epoch 6/20, Training Loss: 4.0683, Validation Loss: 5.5639\n",
      "Epoch 7/20, Training Loss: 4.0940, Validation Loss: 5.4822\n",
      "Epoch 8/20, Training Loss: 4.1377, Validation Loss: 5.4468\n",
      "Epoch 9/20, Training Loss: 4.1900, Validation Loss: 5.4406\n",
      "Epoch 10/20, Training Loss: 4.2464, Validation Loss: 5.4548\n",
      "Epoch 11/20, Training Loss: 4.3042, Validation Loss: 5.4841\n",
      "Epoch 12/20, Training Loss: 4.3623, Validation Loss: 5.5247\n",
      "Epoch 13/20, Training Loss: 4.4198, Validation Loss: 5.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:16:22,354] Trial 35 finished with value: 5.440618982409106 and parameters: {'learning_rate': 0.0074872096977469495, 'lambda_reg': 0.00031240036648782024, 'k': 21}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Training Loss: 4.4760, Validation Loss: 5.6285\n",
      "Early stopping triggered at epoch 14. Best Validation Loss: 5.4406\n",
      "Epoch 1/20, Training Loss: 12.3038, Validation Loss: 12.6076\n",
      "Epoch 2/20, Training Loss: 12.2472, Validation Loss: 12.5877\n",
      "Epoch 3/20, Training Loss: 11.6964, Validation Loss: 12.3596\n",
      "Epoch 4/20, Training Loss: 8.8438, Validation Loss: 10.9833\n",
      "Epoch 5/20, Training Loss: 5.7067, Validation Loss: 8.7711\n",
      "Epoch 6/20, Training Loss: 4.1996, Validation Loss: 7.2213\n",
      "Epoch 7/20, Training Loss: 3.4016, Validation Loss: 6.2125\n",
      "Epoch 8/20, Training Loss: 2.9394, Validation Loss: 5.5257\n",
      "Epoch 9/20, Training Loss: 2.6462, Validation Loss: 5.0338\n",
      "Epoch 10/20, Training Loss: 2.4474, Validation Loss: 4.6666\n",
      "Epoch 11/20, Training Loss: 2.3060, Validation Loss: 4.3835\n",
      "Epoch 12/20, Training Loss: 2.2022, Validation Loss: 4.1598\n",
      "Epoch 13/20, Training Loss: 2.1243, Validation Loss: 3.9794\n",
      "Epoch 14/20, Training Loss: 2.0649, Validation Loss: 3.8318\n",
      "Epoch 15/20, Training Loss: 2.0191, Validation Loss: 3.7094\n",
      "Epoch 16/20, Training Loss: 1.9835, Validation Loss: 3.6071\n",
      "Epoch 17/20, Training Loss: 1.9555, Validation Loss: 3.5207\n",
      "Epoch 18/20, Training Loss: 1.9333, Validation Loss: 3.4474\n",
      "Epoch 19/20, Training Loss: 1.9155, Validation Loss: 3.3849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:16:44,003] Trial 36 finished with value: 3.3314468187549453 and parameters: {'learning_rate': 0.0031190312968784714, 'lambda_reg': 0.0001070111735618258, 'k': 26}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 1.9012, Validation Loss: 3.3314\n",
      "Epoch 1/20, Training Loss: 12.3115, Validation Loss: 12.6081\n",
      "Epoch 2/20, Training Loss: 12.3106, Validation Loss: 12.6081\n",
      "Epoch 3/20, Training Loss: 12.3096, Validation Loss: 12.6081\n",
      "Epoch 4/20, Training Loss: 12.3086, Validation Loss: 12.6080\n",
      "Epoch 5/20, Training Loss: 12.3074, Validation Loss: 12.6080\n",
      "Epoch 6/20, Training Loss: 12.3061, Validation Loss: 12.6078\n",
      "Epoch 7/20, Training Loss: 12.3045, Validation Loss: 12.6076\n",
      "Epoch 8/20, Training Loss: 12.3025, Validation Loss: 12.6072\n",
      "Epoch 9/20, Training Loss: 12.2999, Validation Loss: 12.6066\n",
      "Epoch 10/20, Training Loss: 12.2964, Validation Loss: 12.6057\n",
      "Epoch 11/20, Training Loss: 12.2917, Validation Loss: 12.6044\n",
      "Epoch 12/20, Training Loss: 12.2851, Validation Loss: 12.6023\n",
      "Epoch 13/20, Training Loss: 12.2758, Validation Loss: 12.5993\n",
      "Epoch 14/20, Training Loss: 12.2625, Validation Loss: 12.5946\n",
      "Epoch 15/20, Training Loss: 12.2434, Validation Loss: 12.5878\n",
      "Epoch 16/20, Training Loss: 12.2159, Validation Loss: 12.5776\n",
      "Epoch 17/20, Training Loss: 12.1761, Validation Loss: 12.5625\n",
      "Epoch 18/20, Training Loss: 12.1185, Validation Loss: 12.5404\n",
      "Epoch 19/20, Training Loss: 12.0358, Validation Loss: 12.5081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:17:05,792] Trial 37 finished with value: 12.461318514918853 and parameters: {'learning_rate': 0.0005005086079879693, 'lambda_reg': 0.00010493794867272047, 'k': 26}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 11.9178, Validation Loss: 12.4613\n",
      "Epoch 1/20, Training Loss: 14.7689, Validation Loss: 15.0701\n",
      "Epoch 2/20, Training Loss: 14.8001, Validation Loss: 15.1168\n",
      "Epoch 3/20, Training Loss: 14.9782, Validation Loss: 15.3993\n",
      "Epoch 4/20, Training Loss: 16.2172, Validation Loss: 17.3105\n",
      "Epoch 5/20, Training Loss: 21.8281, Validation Loss: 24.4607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:17:12,289] Trial 38 finished with value: 15.070101766321843 and parameters: {'learning_rate': 0.0028173650056427103, 'lambda_reg': 0.009478382790204616, 'k': 31}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 31.0218, Validation Loss: 34.1494\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 15.0701\n",
      "Epoch 1/20, Training Loss: 12.5208, Validation Loss: 12.8185\n",
      "Epoch 2/20, Training Loss: 12.5204, Validation Loss: 12.8185\n",
      "Epoch 3/20, Training Loss: 12.5200, Validation Loss: 12.8185\n",
      "Epoch 4/20, Training Loss: 12.5196, Validation Loss: 12.8185\n",
      "Epoch 5/20, Training Loss: 12.5191, Validation Loss: 12.8185\n",
      "Epoch 6/20, Training Loss: 12.5187, Validation Loss: 12.8184\n",
      "Epoch 7/20, Training Loss: 12.5183, Validation Loss: 12.8184\n",
      "Epoch 8/20, Training Loss: 12.5179, Validation Loss: 12.8185\n",
      "Epoch 9/20, Training Loss: 12.5175, Validation Loss: 12.8185\n",
      "Epoch 10/20, Training Loss: 12.5170, Validation Loss: 12.8185\n",
      "Epoch 11/20, Training Loss: 12.5166, Validation Loss: 12.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:17:25,370] Trial 39 finished with value: 12.818449853689147 and parameters: {'learning_rate': 0.0001781626744689916, 'lambda_reg': 0.0006244890231866175, 'k': 21}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Training Loss: 12.5161, Validation Loss: 12.8185\n",
      "Early stopping triggered at epoch 12. Best Validation Loss: 12.8184\n",
      "Epoch 1/20, Training Loss: 12.3227, Validation Loss: 12.6219\n",
      "Epoch 2/20, Training Loss: 12.3193, Validation Loss: 12.6215\n",
      "Epoch 3/20, Training Loss: 12.3144, Validation Loss: 12.6207\n",
      "Epoch 4/20, Training Loss: 12.3065, Validation Loss: 12.6187\n",
      "Epoch 5/20, Training Loss: 12.2919, Validation Loss: 12.6142\n",
      "Epoch 6/20, Training Loss: 12.2631, Validation Loss: 12.6043\n",
      "Epoch 7/20, Training Loss: 12.2042, Validation Loss: 12.5825\n",
      "Epoch 8/20, Training Loss: 12.0831, Validation Loss: 12.5358\n",
      "Epoch 9/20, Training Loss: 11.8385, Validation Loss: 12.4385\n",
      "Epoch 10/20, Training Loss: 11.3718, Validation Loss: 12.2462\n",
      "Epoch 11/20, Training Loss: 10.5761, Validation Loss: 11.9011\n",
      "Epoch 12/20, Training Loss: 9.4510, Validation Loss: 11.3696\n",
      "Epoch 13/20, Training Loss: 8.2049, Validation Loss: 10.6962\n",
      "Epoch 14/20, Training Loss: 7.0994, Validation Loss: 9.9853\n",
      "Epoch 15/20, Training Loss: 6.2207, Validation Loss: 9.3160\n",
      "Epoch 16/20, Training Loss: 5.5277, Validation Loss: 8.7156\n",
      "Epoch 17/20, Training Loss: 4.9715, Validation Loss: 8.1855\n",
      "Epoch 18/20, Training Loss: 4.5204, Validation Loss: 7.7199\n",
      "Epoch 19/20, Training Loss: 4.1523, Validation Loss: 7.3112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:17:46,702] Trial 40 finished with value: 6.951803474350786 and parameters: {'learning_rate': 0.0009875991690017532, 'lambda_reg': 0.00010030091967269539, 'k': 18}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 3.8495, Validation Loss: 6.9518\n",
      "Epoch 1/20, Training Loss: 12.2258, Validation Loss: 12.5865\n",
      "Epoch 2/20, Training Loss: 9.0092, Validation Loss: 11.0041\n",
      "Epoch 3/20, Training Loss: 4.7329, Validation Loss: 7.5277\n",
      "Epoch 4/20, Training Loss: 3.4882, Validation Loss: 5.9238\n",
      "Epoch 5/20, Training Loss: 3.0079, Validation Loss: 5.1150\n",
      "Epoch 6/20, Training Loss: 2.7710, Validation Loss: 4.6423\n",
      "Epoch 7/20, Training Loss: 2.6382, Validation Loss: 4.3388\n",
      "Epoch 8/20, Training Loss: 2.5593, Validation Loss: 4.1321\n",
      "Epoch 9/20, Training Loss: 2.5117, Validation Loss: 3.9862\n",
      "Epoch 10/20, Training Loss: 2.4837, Validation Loss: 3.8813\n",
      "Epoch 11/20, Training Loss: 2.4683, Validation Loss: 3.8057\n",
      "Epoch 12/20, Training Loss: 2.4615, Validation Loss: 3.7520\n",
      "Epoch 13/20, Training Loss: 2.4607, Validation Loss: 3.7153\n",
      "Epoch 14/20, Training Loss: 2.4640, Validation Loss: 3.6919\n",
      "Epoch 15/20, Training Loss: 2.4701, Validation Loss: 3.6790\n",
      "Epoch 16/20, Training Loss: 2.4782, Validation Loss: 3.6744\n",
      "Epoch 17/20, Training Loss: 2.4874, Validation Loss: 3.6761\n",
      "Epoch 18/20, Training Loss: 2.4971, Validation Loss: 3.6826\n",
      "Epoch 19/20, Training Loss: 2.5070, Validation Loss: 3.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:18:08,397] Trial 41 finished with value: 3.6743550087672743 and parameters: {'learning_rate': 0.005721741002408693, 'lambda_reg': 0.0001579947381313396, 'k': 28}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.5167, Validation Loss: 3.7056\n",
      "Epoch 1/20, Training Loss: 12.3195, Validation Loss: 12.6218\n",
      "Epoch 2/20, Training Loss: 12.2604, Validation Loss: 12.6000\n",
      "Epoch 3/20, Training Loss: 11.4158, Validation Loss: 12.2433\n",
      "Epoch 4/20, Training Loss: 7.6086, Validation Loss: 10.2513\n",
      "Epoch 5/20, Training Loss: 5.0832, Validation Loss: 8.1036\n",
      "Epoch 6/20, Training Loss: 4.0378, Validation Loss: 6.8508\n",
      "Epoch 7/20, Training Loss: 3.5429, Validation Loss: 6.0917\n",
      "Epoch 8/20, Training Loss: 3.2797, Validation Loss: 5.5977\n",
      "Epoch 9/20, Training Loss: 3.1282, Validation Loss: 5.2574\n",
      "Epoch 10/20, Training Loss: 3.0375, Validation Loss: 5.0133\n",
      "Epoch 11/20, Training Loss: 2.9830, Validation Loss: 4.8328\n",
      "Epoch 12/20, Training Loss: 2.9513, Validation Loss: 4.6965\n",
      "Epoch 13/20, Training Loss: 2.9347, Validation Loss: 4.5922\n",
      "Epoch 14/20, Training Loss: 2.9285, Validation Loss: 4.5117\n",
      "Epoch 15/20, Training Loss: 2.9295, Validation Loss: 4.4493\n",
      "Epoch 16/20, Training Loss: 2.9354, Validation Loss: 4.4011\n",
      "Epoch 17/20, Training Loss: 2.9449, Validation Loss: 4.3641\n",
      "Epoch 18/20, Training Loss: 2.9566, Validation Loss: 4.3362\n",
      "Epoch 19/20, Training Loss: 2.9700, Validation Loss: 4.3157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:18:29,865] Trial 42 finished with value: 4.301248344113954 and parameters: {'learning_rate': 0.003676888477935432, 'lambda_reg': 0.00021175185307307245, 'k': 37}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.9844, Validation Loss: 4.3012\n",
      "Epoch 1/20, Training Loss: 12.3198, Validation Loss: 12.6211\n",
      "Epoch 2/20, Training Loss: 12.3027, Validation Loss: 12.6162\n",
      "Epoch 3/20, Training Loss: 12.2094, Validation Loss: 12.5808\n",
      "Epoch 4/20, Training Loss: 11.6611, Validation Loss: 12.3547\n",
      "Epoch 5/20, Training Loss: 9.5472, Validation Loss: 11.3820\n",
      "Epoch 6/20, Training Loss: 6.7092, Validation Loss: 9.6540\n",
      "Epoch 7/20, Training Loss: 5.0652, Validation Loss: 8.2026\n",
      "Epoch 8/20, Training Loss: 4.1219, Validation Loss: 7.1698\n",
      "Epoch 9/20, Training Loss: 3.5484, Validation Loss: 6.4273\n",
      "Epoch 10/20, Training Loss: 3.1791, Validation Loss: 5.8782\n",
      "Epoch 11/20, Training Loss: 2.9280, Validation Loss: 5.4597\n",
      "Epoch 12/20, Training Loss: 2.7495, Validation Loss: 5.1321\n",
      "Epoch 13/20, Training Loss: 2.6184, Validation Loss: 4.8700\n",
      "Epoch 14/20, Training Loss: 2.5196, Validation Loss: 4.6564\n",
      "Epoch 15/20, Training Loss: 2.4437, Validation Loss: 4.4798\n",
      "Epoch 16/20, Training Loss: 2.3846, Validation Loss: 4.3318\n",
      "Epoch 17/20, Training Loss: 2.3382, Validation Loss: 4.2066\n",
      "Epoch 18/20, Training Loss: 2.3014, Validation Loss: 4.0998\n",
      "Epoch 19/20, Training Loss: 2.2721, Validation Loss: 4.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:18:51,566] Trial 43 finished with value: 3.928825821938669 and parameters: {'learning_rate': 0.002440022178026772, 'lambda_reg': 0.0001352934274528079, 'k': 24}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 2.2488, Validation Loss: 3.9288\n",
      "Epoch 1/20, Training Loss: 12.2196, Validation Loss: 12.6070\n",
      "Epoch 2/20, Training Loss: 7.8531, Validation Loss: 10.3034\n",
      "Epoch 3/20, Training Loss: 4.9534, Validation Loss: 7.4268\n",
      "Epoch 4/20, Training Loss: 4.3296, Validation Loss: 6.4224\n",
      "Epoch 5/20, Training Loss: 4.1745, Validation Loss: 5.9912\n",
      "Epoch 6/20, Training Loss: 4.1488, Validation Loss: 5.7809\n",
      "Epoch 7/20, Training Loss: 4.1738, Validation Loss: 5.6752\n",
      "Epoch 8/20, Training Loss: 4.2221, Validation Loss: 5.6258\n",
      "Epoch 9/20, Training Loss: 4.2811, Validation Loss: 5.6102\n",
      "Epoch 10/20, Training Loss: 4.3446, Validation Loss: 5.6169\n",
      "Epoch 11/20, Training Loss: 4.4093, Validation Loss: 5.6395\n",
      "Epoch 12/20, Training Loss: 4.4737, Validation Loss: 5.6739\n",
      "Epoch 13/20, Training Loss: 4.5370, Validation Loss: 5.7172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:19:06,797] Trial 44 finished with value: 5.610195188102157 and parameters: {'learning_rate': 0.006871367531710096, 'lambda_reg': 0.00033413384069534723, 'k': 32}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Training Loss: 4.5984, Validation Loss: 5.7671\n",
      "Early stopping triggered at epoch 14. Best Validation Loss: 5.6102\n",
      "Epoch 1/20, Training Loss: 12.3073, Validation Loss: 12.6039\n",
      "Epoch 2/20, Training Loss: 12.3073, Validation Loss: 12.6039\n",
      "Epoch 3/20, Training Loss: 12.3073, Validation Loss: 12.6039\n",
      "Epoch 4/20, Training Loss: 12.3073, Validation Loss: 12.6039\n",
      "Epoch 5/20, Training Loss: 12.3072, Validation Loss: 12.6039\n",
      "Epoch 6/20, Training Loss: 12.3072, Validation Loss: 12.6039\n",
      "Epoch 7/20, Training Loss: 12.3072, Validation Loss: 12.6039\n",
      "Epoch 8/20, Training Loss: 12.3072, Validation Loss: 12.6039\n",
      "Epoch 9/20, Training Loss: 12.3071, Validation Loss: 12.6038\n",
      "Epoch 10/20, Training Loss: 12.3071, Validation Loss: 12.6038\n",
      "Epoch 11/20, Training Loss: 12.3071, Validation Loss: 12.6038\n",
      "Epoch 12/20, Training Loss: 12.3071, Validation Loss: 12.6038\n",
      "Epoch 13/20, Training Loss: 12.3071, Validation Loss: 12.6038\n",
      "Epoch 14/20, Training Loss: 12.3070, Validation Loss: 12.6038\n",
      "Epoch 15/20, Training Loss: 12.3070, Validation Loss: 12.6038\n",
      "Epoch 16/20, Training Loss: 12.3070, Validation Loss: 12.6038\n",
      "Epoch 17/20, Training Loss: 12.3070, Validation Loss: 12.6038\n",
      "Epoch 18/20, Training Loss: 12.3069, Validation Loss: 12.6038\n",
      "Epoch 19/20, Training Loss: 12.3069, Validation Loss: 12.6038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:19:28,452] Trial 45 finished with value: 12.60383339840268 and parameters: {'learning_rate': 1.8717546935274316e-05, 'lambda_reg': 0.0001388754878712323, 'k': 41}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 12.3069, Validation Loss: 12.6038\n",
      "Epoch 1/20, Training Loss: 12.3212, Validation Loss: 12.6190\n",
      "Epoch 2/20, Training Loss: 12.3185, Validation Loss: 12.6186\n",
      "Epoch 3/20, Training Loss: 12.3121, Validation Loss: 12.6170\n",
      "Epoch 4/20, Training Loss: 12.2903, Validation Loss: 12.6096\n",
      "Epoch 5/20, Training Loss: 12.2063, Validation Loss: 12.5780\n",
      "Epoch 6/20, Training Loss: 11.8853, Validation Loss: 12.4515\n",
      "Epoch 7/20, Training Loss: 10.8536, Validation Loss: 12.0219\n",
      "Epoch 8/20, Training Loss: 8.8208, Validation Loss: 11.0517\n",
      "Epoch 9/20, Training Loss: 6.8949, Validation Loss: 9.8410\n",
      "Epoch 10/20, Training Loss: 5.6782, Validation Loss: 8.8116\n",
      "Epoch 11/20, Training Loss: 4.8944, Validation Loss: 8.0127\n",
      "Epoch 12/20, Training Loss: 4.3762, Validation Loss: 7.3984\n",
      "Epoch 13/20, Training Loss: 4.0270, Validation Loss: 6.9221\n",
      "Epoch 14/20, Training Loss: 3.7856, Validation Loss: 6.5472\n",
      "Epoch 15/20, Training Loss: 3.6146, Validation Loss: 6.2473\n",
      "Epoch 16/20, Training Loss: 3.4913, Validation Loss: 6.0037\n",
      "Epoch 17/20, Training Loss: 3.4011, Validation Loss: 5.8034\n",
      "Epoch 18/20, Training Loss: 3.3348, Validation Loss: 5.6368\n",
      "Epoch 19/20, Training Loss: 3.2862, Validation Loss: 5.4970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:19:50,530] Trial 46 finished with value: 5.378626452485868 and parameters: {'learning_rate': 0.0018213751394938742, 'lambda_reg': 0.0002493204899506392, 'k': 47}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Training Loss: 3.2507, Validation Loss: 5.3786\n",
      "Epoch 1/20, Training Loss: 12.4565, Validation Loss: 12.7647\n",
      "Epoch 2/20, Training Loss: 12.0803, Validation Loss: 12.6524\n",
      "Epoch 3/20, Training Loss: 8.7236, Validation Loss: 11.3138\n",
      "Epoch 4/20, Training Loss: 7.2099, Validation Loss: 10.0674\n",
      "Epoch 5/20, Training Loss: 7.2003, Validation Loss: 9.7325\n",
      "Epoch 6/20, Training Loss: 7.5135, Validation Loss: 9.7443\n",
      "Epoch 7/20, Training Loss: 7.8814, Validation Loss: 9.8821\n",
      "Epoch 8/20, Training Loss: 8.2440, Validation Loss: 10.0704\n",
      "Epoch 9/20, Training Loss: 8.5861, Validation Loss: 10.2775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:20:01,339] Trial 47 finished with value: 9.73246009553857 and parameters: {'learning_rate': 0.004866090746483509, 'lambda_reg': 0.0008855739369574304, 'k': 38}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Training Loss: 8.9039, Validation Loss: 10.4887\n",
      "Early stopping triggered at epoch 10. Best Validation Loss: 9.7325\n",
      "Epoch 1/20, Training Loss: 13.9931, Validation Loss: 14.2994\n",
      "Epoch 2/20, Training Loss: 13.9806, Validation Loss: 14.3242\n",
      "Epoch 3/20, Training Loss: 13.8153, Validation Loss: 14.4568\n",
      "Epoch 4/20, Training Loss: 13.2053, Validation Loss: 15.2487\n",
      "Epoch 5/20, Training Loss: 14.1676, Validation Loss: 17.2228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:20:08,044] Trial 48 finished with value: 14.29940398513581 and parameters: {'learning_rate': 0.0030792340387138886, 'lambda_reg': 0.003169704018302183, 'k': 15}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Training Loss: 16.1871, Validation Loss: 19.2361\n",
      "Early stopping triggered at epoch 6. Best Validation Loss: 14.2994\n",
      "Epoch 1/20, Training Loss: 10.4513, Validation Loss: 11.7126\n",
      "Epoch 2/20, Training Loss: 4.6418, Validation Loss: 6.9581\n",
      "Epoch 3/20, Training Loss: 3.2564, Validation Loss: 5.1629\n",
      "Epoch 4/20, Training Loss: 2.7875, Validation Loss: 4.4094\n",
      "Epoch 5/20, Training Loss: 2.5550, Validation Loss: 4.0074\n",
      "Epoch 6/20, Training Loss: 2.4207, Validation Loss: 3.7638\n",
      "Epoch 7/20, Training Loss: 2.3408, Validation Loss: 3.6074\n",
      "Epoch 8/20, Training Loss: 2.2944, Validation Loss: 3.5056\n",
      "Epoch 9/20, Training Loss: 2.2686, Validation Loss: 3.4413\n",
      "Epoch 10/20, Training Loss: 2.2559, Validation Loss: 3.4042\n",
      "Epoch 11/20, Training Loss: 2.2514, Validation Loss: 3.3874\n",
      "Epoch 12/20, Training Loss: 2.2524, Validation Loss: 3.3856\n",
      "Epoch 13/20, Training Loss: 2.2568, Validation Loss: 3.3947\n",
      "Epoch 14/20, Training Loss: 2.2632, Validation Loss: 3.4110\n",
      "Epoch 15/20, Training Loss: 2.2704, Validation Loss: 3.4319\n",
      "Epoch 16/20, Training Loss: 2.2779, Validation Loss: 3.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 16:20:26,543] Trial 49 finished with value: 3.385616004031578 and parameters: {'learning_rate': 0.008575062139789553, 'lambda_reg': 0.00013331201319424275, 'k': 22}. Best is trial 21 with value: 3.0987913564505427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Training Loss: 2.2853, Validation Loss: 3.4803\n",
      "Early stopping triggered at epoch 17. Best Validation Loss: 3.3856\n",
      "\n",
      "Best Parameters Found:\n",
      "Learning Rate: 0.004099151484173524\n",
      "Regularization: 0.00010453688227089353\n",
      "Latent Factors: 27\n",
      "Best Validation Loss: 3.0988\n",
      "\n",
      "Optuna Study Results:\n",
      "    trial  learning_rate  lambda_reg   k   val_loss\n",
      "0       0       0.005959    0.096431  25  45.545911\n",
      "1       1       0.000743    0.006593  38  13.987573\n",
      "2       2       0.002544    0.000110  28   3.646739\n",
      "3       3       0.000704    0.000894  50  12.721222\n",
      "4       4       0.005405    0.000332  47   5.527626\n",
      "5       5       0.000050    0.005194  23  14.405538\n",
      "6       6       0.000045    0.001123  15  13.181947\n",
      "7       7       0.002667    0.001261  22  11.607918\n",
      "8       8       0.000250    0.000742  46  12.706324\n",
      "9       9       0.000104    0.005297  12  16.171538\n",
      "10     10       0.000011    0.000122  34  12.604360\n",
      "11     11       0.006966    0.000141  39   3.433063\n",
      "12     12       0.001723    0.000112  39   4.527516\n",
      "13     13       0.009611    0.000273  30   5.000875\n",
      "14     14       0.001962    0.038178  40  20.366681\n",
      "15     15       0.000792    0.000288  30   9.496582\n",
      "16     16       0.002874    0.014694  18  19.225857\n",
      "17     17       0.009642    0.000116  35   3.147726\n",
      "18     18       0.009558    0.001993  35  13.519763\n",
      "19     19       0.000307    0.000437  44  12.652867\n",
      "20     20       0.004806    0.000195  34   4.074509\n",
      "21     21       0.004099    0.000105  27   3.098791\n",
      "22     22       0.001266    0.000479  43   7.638617\n",
      "23     23       0.004094    0.000180  36   3.925364\n",
      "24     24       0.009972    0.000174  27   3.875663\n",
      "25     25       0.005123    0.000589  33   7.809018\n",
      "26     26       0.001424    0.000104  41   5.247331\n",
      "27     27       0.003537    0.001903  32  13.026800\n",
      "28     28       0.006693    0.000212  19   4.339479\n",
      "29     29       0.006818    0.045052  27  28.657710\n",
      "30     30       0.000433    0.000376  37  12.609613\n",
      "31     31       0.002933    0.000108  26   3.428860\n",
      "32     32       0.003849    0.000161  25   3.757820\n",
      "33     33       0.001074    0.000241  29   7.152407\n",
      "34     34       0.002161    0.000140  24   4.170464\n",
      "35     35       0.007487    0.000312  21   5.440619\n",
      "36     36       0.003119    0.000107  26   3.331447\n",
      "37     37       0.000501    0.000105  26  12.461319\n",
      "38     38       0.002817    0.009478  31  15.070102\n",
      "39     39       0.000178    0.000624  21  12.818450\n",
      "40     40       0.000988    0.000100  18   6.951803\n",
      "41     41       0.005722    0.000158  28   3.674355\n",
      "42     42       0.003677    0.000212  37   4.301248\n",
      "43     43       0.002440    0.000135  24   3.928826\n",
      "44     44       0.006871    0.000334  32   5.610195\n",
      "45     45       0.000019    0.000139  41  12.603833\n",
      "46     46       0.001821    0.000249  47   5.378626\n",
      "47     47       0.004866    0.000886  38   9.732460\n",
      "48     48       0.003079    0.003170  15  14.299404\n",
      "49     49       0.008575    0.000133  22   3.385616\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# Define the training function with Optuna integration\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    lambda_reg = trial.suggest_loguniform('lambda_reg', 1e-4, 1e-1)\n",
    "    k = trial.suggest_int('k', 10, 50)\n",
    "    \n",
    "    # Initialize latent factor matrices\n",
    "    U = np.random.normal(scale=1./k, size=(train_normalized.shape[0], k))\n",
    "    V = np.random.normal(scale=1./k, size=(train_normalized.shape[1], k))\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    num_epochs = 20\n",
    "    patience = 5\n",
    "    U, V, _, val_loss, _ = train_mf_sgd(\n",
    "        train_matrix=train_normalized,\n",
    "        val_matrix=val_normalized,\n",
    "        U=U,\n",
    "        V=V,\n",
    "        num_epochs=num_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        lambda_reg=lambda_reg,\n",
    "        patience=patience\n",
    "    )\n",
    "    \n",
    "    # Return the validation loss (Optuna minimizes this value)\n",
    "    return val_loss\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters and validation loss\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(f\"Learning Rate: {study.best_params['learning_rate']}\")\n",
    "print(f\"Regularization: {study.best_params['lambda_reg']}\")\n",
    "print(f\"Latent Factors: {study.best_params['k']}\")\n",
    "print(f\"Best Validation Loss: {study.best_value:.4f}\")\n",
    "\n",
    "# Optional: Analyze study results\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame([\n",
    "    {'trial': t.number, **t.params, 'val_loss': t.value}\n",
    "    for t in study.trials\n",
    "])\n",
    "print(\"\\nOptuna Study Results:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results on the Test Set:\n",
      "Mean Squared Error (MSE): 1.8485\n",
      "Root Mean Squared Error (RMSE): 1.3596\n"
     ]
    }
   ],
   "source": [
    "# Function to compute predictions\n",
    "def compute_predictions(U, V):\n",
    "    \"\"\"Compute the full user-item rating prediction matrix.\"\"\"\n",
    "    return U @ V.T\n",
    "\n",
    "# Function to evaluate on the test set\n",
    "def evaluate_model(test_matrix, U, V):\n",
    "    \"\"\"Evaluate the model on the test set and compute the RMSE.\"\"\"\n",
    "    row_indices, col_indices = test_matrix.nonzero()\n",
    "    predictions = np.sum(U[row_indices] * V[col_indices], axis=1)\n",
    "    actual = test_matrix.data\n",
    "    errors = actual - predictions\n",
    "    mse = np.mean(errors ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, rmse\n",
    "\n",
    "# Compute predictions using the learned U and V\n",
    "predicted_matrix = compute_predictions(U, V)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_mse, test_rmse = evaluate_model(test_normalized, U, V)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nEvaluation Results on the Test Set:\")\n",
    "print(f\"Mean Squared Error (MSE): {test_mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Get recommendations for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for User 1:\n",
      "Movie ID: 86781, Predicted Rating: 5.85\n",
      "Movie ID: 1104, Predicted Rating: 5.82\n",
      "Movie ID: 750, Predicted Rating: 5.82\n",
      "Movie ID: 38304, Predicted Rating: 5.71\n",
      "Movie ID: 6460, Predicted Rating: 5.71\n",
      "Movie ID: 1446, Predicted Rating: 5.69\n",
      "Movie ID: 1178, Predicted Rating: 5.69\n",
      "Movie ID: 951, Predicted Rating: 5.68\n",
      "Movie ID: 3030, Predicted Rating: 5.68\n",
      "Movie ID: 5690, Predicted Rating: 5.68\n"
     ]
    }
   ],
   "source": [
    "def get_user_recommendations(user_id, user_map, item_map, U, V, user_means, train_matrix, top_n=10):\n",
    "    \"\"\"\n",
    "    Generate movie recommendations for a specific user.\n",
    "    \n",
    "    Args:\n",
    "        user_id (int): ID of the user to generate recommendations for.\n",
    "        user_map (dict): Mapping of user IDs to matrix indices.\n",
    "        item_map (dict): Mapping of movie IDs to matrix indices.\n",
    "        U (ndarray): User latent factors.\n",
    "        V (ndarray): Item latent factors.\n",
    "        user_means (ndarray): Mean rating for each user.\n",
    "        train_matrix (csr_matrix): The training matrix to check existing interactions.\n",
    "        top_n (int): Number of recommendations to return.\n",
    "    \n",
    "    Returns:\n",
    "        recommendations (list): List of tuples with (movie_id, predicted_rating).\n",
    "    \"\"\"\n",
    "    # Ensure the user exists in the training data\n",
    "    if user_id not in user_map:\n",
    "        raise ValueError(f\"User {user_id} not found in training data.\")\n",
    "    \n",
    "    # Get the user's row index in the matrix\n",
    "    user_idx = user_map[user_id]\n",
    "    \n",
    "    # Compute predictions for all movies\n",
    "    predictions = U[user_idx] @ V.T\n",
    "    \n",
    "    # Denormalize the predictions\n",
    "    predictions += user_means[user_idx]\n",
    "    \n",
    "    # Find movies the user hasn't rated yet\n",
    "    user_interactions = train_matrix.getrow(user_idx).toarray().flatten()\n",
    "    unseen_items = np.where(user_interactions == 0)[0]\n",
    "    \n",
    "    # Get predictions for unseen movies only\n",
    "    unseen_predictions = {item: predictions[item] for item in unseen_items}\n",
    "    \n",
    "    # Sort predictions by rating in descending order\n",
    "    sorted_predictions = sorted(unseen_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Map matrix indices back to movie IDs and return the top N\n",
    "    reverse_item_map = {v: k for k, v in item_map.items()}\n",
    "    recommendations = [(reverse_item_map[item_idx], rating) for item_idx, rating in sorted_predictions[:top_n]]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage:\n",
    "# Generate recommendations for user 1\n",
    "user_id = 1\n",
    "top_n = 10\n",
    "recommendations = get_user_recommendations(\n",
    "    user_id=user_id,\n",
    "    user_map=train_user_map,\n",
    "    item_map=train_item_map,\n",
    "    U=U,\n",
    "    V=V,\n",
    "    user_means=train_user_means,\n",
    "    train_matrix=train_sparse,\n",
    "    top_n=top_n\n",
    ")\n",
    "\n",
    "# Print recommendations\n",
    "print(f\"Top {top_n} recommendations for User {user_id}:\")\n",
    "for movie_id, predicted_rating in recommendations:\n",
    "    print(f\"Movie ID: {movie_id}, Predicted Rating: {predicted_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored and implemented a **Matrix Factorization-based Recommendation System** step by step, covering key aspects of building and evaluating a recommender model. Here’s a summary of what we achieved:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Problem Definition**\n",
    "We began by defining the goal: to predict user ratings for items they haven't interacted with and to recommend the top-N items based on these predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Exploration and Preprocessing**\n",
    "- Explored the structure of the dataset (`ratings.csv`).\n",
    "- Prepared a **User-Item Interaction Matrix** in a memory-efficient sparse format.\n",
    "- Normalized the data to account for user biases (mean rating per user).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Model Implementation**\n",
    "- Implemented a **Matrix Factorization Model** using Stochastic Gradient Descent (SGD).\n",
    "- Incorporated key techniques such as:\n",
    "  - **Regularization** to prevent overfitting.\n",
    "  - **Early Stopping** to stop training when the validation loss stopped improving.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Hyperparameter Tuning**\n",
    "- Used both **Grid Search** and **Optuna** to tune hyperparameters such as:\n",
    "  - Learning rate\n",
    "  - Regularization strength\n",
    "  - Number of latent factors (dimensions of $U$ and $V$).\n",
    "- Identified the best hyperparameters for our model using validation loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Model Evaluation**\n",
    "- Evaluated the model on a separate **test set** to ensure generalization.\n",
    "- Computed key metrics such as **Mean Squared Error (MSE)** and **Root Mean Squared Error (RMSE)** to assess the model’s accuracy in predicting user ratings.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Generating Recommendations**\n",
    "- Implemented a method to generate recommendations for individual users:\n",
    "  - Predicted ratings for all unseen items.\n",
    "  - Denormalized predictions to restore the original rating scale.\n",
    "  - Ranked items by predicted ratings and retrieved the top-N recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Reflection and Next Steps**\n",
    "This project demonstrated the process of building a collaborative filtering system based on matrix factorization. While effective, it comes with limitations, such as handling:\n",
    "- **Dynamic data**: New users, new items, or updated ratings require retraining or partial updates.\n",
    "- **Cold-start problems**: Sparse user-item interactions can limit the system's ability to recommend effectively.\n",
    "\n",
    "### **Future Directions**\n",
    "To overcome these limitations, we could explore:\n",
    "- **Advanced Matrix Factorization Approaches**:\n",
    "  - Incorporate online updates or hybrid models (e.g., blending content-based and collaborative filtering).\n",
    "- **Neural Networks**:\n",
    "  - Use deep learning techniques to build a more robust recommendation system.\n",
    "- **Evaluation Metrics**:\n",
    "  - Evaluate recommendations using ranking metrics like Precision@K and Recall@K to measure the quality of top-N recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "By following this systematic approach, we gained a deeper understanding of collaborative filtering and built a foundation for creating more advanced and scalable recommendation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
