{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning-Based Hybrid Recommendation System (with enriched content for users and movies)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As recommendation systems evolve, **deep learning** has emerged as a powerful tool to enhance predictions by combining collaborative filtering (CF) and content-based filtering (CBF). Traditional methods like **matrix factorization** and **similarity-based approaches** are effective but often fail to capture complex, non-linear relationships between users, items, and additional features. By leveraging deep neural networks, we can design flexible models capable of learning richer representations for users and items while incorporating metadata.\n",
    "\n",
    "This notebook will introduce **Neural Collaborative Filtering (NCF)** and a **deep hybrid model** that combines user-item interaction data with item and user content features. These methods aim to improve prediction accuracy, particularly in sparse datasets, while addressing cold-start problems.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. **Embeddings**\n",
    "Embeddings are **dense vector representations** of discrete variables like users or items. Instead of using one-hot vectors, embeddings project users and items into a continuous latent space, capturing similarities based on interaction history.\n",
    "\n",
    "- **User Embeddings**: Learn latent preferences of users.\n",
    "- **Item Embeddings**: Capture latent characteristics of items.\n",
    "\n",
    "Embeddings are trainable and updated during the training process, similar to weights in a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Neural Collaborative Filtering (NCF)**\n",
    "\n",
    "Neural Collaborative Filtering extends matrix factorization by replacing the linear dot product with a **neural network** that models complex interactions between users and items. Key components include:\n",
    "\n",
    "- **Embedding Layers**: Represent users and items as embeddings.\n",
    "- **Concatenation**: Combine user and item embeddings to create a feature vector.\n",
    "- **Multi-Layer Perceptron (MLP)**: A feedforward neural network processes the concatenated embeddings to predict user-item interaction scores.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Hybrid Models: Combining CF and Content Features**\n",
    "\n",
    "To address the cold-start problem and sparse data limitations of CF, we can incorporate **item metadata** (e.g., genre, description) and **user attributes** into the model:\n",
    "\n",
    "- **Collaborative Features**: Learned embeddings from user-item interaction data.\n",
    "- **Content-Based Features**: Preprocessed metadata, such as TF-IDF vectors or numerical attributes, fed as input to the neural network.\n",
    "\n",
    "The **deep hybrid model** concatenates these two feature sets, allowing the network to learn from both collaborative and content-based information.\n",
    "\n",
    "In this notebook I realized that I have created a recommendation model for a fixed amount of users. The embeddings depend on the number of users in the nets architecture. Here is some more info about that kind of recommendation systems:\n",
    "\n",
    "### **Fixed user dimensions: When Was It Used in Real Life?**\n",
    "\n",
    "1. **Early Collaborative Filtering Systems**:\n",
    "   - Early systems like **Matrix Factorization (e.g., SVD)** rely on fixed user and item embedding dimensions.\n",
    "   - These systems work well in environments where:\n",
    "     - The user base and catalog are relatively static.\n",
    "     - Retraining can be done frequently enough to include new users/items.\n",
    "\n",
    "2. **Small-Scale Systems**:\n",
    "   - In applications with a stable number of users/items (e.g., internal company systems or niche platforms), fixed embeddings might still be practical.\n",
    "\n",
    "3. **Static Datasets**:\n",
    "   - In benchmarks like **MovieLens**, the user/item base is fixed, so this approach is used for simplicity in academic research.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Challenges in Real-Life Applications**\n",
    "\n",
    "1. **Dynamic User/Item Bases**:\n",
    "   - Real-world systems often see frequent additions of new users and items (e.g., e-commerce, streaming platforms).\n",
    "   - Fixed embedding tables require retraining to accommodate new users or items, which can be computationally expensive and infeasible at scale.\n",
    "\n",
    "2. **Cold-Start Problems**:\n",
    "   - Fixed embeddings do not handle cold-start scenarios well because new users/items lack pre-trained embeddings.\n",
    "\n",
    "3. **Scalability**:\n",
    "   - As the number of users/items grows, embedding tables become larger, increasing memory usage and computational costs.\n",
    "\n",
    "4. **Sparse Data**:\n",
    "   - Many real-world datasets are sparse, meaning many users interact with only a small subset of items.\n",
    "   - Fixed embedding sizes don’t inherently address sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Real-Life Alternatives to Fixed Dimension Approaches**\n",
    "\n",
    "1. **Feature-Based Representations (Content-Driven)**:\n",
    "   - Instead of tying the model to fixed user/item IDs, use **feature vectors** derived from metadata (e.g., age, genre, description embeddings).\n",
    "   - **Example**: Streaming platforms like Netflix use **content-based features** to handle new users or items.\n",
    "\n",
    "2. **Hybrid Systems**:\n",
    "   - Combine fixed embeddings for frequent users/items with content-based embeddings for new users/items.\n",
    "   - **Example**: Amazon combines user interaction history with item content features to improve personalization.\n",
    "\n",
    "3. **Dynamic Embedding Models**:\n",
    "   - Models like **Deep Factorization Machines (DeepFM)** and **Wide & Deep Networks** generate embeddings dynamically based on input features, making them adaptable to changing datasets.\n",
    "\n",
    "4. **Meta-Learning for Cold Start**:\n",
    "   - Meta-learning approaches like **MAML (Model-Agnostic Meta-Learning)** train the model to generalize well to new users/items with minimal fine-tuning.\n",
    "\n",
    "5. **Continuous Learning**:\n",
    "   - Incremental learning models update embeddings as new data arrives without retraining the entire network.\n",
    "   - **Example**: Google’s recommendation systems often use real-time updates to embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Examples of Real-Life Systems**\n",
    "\n",
    "1. **Netflix**:\n",
    "   - Initially used Matrix Factorization (a fixed dimension approach) during the Netflix Prize.\n",
    "   - Transitioned to a hybrid system combining collaborative filtering and content-based approaches.\n",
    "\n",
    "2. **Amazon**:\n",
    "   - Uses a mix of collaborative filtering and item-based recommendations augmented with dynamic content-based features.\n",
    "\n",
    "3. **Spotify**:\n",
    "   - Uses user and item embeddings enriched with audio features, user preferences, and metadata.\n",
    "\n",
    "4. **YouTube**:\n",
    "   - Combines user interaction embeddings with video metadata for recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "The fixed-dimension approach has been used historically, especially in systems with static datasets or user bases. However, in modern, dynamic environments, it is considered inflexible. Most real-life systems now adopt **feature-based representations**, **hybrid models**, or **incremental learning** to handle dynamic user/item bases and cold-start scenarios effectively.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Workflow for Building a Deep Hybrid Model\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - Preprocess user-item interaction data (e.g., rating matrix).\n",
    "   - Extract and preprocess item content features (e.g., TF-IDF for text, numerical metadata).\n",
    "\n",
    "2. **Neural Collaborative Filtering (NCF)**:\n",
    "   - Design embedding layers for users and items.\n",
    "   - Train a basic NCF model using user-item interactions.\n",
    "\n",
    "3. **Incorporating Content-Based Features**:\n",
    "   - Merge item metadata or user features with embeddings.\n",
    "   - Pass the combined features through an MLP for final predictions.\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - Use metrics like RMSE, Precision@K, or Hit Ratio to evaluate the model's performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective of This Notebook\n",
    "\n",
    "ToDo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of unique users: 610, Number of unique items: 9724\n",
      "Item embeddings loaded.\n",
      "\n",
      "Sample Movie Features:\n",
      "Type: <class 'dict'>\n",
      "\n",
      "First 5 keys:\n",
      "[0, 1, 2, 3, 4]\n",
      "\n",
      "First 5 values (shapes and examples):\n",
      "Key: 0, Value Shape: (768,), Value Sample: [ 0.06085226  0.05872172 -0.01586064 -0.03862809  0.01193655]\n",
      "Key: 1, Value Shape: (768,), Value Sample: [ 0.08129846  0.01215789 -0.00998183 -0.0026806   0.01916343]\n",
      "Key: 2, Value Shape: (768,), Value Sample: [ 0.07269978  0.03968232  0.01705796 -0.03007039  0.01678096]\n",
      "Key: 3, Value Shape: (768,), Value Sample: [ 0.06406538  0.00694661 -0.00816733 -0.02460494 -0.01866039]\n",
      "Key: 4, Value Shape: (768,), Value Sample: [ 0.03503072  0.0034123  -0.00281779 -0.03417671 -0.06114816]\n",
      "\n",
      "Sample Combined Item Features (Transformer Embeddings):\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.060852  0.058722 -0.015861 -0.038628  0.011937  0.013824  0.015017   \n",
      "1  0.081298  0.012158 -0.009982 -0.002681  0.019163 -0.008715 -0.032311   \n",
      "2  0.072700  0.039682  0.017058 -0.030070  0.016781  0.027756 -0.040944   \n",
      "3  0.064065  0.006947 -0.008167 -0.024605 -0.018660  0.023892 -0.057732   \n",
      "4  0.035031  0.003412 -0.002818 -0.034177 -0.061148  0.020173 -0.039333   \n",
      "\n",
      "        7         8         9    ...       758       759       760       761  \\\n",
      "0 -0.026930 -0.036154 -0.006908  ... -0.028537  0.024825 -0.069737  0.028449   \n",
      "1 -0.005040 -0.013281  0.030031  ... -0.035930  0.026338 -0.113176  0.002544   \n",
      "2  0.039113  0.043561 -0.013289  ... -0.012416  0.005059 -0.018671  0.000386   \n",
      "3  0.009338 -0.005190  0.009401  ... -0.044603 -0.004084 -0.069390 -0.010621   \n",
      "4  0.063187  0.027249 -0.005937  ... -0.013261  0.047464 -0.025691  0.004392   \n",
      "\n",
      "        762       763       764       765       766       767  \n",
      "0  0.028543  0.089822  0.002661 -0.025497  0.034491 -0.052370  \n",
      "1  0.026950 -0.033344  0.012914 -0.002634  0.015404 -0.006947  \n",
      "2  0.002897 -0.052728  0.019632 -0.003852  0.050511  0.002378  \n",
      "3 -0.023677  0.032590  0.006867 -0.036226  0.037471 -0.061078  \n",
      "4  0.010980  0.005588  0.010879 -0.011338  0.022924 -0.004739  \n",
      "\n",
      "[5 rows x 768 columns]\n",
      "User embeddings loaded.\n",
      "\n",
      "Sample User Features:\n",
      "Type: <class 'dict'>\n",
      "\n",
      "First 5 keys:\n",
      "[0, 1, 2, 3, 4]\n",
      "\n",
      "First 5 values (shapes and examples):\n",
      "Key: 0, Value Shape: (768,), Value Sample: [ 0.06323417  0.05469479 -0.0112031   0.01049597 -0.00690331]\n",
      "Key: 1, Value Shape: (768,), Value Sample: [ 0.07237072  0.05549701 -0.0149012   0.01781037 -0.00210234]\n",
      "Key: 2, Value Shape: (768,), Value Sample: [ 0.06053733  0.05044693 -0.01185638  0.00597226 -0.00700782]\n",
      "Key: 3, Value Shape: (768,), Value Sample: [ 0.06408209  0.05614042 -0.01578992  0.00971217 -0.00243444]\n",
      "Key: 4, Value Shape: (768,), Value Sample: [ 0.05750029  0.04857986 -0.01605756  0.00918341 -0.00676748]\n",
      "\n",
      "Sample Combined User Features (Transformer Embeddings):\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0  0.063234  0.054695 -0.011203  0.010496 -0.006903 -0.025627  0.001266   \n",
      "1  0.072371  0.055497 -0.014901  0.017810 -0.002102 -0.027858 -0.008955   \n",
      "2  0.060537  0.050447 -0.011856  0.005972 -0.007008 -0.024111  0.014154   \n",
      "3  0.064082  0.056140 -0.015790  0.009712 -0.002434 -0.025924 -0.002200   \n",
      "4  0.057500  0.048580 -0.016058  0.009183 -0.006767 -0.028095  0.003550   \n",
      "\n",
      "        7         8         9    ...       758       759       760       761  \\\n",
      "0  0.007551 -0.021270 -0.009413  ...  0.050303  0.036862 -0.071625  0.032331   \n",
      "1  0.010414 -0.026534 -0.012734  ...  0.039127  0.041243 -0.062454  0.028189   \n",
      "2  0.005733 -0.025987 -0.017484  ...  0.054453  0.041200 -0.076459  0.034314   \n",
      "3  0.010526 -0.018674 -0.006985  ...  0.046840  0.044815 -0.068659  0.022331   \n",
      "4  0.008187 -0.023101 -0.007941  ...  0.052925  0.041987 -0.076024  0.033305   \n",
      "\n",
      "        762       763       764       765       766       767  \n",
      "0 -0.024515 -0.015044 -0.028561  0.047200 -0.008740 -0.001055  \n",
      "1 -0.031655 -0.016685 -0.029405  0.043523 -0.019940 -0.004420  \n",
      "2 -0.020931 -0.024046 -0.030649  0.044607 -0.005812 -0.004491  \n",
      "3 -0.028575 -0.016325 -0.026192  0.046129 -0.009431 -0.000694  \n",
      "4 -0.026339 -0.023132 -0.027728  0.048318 -0.004748 -0.006324  \n",
      "\n",
      "[5 rows x 768 columns]\n",
      "\n",
      "User-Level Split: Train = 70312, Validation = 15102, Test = 15422\n",
      "\n",
      "Data Preparation Complete. Dataloaders Ready!\n",
      "Train Dataset: 70312 samples\n",
      "Validation Dataset: 15102 samples\n",
      "Test Dataset: 15422 samples\n"
     ]
    }
   ],
   "source": [
    "# Watch for changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from codebase.prep_data import ContentFeatureEngineeringUsingSBERT, DataManager, UserFeatureEngineeringUsingSBERT, MovieLensDataset, split_data_by_user, inspect_dictionary\n",
    "import sys\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Configuration\n",
    "RATINGS_PATH = 'data/ml-latest-small/ratings.csv'\n",
    "MOVIES_ENRICHED_PATH = 'data/ml-latest-small/movies_enriched.csv'\n",
    "USER_ENRICHED_PATH = 'data/ml-latest-small/users_enriched.csv'\n",
    "TAGS_PATH = 'data/ml-latest-small/tags.csv'\n",
    "BATCH_SIZE = 128\n",
    "CONTENT_DIM = 768\n",
    "\n",
    "# Paths for saved embeddings\n",
    "ITEM_EMBEDDINGS_PATH = \"data/ml-latest-small/item_embeddings.npy\"\n",
    "ITEM_MAPPING_PATH = \"data/ml-latest-small/item_mapping.npy\"\n",
    "USER_EMBEDDINGS_PATH = \"data/ml-latest-small/user_embeddings.npy\"\n",
    "USER_MAPPING_PATH = \"data/ml-latest-small/user_mapping.npy\"\n",
    "\n",
    "# Step 1: Load ratings\n",
    "data_manager = DataManager(RATINGS_PATH)\n",
    "ratings = data_manager.load_ratings()\n",
    "tags = pd.read_csv(TAGS_PATH)\n",
    "\n",
    "# Encode user-item interactions\n",
    "ratings = data_manager.encode_interactions(ratings)\n",
    "n_users, n_items = data_manager.get_user_item_count(ratings)\n",
    "print(f\"\\nNumber of unique users: {n_users}, Number of unique items: {n_items}\")\n",
    "\n",
    "# Step 2: Prepare item content features\n",
    "valid_movie_ids = ratings['movieId'].unique()\n",
    "feature_engineer = ContentFeatureEngineeringUsingSBERT(MOVIES_ENRICHED_PATH, data_manager.item_encoder, device)\n",
    "\n",
    "# Load or create item embeddings\n",
    "if os.path.exists(ITEM_EMBEDDINGS_PATH) and os.path.exists(ITEM_MAPPING_PATH):\n",
    "    # we need this for later\n",
    "    movies_enriched = feature_engineer.load_enriched_movies(valid_movie_ids)\n",
    "    movies_enriched = feature_engineer.prepare_combined_features(movies_enriched)\n",
    "\n",
    "    item_content_features = np.load(ITEM_EMBEDDINGS_PATH)\n",
    "    movie_ids = np.load(ITEM_MAPPING_PATH)\n",
    "    movie_id_to_content = {movie_id: embedding for movie_id, embedding in zip(movie_ids, item_content_features)}\n",
    "    print(\"Item embeddings loaded.\")\n",
    "else:\n",
    "    movies_enriched = feature_engineer.load_enriched_movies(valid_movie_ids)\n",
    "    movies_enriched = feature_engineer.prepare_combined_features(movies_enriched)\n",
    "    movie_id_to_content, item_content_features = feature_engineer.create_content_features(movies_enriched)\n",
    "    np.save(ITEM_EMBEDDINGS_PATH, item_content_features)\n",
    "    np.save(ITEM_MAPPING_PATH, np.array(list(movie_id_to_content.keys())))\n",
    "    print(\"Item embeddings calculated and saved.\")\n",
    "\n",
    "print(\"\\nSample Movie Features:\")\n",
    "inspect_dictionary(movie_id_to_content)\n",
    "\n",
    "print(\"\\nSample Combined Item Features (Transformer Embeddings):\")\n",
    "print(pd.DataFrame(item_content_features[:5]).head())\n",
    "\n",
    "# Step 3: Prepare user content features\n",
    "valid_user_ids = ratings['userId'].unique()  # Get userIds from ratings\n",
    "user_feature_engineer = UserFeatureEngineeringUsingSBERT(USER_ENRICHED_PATH, data_manager.user_encoder, device)\n",
    "\n",
    "# Load or create user embeddings\n",
    "if os.path.exists(USER_EMBEDDINGS_PATH) and os.path.exists(USER_MAPPING_PATH):\n",
    "    # we need this for later\n",
    "    users_enriched = user_feature_engineer.load_enriched_users(valid_user_ids)\n",
    "    users_enriched = user_feature_engineer.prepare_combined_features(users_enriched)\n",
    "\n",
    "    user_content_features = np.load(USER_EMBEDDINGS_PATH)\n",
    "    user_ids = np.load(USER_MAPPING_PATH)\n",
    "    user_id_to_content = {user_id: embedding for user_id, embedding in zip(user_ids, user_content_features)}\n",
    "    print(\"User embeddings loaded.\")\n",
    "else:\n",
    "    users_enriched = user_feature_engineer.load_enriched_users(valid_user_ids)\n",
    "    users_enriched = user_feature_engineer.prepare_combined_features(users_enriched)\n",
    "    user_id_to_content, user_content_features = user_feature_engineer.create_user_features(users_enriched)\n",
    "    np.save(USER_EMBEDDINGS_PATH, user_content_features)\n",
    "    np.save(USER_MAPPING_PATH, np.array(list(user_id_to_content.keys())))\n",
    "    print(\"User embeddings calculated and saved.\")\n",
    "\n",
    "print(\"\\nSample User Features:\")\n",
    "inspect_dictionary(user_id_to_content)\n",
    "\n",
    "print(\"\\nSample Combined User Features (Transformer Embeddings):\")\n",
    "print(pd.DataFrame(user_content_features[:5]).head())\n",
    "\n",
    "# Step 4: Split data by user\n",
    "train_data, val_data, test_data = split_data_by_user(ratings)\n",
    "print(f\"\\nUser-Level Split: Train = {len(train_data)}, Validation = {len(val_data)}, Test = {len(test_data)}\")\n",
    "\n",
    "# Step 5: Create datasets and dataloaders\n",
    "train_dataset = MovieLensDataset(train_data, user_id_to_content, movie_id_to_content)\n",
    "val_dataset = MovieLensDataset(val_data, user_id_to_content, movie_id_to_content)\n",
    "test_dataset = MovieLensDataset(test_data, user_id_to_content, movie_id_to_content)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=8)\n",
    "\n",
    "print(\"\\nData Preparation Complete. Dataloaders Ready!\")\n",
    "print(f\"Train Dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation Dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test Dataset: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the embeddings so we not need to recreate them again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item embeddings saved to data/ml-latest-small/item_embeddings.npy\n",
      "Item mapping saved to data/ml-latest-small/item_mapping.npy\n",
      "User embeddings saved to data/ml-latest-small/user_embeddings.npy\n",
      "User mapping saved to data/ml-latest-small/user_mapping.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save item embeddings and mapping\n",
    "item_embeddings_path = \"data/ml-latest-small/item_embeddings.npy\"\n",
    "item_mapping_path = \"data/ml-latest-small/item_mapping.npy\"\n",
    "\n",
    "np.save(item_embeddings_path, item_content_features)\n",
    "np.save(item_mapping_path, np.array(list(movie_id_to_content.keys())))\n",
    "\n",
    "print(f\"Item embeddings saved to {item_embeddings_path}\")\n",
    "print(f\"Item mapping saved to {item_mapping_path}\")\n",
    "\n",
    "# Save user embeddings and mapping\n",
    "user_embeddings_path = \"data/ml-latest-small/user_embeddings.npy\"\n",
    "user_mapping_path = \"data/ml-latest-small/user_mapping.npy\"\n",
    "\n",
    "np.save(user_embeddings_path, user_content_features)\n",
    "np.save(user_mapping_path, np.array(list(user_id_to_content.keys())))\n",
    "\n",
    "print(f\"User embeddings saved to {user_embeddings_path}\")\n",
    "print(f\"User mapping saved to {user_mapping_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the embeddings in the data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Batch 1\n",
      "\n",
      "Item 1:\n",
      "  User Index: 0\n",
      "  User Embedding (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  User Embedding from Dictionary (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  Item Index (Encoded Movie ID): 2523\n",
      "  Item Embedding (First 5 Dimensions): [0.059084683656692505, 0.058120932430028915, 0.02137676067650318, 0.019144346937537193, -0.05452856048941612]\n",
      "  Item Embedding from Dictionary (First 5 Dimensions): [0.059084683656692505, 0.058120932430028915, 0.02137676067650318, 0.019144346937537193, -0.05452856048941612]\n",
      "  Rating: 5.0\n",
      "\n",
      "Item 2:\n",
      "  User Index: 0\n",
      "  User Embedding (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  User Embedding from Dictionary (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  Item Index (Encoded Movie ID): 1223\n",
      "  Item Embedding (First 5 Dimensions): [0.06987378746271133, 0.029825683683156967, -0.03166566044092178, 0.024813158437609673, -0.04707203432917595]\n",
      "  Item Embedding from Dictionary (First 5 Dimensions): [0.06987378746271133, 0.029825683683156967, -0.03166566044092178, 0.024813158437609673, -0.04707203432917595]\n",
      "  Rating: 5.0\n",
      "\n",
      "Item 3:\n",
      "  User Index: 0\n",
      "  User Embedding (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  User Embedding from Dictionary (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  Item Index (Encoded Movie ID): 2392\n",
      "  Item Embedding (First 5 Dimensions): [0.06530790030956268, 0.03149483725428581, 0.021633848547935486, -0.02256925217807293, -0.03319326788187027]\n",
      "  Item Embedding from Dictionary (First 5 Dimensions): [0.06530790030956268, 0.03149483725428581, 0.021633848547935486, -0.02256925217807293, -0.03319326788187027]\n",
      "  Rating: 1.0\n",
      "\n",
      "Item 4:\n",
      "  User Index: 0\n",
      "  User Embedding (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  User Embedding from Dictionary (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  Item Index (Encoded Movie ID): 1234\n",
      "  Item Embedding (First 5 Dimensions): [0.02543855644762516, 0.045203737914562225, -0.013152184896171093, -0.035273920744657516, 0.0026030782610177994]\n",
      "  Item Embedding from Dictionary (First 5 Dimensions): [0.02543855644762516, 0.045203737914562225, -0.013152184896171093, -0.035273920744657516, 0.0026030782610177994]\n",
      "  Rating: 3.0\n",
      "\n",
      "Item 5:\n",
      "  User Index: 0\n",
      "  User Embedding (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  User Embedding from Dictionary (First 5 Dimensions): [0.06323417276144028, 0.05469479411840439, -0.011203102767467499, 0.010495969094336033, -0.006903305184096098]\n",
      "  Item Index (Encoded Movie ID): 2458\n",
      "  Item Embedding (First 5 Dimensions): [0.08580240607261658, 0.0015979306772351265, 0.02330714650452137, -0.01567155309021473, -0.01604613848030567]\n",
      "  Item Embedding from Dictionary (First 5 Dimensions): [0.08580240607261658, 0.0015979306772351265, 0.02330714650452137, -0.01567155309021473, -0.01604613848030567]\n",
      "  Rating: 5.0\n",
      "Validating Batch 1\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2523: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1223: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2392: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1234: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2458: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1742: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 963: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 723: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 926: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 136: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1703: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 968: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 938: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 906: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2760: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2284: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 510: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 43: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1754: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2370: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 461: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 3668: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 924: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 828: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 124: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2576: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 995: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 615: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 2301: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 130: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1109: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 485: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1865: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1486: Embedding matches!\n",
      "User ID 0: Embedding matches!\n",
      "Movie ID 1873: Embedding matches!\n",
      "User ID 1: Embedding matches!\n",
      "Movie ID 7679: Embedding matches!\n",
      "User ID 1: Embedding matches!\n",
      "Movie ID 1283: Embedding matches!\n",
      "User ID 1: Embedding matches!\n",
      "Movie ID 8663: Embedding matches!\n",
      "User ID 1: Embedding matches!\n",
      "Movie ID 8491: Embedding matches!\n",
      "User ID 1: Embedding matches!\n",
      "Movie ID 7398: Embedding matches!\n",
      "User ID 2: Embedding matches!\n",
      "Movie ID 565: Embedding matches!\n",
      "User ID 2: Embedding matches!\n",
      "Movie ID 2941: Embedding matches!\n",
      "User ID 2: Embedding matches!\n",
      "Movie ID 1552: Embedding matches!\n",
      "User ID 2: Embedding matches!\n",
      "Movie ID 461: Embedding matches!\n",
      "User ID 2: Embedding matches!\n",
      "Movie ID 3661: Embedding matches!\n",
      "User ID 2: Embedding matches!\n",
      "Movie ID 1823: Embedding matches!\n",
      "User ID 3: Embedding matches!\n",
      "Movie ID 977: Embedding matches!\n",
      "User ID 3: Embedding matches!\n",
      "Movie ID 1403: Embedding matches!\n",
      "User ID 3: Embedding matches!\n",
      "Movie ID 2200: Embedding matches!\n",
      "User ID 3: Embedding matches!\n",
      "Movie ID 2306: Embedding matches!\n"
     ]
    }
   ],
   "source": [
    "# Function to check the first n items of a batch (including user and item embeddings)\n",
    "def inspect_batch(test_loader, user_id_to_content, movie_id_to_content, n=5):\n",
    "    # Get one batch from the test_loader\n",
    "    for batch_idx, (user_indices, item_indices, user_content_features, item_content_features, ratings) in enumerate(test_loader):\n",
    "        print(f\"Inspecting Batch {batch_idx + 1}\")\n",
    "        \n",
    "        # Print the first n items in the batch\n",
    "        for i in range(min(n, len(user_indices))):  # Ensure we don't exceed batch size\n",
    "            user_id = user_indices[i].item()\n",
    "            movie_id = item_indices[i].item()\n",
    "            \n",
    "            print(f\"\\nItem {i + 1}:\")\n",
    "            print(f\"  User Index: {user_id}\")\n",
    "            print(f\"  User Embedding (First 5 Dimensions): {user_content_features[i][:5].tolist()}\")\n",
    "            print(f\"  User Embedding from Dictionary (First 5 Dimensions): {user_id_to_content[user_id][:5].tolist()}\")\n",
    "            print(f\"  Item Index (Encoded Movie ID): {movie_id}\")\n",
    "            print(f\"  Item Embedding (First 5 Dimensions): {item_content_features[i][:5].tolist()}\")\n",
    "            print(f\"  Item Embedding from Dictionary (First 5 Dimensions): {movie_id_to_content[movie_id][:5].tolist()}\")\n",
    "            print(f\"  Rating: {ratings[i].item()}\")\n",
    "        \n",
    "        # Only inspect the first batch\n",
    "        break\n",
    "\n",
    "# Inspect the first batch of test_loader\n",
    "inspect_batch(test_loader, user_id_to_content, movie_id_to_content, n=5)\n",
    "\n",
    "# Function to validate embeddings (both user and item embeddings)\n",
    "def validate_embeddings(test_loader, user_id_to_content, movie_id_to_content, n=5):\n",
    "    for batch_idx, (user_indices, item_indices, user_content_features, item_content_features, ratings) in enumerate(test_loader):\n",
    "        print(f\"Validating Batch {batch_idx + 1}\")\n",
    "        \n",
    "        for i in range(min(n, len(user_indices))):\n",
    "            user_id = user_indices[i].item()\n",
    "            movie_id = item_indices[i].item()\n",
    "            \n",
    "            # Validate user embedding\n",
    "            embedding_from_user_dataloader = user_content_features[i].tolist()\n",
    "            embedding_from_user_dict = user_id_to_content[user_id]\n",
    "            assert np.allclose(embedding_from_user_dataloader, embedding_from_user_dict), \\\n",
    "                f\"Mismatch for User ID: {user_id}\"\n",
    "            print(f\"User ID {user_id}: Embedding matches!\")\n",
    "            \n",
    "            # Validate movie embedding\n",
    "            embedding_from_movie_dataloader = item_content_features[i].tolist()\n",
    "            embedding_from_movie_dict = movie_id_to_content[movie_id]\n",
    "            assert np.allclose(embedding_from_movie_dataloader, embedding_from_movie_dict), \\\n",
    "                f\"Mismatch for Movie ID: {movie_id}\"\n",
    "            print(f\"Movie ID {movie_id}: Embedding matches!\")\n",
    "        \n",
    "        # Only validate the first batch\n",
    "        break\n",
    "\n",
    "# Validate the embeddings for both users and items\n",
    "validate_embeddings(test_loader, user_id_to_content, movie_id_to_content, n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the distribution of the users in our datasets. Ideally they should be **equally** distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users:\n",
      "Train: 610\n",
      "Val:   610\n",
      "Test:  610\n",
      "\n",
      "Overlap between datasets:\n",
      "Users in all three (Train, Val, Test): 610\n",
      "Users only in Train: 0\n",
      "Users only in Val: 0\n",
      "Users only in Test: 0\n",
      "Users in Train & Val but not Test: 0\n",
      "Users in Train & Test but not Val: 0\n",
      "Users in Val & Test but not Train: 0\n",
      "\n",
      "Average number of ratings per user:\n",
      "Train: 115.2655737704918\n",
      "Val:   24.757377049180327\n",
      "Test:  25.281967213114754\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have train_data, val_data, and test_data as DataFrames from your preparation step\n",
    "\n",
    "# Number of unique users in each dataset\n",
    "print(\"Number of unique users:\")\n",
    "print(\"Train:\", train_data['user_id'].nunique())\n",
    "print(\"Val:  \", val_data['user_id'].nunique())\n",
    "print(\"Test: \", test_data['user_id'].nunique())\n",
    "\n",
    "# Get sets of users for each dataset\n",
    "train_users = set(train_data['user_id'].unique())\n",
    "val_users = set(val_data['user_id'].unique())\n",
    "test_users = set(test_data['user_id'].unique())\n",
    "\n",
    "# Check overlaps\n",
    "print(\"\\nOverlap between datasets:\")\n",
    "print(\"Users in all three (Train, Val, Test):\", len(train_users & val_users & test_users))\n",
    "print(\"Users only in Train:\", len(train_users - val_users - test_users))\n",
    "print(\"Users only in Val:\", len(val_users - train_users - test_users))\n",
    "print(\"Users only in Test:\", len(test_users - train_users - val_users))\n",
    "\n",
    "# Check two-set overlaps\n",
    "print(\"Users in Train & Val but not Test:\", len((train_users & val_users) - test_users))\n",
    "print(\"Users in Train & Test but not Val:\", len((train_users & test_users) - val_users))\n",
    "print(\"Users in Val & Test but not Train:\", len((val_users & test_users) - train_users))\n",
    "\n",
    "# Compute number of ratings per user in each dataset\n",
    "user_train_counts = train_data['user_id'].value_counts()\n",
    "user_val_counts = val_data['user_id'].value_counts()\n",
    "user_test_counts = test_data['user_id'].value_counts()\n",
    "\n",
    "print(\"\\nAverage number of ratings per user:\")\n",
    "print(\"Train:\", user_train_counts.mean())\n",
    "print(\"Val:  \", user_val_counts.mean())\n",
    "print(\"Test: \", user_test_counts.mean())\n",
    "\n",
    "# Optionally, visualize the distribution of ratings per user\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(user_train_counts, bins=50, alpha=0.5, label='Train')\n",
    "plt.hist(user_val_counts, bins=50, alpha=0.5, label='Val')\n",
    "plt.hist(user_test_counts, bins=50, alpha=0.5, label='Test')\n",
    "plt.xlabel(\"Number of ratings per user\")\n",
    "plt.ylabel(\"Count of users\")\n",
    "plt.title(\"Distribution of user ratings across Train, Val, and Test sets\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Code Explanation: Data Preparation for Deep Hybrid Recommendation System\n",
    "\n",
    "ToDo: Fill out section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But what is with the unrated movies that have been excluded in the content matrix (because they have no ratings)?\n",
    "\n",
    "### 1. Why Include Unrated Movies?\n",
    "\n",
    "In a typical recommendation system, **unrated movies** (i.e., movies with no user ratings) are often ignored because collaborative filtering (CF) relies on user-item interactions. However, excluding these movies leads to:\n",
    "\n",
    "1. **Cold-Start Problem**: New or unseen movies cannot be recommended because they lack historical ratings.\n",
    "2. **Loss of Content Information**: Movies often have meaningful **content features** (e.g., genres, tags) that can still provide recommendations.\n",
    "\n",
    "By including unrated movies, we ensure that the system is capable of **exploring new content** and delivering diverse recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Content-Based Approach for Unrated Movies\n",
    "\n",
    "#### How it Works:\n",
    "For unrated movies, recommendations rely on **content similarity** rather than collaborative filtering:\n",
    "- **Content Features**: Tags, genres, or textual metadata are processed (e.g., using TF-IDF) to form feature vectors.\n",
    "- **Similarity**: Unrated movies are compared to rated movies using **cosine similarity** or other similarity metrics.\n",
    "- **Recommendation**: Movies with the highest content similarity to the user's watched/rated movies are recommended.\n",
    "\n",
    "#### Key Points:\n",
    "1. **No Learning Required**: Content-based recommendations for unrated movies require no model training.\n",
    "2. **Similarity-Based**: Recommendations are purely based on similarity between content features.\n",
    "3. **Independent of Ratings**: Unrated movies are handled differently from rated movies because there is no user-item interaction.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Combining Rated and Unrated Movies\n",
    "\n",
    "To unify recommendations for both rated and unrated movies, we can combine:\n",
    "1. **Collaborative Filtering Predictions**: Predict ratings for rated movies using user-item interaction learning (e.g., matrix factorization).\n",
    "2. **Content Similarity Scores**: Compute similarity-based scores for unrated movies using content features.\n",
    "\n",
    "#### Combined Score Formula:\n",
    "The final score for each movie is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Final Score} = \\alpha \\times \\text{CF Prediction} + (1 - \\alpha) \\times \\text{Content Similarity}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **CF Prediction**: Predicted ratings from collaborative filtering.\n",
    "- **Content Similarity**: Similarity score based on content features.\n",
    "- **\\alpha**: Weight factor (e.g., 0.7 for collaborative filtering, 0.3 for content similarity).\n",
    "\n",
    "#### Steps to Handle Combined Recommendations:\n",
    "1. Compute predicted ratings for all movies (including unrated ones).\n",
    "2. For movies with no user interactions:\n",
    "   - Use their content similarity score instead of collaborative filtering predictions.\n",
    "3. Rank all movies based on the combined score.\n",
    "4. Exclude movies already rated by the user.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Handling Unrated Movies from a User's Perspective\n",
    "\n",
    "From the perspective of a user, unrated movies can be seen as:\n",
    "\n",
    "1. **\"You Might Also Like\" Suggestions**:\n",
    "   - These are recommendations based on content features (e.g., genres or tags) similar to movies the user has already interacted with.\n",
    "\n",
    "2. **Cold-Start Recommendations**:\n",
    "   - For new movies with no ratings, the system can still recommend them based on their **content similarity**.\n",
    "\n",
    "3. **Exploration Opportunities**:\n",
    "   - Unrated movies help users discover new content, encouraging exploration beyond their typical preferences.\n",
    "\n",
    "4. **Fallback for Sparse Users**:\n",
    "   - For users with very few ratings, content-based recommendations ensure the system can still provide meaningful suggestions.\n",
    "\n",
    "#### Comparison Between Rated and Unrated Movies:\n",
    "\n",
    "| Aspect                         | Rated Movies                        | Unrated Movies                      |\n",
    "|--------------------------------|------------------------------------|------------------------------------|\n",
    "| **Source of Recommendation**   | Collaborative Filtering (CF)       | Content-Based Filtering (Similarity)|\n",
    "| **Dependency**                 | Requires historical user ratings   | Relies only on content features     |\n",
    "| **Learning Requirement**       | Requires model training            | No learning required                |\n",
    "| **Cold-Start Capability**      | Not effective for new items        | Effective for new or unseen items   |\n",
    "| **Diversity**                  | Often biased by user preferences   | Promotes content-driven exploration |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Benefits of Including Unrated Movies\n",
    "\n",
    "- **Cold-Start Problem Solved**: New movies can still be recommended based on their content features.\n",
    "- **Increased Coverage**: The system considers all available movies, not just those with ratings.\n",
    "- **Diverse Recommendations**: Content-based methods ensure users are exposed to new and relevant content.\n",
    "\n",
    "By combining collaborative filtering and content-based approaches, we create a unified system capable of handling both rated and unrated movies effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Unrated movies are recommended based on **content similarity** rather than user interactions. By combining collaborative filtering predictions for rated movies and content-based scores for unrated movies, we ensure a robust, unified recommendation system that improves coverage, diversity, and user satisfaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo: Handle the unrated movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Neural Collaborative Filtering (NCF)\n",
    "\n",
    "Neural Collaborative Filtering (NCF) is a deep learning-based approach to collaborative filtering that replaces the traditional matrix factorization with a neural network to model user-item interactions. The steps to implement NCF in PyTorch are as follows:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Define the Problem**\n",
    "- **Objective**: Predict user-item interactions (ratings or implicit feedback) using a neural network.\n",
    "- **Inputs**:\n",
    "  - **User Embeddings**: Encoded representations for users.\n",
    "  - **Item Embeddings**: Encoded representations for items.\n",
    "  - **Optional**: Item content features (e.g., genres, tags) can be integrated later for a hybrid model.\n",
    "- **Output**: A predicted interaction score (e.g., a rating or a probability of interaction).\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Prepare the Data**\n",
    "- Data is already prepared from the previous steps:\n",
    "  - User and item indices are encoded using `LabelEncoder`.\n",
    "  - Train, validation, and test splits have been created using temporal ordering.\n",
    "  - Item content features (TF-IDF vectors) are available for future hybridization.\n",
    "- Dataloaders for train, validation, and test datasets are ready.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Build the NCF Model**\n",
    "Design the Neural Collaborative Filtering architecture. The model will include:\n",
    "1. **Embedding Layers**:\n",
    "   - Learn low-dimensional dense representations for users and items.\n",
    "2. **Concatenation**:\n",
    "   - Combine the user and item embeddings to form the input to the neural network.\n",
    "3. **Fully Connected Layers**:\n",
    "   - Pass the concatenated embeddings through multiple dense (linear) layers.\n",
    "   - Use activation functions (e.g., ReLU) between layers.\n",
    "4. **Output Layer**:\n",
    "   - Predict a single score (e.g., a rating) with a suitable activation function:\n",
    "     - **Regression Task** (e.g., predicting ratings): Linear output.\n",
    "     - **Binary Classification** (e.g., implicit feedback): Sigmoid activation for probability.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Train the NCF Model**\n",
    "- Define a loss function:\n",
    "  - **Mean Squared Error (MSE)** for regression tasks.\n",
    "  - **Binary Cross-Entropy Loss** for implicit feedback.\n",
    "- Choose an optimizer (e.g., Adam).\n",
    "- Use backpropagation to update the model parameters.\n",
    "- Train the model for a specified number of epochs and evaluate on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Evaluate the Model**\n",
    "- Use evaluation metrics suitable for the problem type:\n",
    "  - **Regression**:\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "    - Mean Absolute Error (MAE)\n",
    "  - **Ranking/Implicit Feedback**:\n",
    "    - Precision@K\n",
    "    - Recall@K\n",
    "    - NDCG (Normalized Discounted Cumulative Gain)\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Predict and Recommend**\n",
    "- For each user, generate predictions for all unrated movies.\n",
    "- Rank the predictions to recommend the top-N movies.\n",
    "- Combine this with content-based filtering scores later to improve diversity and handle cold-start issues.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary of the Steps**\n",
    "1. Define the problem and clarify inputs/outputs.\n",
    "2. Prepare the data (user indices, item indices, content features).\n",
    "3. Build the NCF model with embedding layers, fully connected layers, and an output layer.\n",
    "4. Train the model using appropriate loss functions and optimizers.\n",
    "5. Evaluate the model using metrics like RMSE, Precision@K, or Recall@K.\n",
    "6. Generate predictions for unrated items and recommend the top-N items for each user.\n",
    "\n",
    "---\n",
    "\n",
    "With these steps, we will implement NCF as the next part of our recommendation system pipeline in PyTorch. 🚀\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Embeddings in NCF: Why Are User and Item Embeddings Needed?\n",
    "\n",
    "In Neural Collaborative Filtering (NCF), **embeddings** for users and items play a crucial role in transforming **discrete indices** into **learnable representations** that capture relationships and patterns. Here's why embeddings are essential and what they add to the network:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What Are Embeddings?**\n",
    "- An **embedding** is a low-dimensional, dense vector representation of a categorical variable (e.g., users or items).\n",
    "- In NCF:\n",
    "  - Each **user** index is mapped to a unique embedding vector.\n",
    "  - Each **item** index is mapped to a unique embedding vector.\n",
    "\n",
    "Example:\n",
    "- Suppose we have 3 users (`[0, 1, 2]`) and 3 items (`[0, 1, 2]`).\n",
    "- If the embedding dimension is 2:\n",
    "  - Users are represented as:  \n",
    "    `User 0 → [0.1, 0.3]`, `User 1 → [0.5, 0.7]`, `User 2 → [0.9, 0.2]`\n",
    "  - Items are represented as:  \n",
    "    `Item 0 → [0.4, 0.6]`, `Item 1 → [0.8, 0.5]`, `Item 2 → [0.2, 0.9]`\n",
    "\n",
    "These embeddings are **learned** during training, meaning their values adjust to minimize the prediction error.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Why Are Embeddings Used?**\n",
    "\n",
    "#### (a) **Representing High-Dimensional Categorical Data**\n",
    "- User and item IDs are **discrete indices** (e.g., `User 0`, `Item 1`), which have no inherent numerical meaning.\n",
    "- Embeddings transform these indices into **dense vectors** in a continuous space, where:\n",
    "  - Users with similar behaviors will have similar embedding vectors.\n",
    "  - Items with similar characteristics will have similar embedding vectors.\n",
    "\n",
    "#### (b) **Capturing Latent Features**\n",
    "- Embeddings act as **feature extractors**:\n",
    "  - For users: Embeddings learn **latent preferences** of the users.\n",
    "  - For items: Embeddings learn **latent characteristics** of the items.\n",
    "- These features are not explicitly provided in the data but are **automatically learned** by the network.\n",
    "\n",
    "Example:\n",
    "- A user's embedding might represent that they prefer \"action\" and \"sci-fi\" movies.\n",
    "- An item's embedding might represent that it is categorized as \"action\" and has \"high ratings.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **What Do Embeddings Add to the Network?**\n",
    "\n",
    "#### (a) **Enable the Network to Generalize**\n",
    "- By learning embeddings, the NCF network generalizes user-item interactions beyond the raw indices.\n",
    "- Users and items with **similar embeddings** will behave similarly in the network's predictions.\n",
    "\n",
    "For example:\n",
    "- If `User A` has preferences similar to `User B`, their embeddings will be close in the latent space.\n",
    "- The model will predict similar ratings for items rated by `User B` when recommending to `User A`.\n",
    "\n",
    "#### (b) **Reduce Input Dimensionality**\n",
    "- Instead of using one-hot encoding (which would create huge sparse vectors for users and items), embeddings reduce the input to **low-dimensional dense vectors**.\n",
    "- This makes training more efficient and computationally feasible.\n",
    "\n",
    "For example:\n",
    "- One-hot encoding for 1 million users would create a vector of size 1,000,000.\n",
    "- Using embeddings of dimension 32 reduces it to a **32-dimensional dense vector**.\n",
    "\n",
    "#### (c) **Capture Complex User-Item Interactions**\n",
    "- Embeddings provide a numerical basis to model **relationships** between users and items.\n",
    "- By concatenating the user and item embeddings, the network can learn non-linear interactions through its fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **How Are Embeddings Learned?**\n",
    "- Embedding values are initialized randomly at the start of training.\n",
    "- During training, embeddings are updated using **backpropagation** to minimize the prediction loss.\n",
    "- Over time, embeddings adjust to capture user preferences and item characteristics that lead to accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Summary: What Do Embeddings Add?**\n",
    "\n",
    "| **Purpose**                    | **Explanation**                                                   |\n",
    "|--------------------------------|-------------------------------------------------------------------|\n",
    "| **Feature Representation**     | Converts indices to dense vectors that represent latent features. |\n",
    "| **Dimensionality Reduction**   | Replaces large sparse one-hot vectors with small dense embeddings.|\n",
    "| **Generalization**             | Allows the model to identify similarities between users and items.|\n",
    "| **Efficient Training**         | Enables the model to learn patterns in user-item interactions.   |\n",
    "| **Capture Relationships**      | Encodes complex relationships between users and items.           |\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Imagine predicting movie ratings for `User 0` and `Item 0`:\n",
    "1. User 0 → Embedding `[0.1, 0.3]`\n",
    "2. Item 0 → Embedding `[0.4, 0.6]`\n",
    "3. Combine these embeddings and pass them through the network:\n",
    "   - Output: Predicted Rating = 4.2\n",
    "\n",
    "The embeddings allow the model to **learn patterns** that map user-item pairs to predicted ratings, even if the exact pair has not been seen before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Encoded in User-Item Embeddings\n",
    "\n",
    "When the only available data consists of **user IDs**, **item IDs**, and their **ratings**, the embeddings in a Neural Collaborative Filtering (NCF) model capture specific patterns that reflect the **relationship between users and items**. Below, we explore the types of information that can be encoded with this minimal setup.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Latent Features of Users and Items**\n",
    "\n",
    "#### What Are Latent Features?\n",
    "Latent features are **hidden characteristics** of users and items that are **inferred indirectly** from the data. These features are not explicitly provided but are learned based on user-item interactions (ratings).\n",
    "\n",
    "#### Examples of Latent Features:\n",
    "- For Users:\n",
    "  - Preferences for specific genres or categories (e.g., \"likes action movies\").\n",
    "  - General tendencies, such as rating items highly or conservatively.\n",
    "  - Hidden group affiliations (e.g., \"Sci-fi enthusiasts\").\n",
    "\n",
    "- For Items:\n",
    "  - Belonging to certain categories (e.g., \"Action movies\").\n",
    "  - Popularity among specific user groups.\n",
    "  - Qualities that appeal to particular types of users.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **User-Item Interaction Patterns**\n",
    "\n",
    "The embeddings also capture **interaction patterns** between users and items, such as:\n",
    "- **Affinity**: Users who rate certain items highly will have embeddings that align with those items' embeddings.\n",
    "- **Diversity**: A user's embedding can encode preferences for a variety of items (e.g., action and comedy movies).\n",
    "\n",
    "#### Example:\n",
    "- If two users rate similar items highly, their embeddings will be closer in the latent space.\n",
    "- Items rated highly by the same users will also have embeddings that are close to each other.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Implicit Relationships**\n",
    "\n",
    "Even if the user-item matrix is sparse (many missing ratings), embeddings can still encode **implicit relationships**:\n",
    "- **Similarity Between Users**: Users with similar preferences will have similar embeddings, even if they haven't rated exactly the same items.\n",
    "- **Similarity Between Items**: Items frequently rated by the same users will have similar embeddings, even if their explicit attributes (e.g., genres) are unknown.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Behavioral Patterns**\n",
    "\n",
    "The model can learn and encode specific **behavioral patterns**, such as:\n",
    "- **Rating Tendencies**:\n",
    "  - Some users might consistently rate items highly (lenient raters), while others are more conservative.\n",
    "  - Embeddings can reflect this tendency, influencing predictions.\n",
    "- **Preference Strength**:\n",
    "  - Users who rate specific types of items frequently or highly will have embeddings that emphasize those preferences.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Popularity Patterns**\n",
    "\n",
    "- **Popular Items**: Items that are rated highly by many users will have embeddings closer to a \"center of gravity\" for the user embeddings.\n",
    "- **Influential Users**: Users who rate a wide variety of items might influence multiple item embeddings, leading to generalization across item features.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations of User-Item Embeddings Without Additional Information\n",
    "\n",
    "1. **Cold-Start Problem**:\n",
    "   - New users and items have no historical interactions, so embeddings cannot encode meaningful patterns for them.\n",
    "2. **Lack of Context**:\n",
    "   - Without contextual information (e.g., time, location), embeddings cannot capture temporal or situational preferences.\n",
    "3. **Missing Explicit Attributes**:\n",
    "   - Embeddings may group items together due to shared user preferences but cannot explain **why** (e.g., genres or tags are unknown).\n",
    "\n",
    "---\n",
    "\n",
    "### Information Patterns Encoded in Practice\n",
    "\n",
    "| **Type of Pattern**             | **Description**                                                                 |\n",
    "|----------------------------------|---------------------------------------------------------------------------------|\n",
    "| **Latent Features**              | Hidden characteristics of users (preferences) and items (categories).           |\n",
    "| **Similarity**                   | Users/items with similar interactions are closer in the latent space.           |\n",
    "| **Affinity**                     | Captures which items a user is likely to rate highly.                           |\n",
    "| **Popularity**                   | Items rated by many users are closer to a general \"center\" in the embedding.    |\n",
    "| **Behavioral Tendencies**        | Encodes whether users are lenient/conservative raters or exhibit strong biases. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Embeddings generalize patterns** from user-item interactions and ratings.\n",
    "- They are **powerful representations** that allow the model to predict new user-item interactions.\n",
    "- However, without additional information (e.g., item metadata, user demographics), embeddings are limited to encoding patterns directly inferred from ratings.\n",
    "\n",
    "In more advanced setups, integrating extra information (e.g., genres, tags, timestamps) enriches embeddings and allows the model to capture **contextual and explicit relationships**, making it more robust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Generalization in NCF: User Likes Action Movies (Implicit Patterns)\n",
    "\n",
    "When the system has no explicit knowledge of genres or tags (e.g., \"action movies\"), it infers patterns from **user-item interactions** (ratings) alone. Here’s how this works:\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Process\n",
    "\n",
    "#### 1. **Latent Patterns in the Embedding Space**\n",
    "- **Users who like action movies** will have embeddings that cluster together, even though the system doesn't explicitly know what \"action movies\" are.\n",
    "- **Action movies themselves** (items) will have embeddings that cluster together because they are rated highly by the same group of users.\n",
    "\n",
    "This clustering happens because the embeddings encode **implicit relationships** based on the patterns in the ratings.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Intersecting User-Item Sets**\n",
    "- If multiple users like the same set of movies (e.g., action movies), their embeddings will become similar during training.\n",
    "- Similarly, the embeddings of items rated highly by these users will also cluster together.\n",
    "\n",
    "For example:\n",
    "- User A and User B both rate `Item X` and `Item Y` highly.\n",
    "- The embeddings for User A and User B will become similar because they share common preferences.\n",
    "- The embeddings for `Item X` and `Item Y` will also become similar because they are frequently rated highly by the same users.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Generalization to New Users**\n",
    "Now consider a **new user** who hasn’t rated any of the action movies in the \"pool\":\n",
    "\n",
    "##### Case 1: The New User Rates Other Items (Action Movies or Not)\n",
    "- Suppose this new user rates some items (e.g., `Item Z`) that are also liked by action movie enthusiasts.\n",
    "- The system will:\n",
    "  - Learn that the user’s preferences overlap with the embeddings of action movie enthusiasts.\n",
    "  - Place this user closer to the cluster of action movie enthusiasts in the embedding space.\n",
    "\n",
    "- As a result, the system can recommend other items in the action movie cluster, even if the user hasn’t rated those specific items.\n",
    "\n",
    "##### Case 2: The New User Rates Non-Action Items\n",
    "- Even if the new user hasn’t rated action movies specifically, they might rate items with embeddings close to the action movie cluster.\n",
    "- For example, items in adjacent clusters (e.g., \"adventure movies\") might have overlapping embeddings.\n",
    "- The system can generalize preferences and recommend action movies based on nearby embeddings in the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Recommendation Without Direct Ratings**\n",
    "When a user hasn’t rated any specific action movies but likes items that are **closely related** (based on the embedding space), the system can:\n",
    "- Use the user’s embedding to compute similarity to other item embeddings.\n",
    "- Recommend items (e.g., action movies) that are most similar to the user’s preferences.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: How the System Generalizes\n",
    "\n",
    "| **Scenario**                         | **What Happens**                                                                                      |\n",
    "|---------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| **User rates items liked by others**  | Their embedding moves closer to the cluster of similar users (e.g., action movie enthusiasts).       |\n",
    "| **Item is rated by many users**       | Its embedding becomes representative of the preferences of users who rated it highly.                |\n",
    "| **New user hasn’t rated specific items** | Recommendations are based on similarity between their embedding and the embeddings of other items. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway:\n",
    "Even without explicit knowledge of genres or tags:\n",
    "1. The system identifies **clusters of users** and **clusters of items** based on shared rating patterns.\n",
    "2. For new users, the system generalizes by:\n",
    "   - Placing the user in a cluster based on their ratings.\n",
    "   - Recommending items from the closest clusters in the embedding space.\n",
    "\n",
    "This process is why embeddings are so powerful: they encode implicit relationships that enable the system to generalize to new and unseen user-item interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Collaborative Filtering (NCF): A Simple Example\n",
    "\n",
    "To understand what the NCF network is doing, let’s work through a simple example that demonstrates the **input**, **output**, and **loss calculation**. For simplicity, we assume small values for users, items, and ratings.\n",
    "\n",
    "---\n",
    "\n",
    "### Problem Setup\n",
    "\n",
    "We have:\n",
    "- **2 users**: `User A` (index 0), `User B` (index 1)\n",
    "- **2 items**: `Item X` (index 0), `Item Y` (index 1)\n",
    "- **Interaction data (ratings)**:\n",
    "  - `User A` rated `Item X` with a score of 4.\n",
    "  - `User B` rated `Item Y` with a score of 3.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Input to the Model\n",
    "\n",
    "The input to the NCF model consists of:\n",
    "- **User Indices**: `[0, 1]` → These correspond to `User A` and `User B`.\n",
    "- **Item Indices**: `[0, 1]` → These correspond to `Item X` and `Item Y`.\n",
    "- **Ratings (Target Values)**: `[4, 3]` → Actual ratings provided by the users.\n",
    "\n",
    "| User Index | Item Index | Rating |\n",
    "|------------|------------|--------|\n",
    "| 0          | 0          | 4      |\n",
    "| 1          | 1          | 3      |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Model Output\n",
    "\n",
    "The NCF model predicts a **single score** for each user-item pair:\n",
    "- **User Embedding** and **Item Embedding** are looked up for the given indices.\n",
    "- These embeddings are passed through the network, and the output layer generates the predicted score.\n",
    "\n",
    "Suppose the model predicts the following scores:\n",
    "- For `User A` and `Item X`: Predicted Score = `3.8`\n",
    "- For `User B` and `Item Y`: Predicted Score = `2.9`\n",
    "\n",
    "| User Index | Item Index | True Rating | Predicted Score |\n",
    "|------------|------------|-------------|-----------------|\n",
    "| 0          | 0          | 4           | 3.8             |\n",
    "| 1          | 1          | 3           | 2.9             |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Loss Calculation\n",
    "\n",
    "The loss measures the difference between the **true ratings** and the **predicted scores**. In this case, we use **Mean Squared Error (MSE)** as the loss function:\n",
    "\n",
    "$$\n",
    "\\text{MSE Loss} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the true rating.\n",
    "- $ \\hat{y}_i $ is the predicted score.\n",
    "- $ N $ is the number of user-item pairs.\n",
    "\n",
    "#### Loss Calculation for Our Example:\n",
    "\n",
    "1. For `User A` and `Item X`:\n",
    "   $$\n",
    "   (4 - 3.8)^2 = 0.04\n",
    "   $$\n",
    "\n",
    "2. For `User B` and `Item Y`:\n",
    "   $$\n",
    "   (3 - 2.9)^2 = 0.01\n",
    "   $$\n",
    "\n",
    "3. Combine the losses and calculate the average (MSE):\n",
    "   $$\n",
    "   \\text{MSE Loss} = \\frac{1}{2} (0.04 + 0.01) = 0.025\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Summary\n",
    "\n",
    "#### **Input**:\n",
    "- User Indices: `[0, 1]`\n",
    "- Item Indices: `[0, 1]`\n",
    "- True Ratings: `[4, 3]`\n",
    "\n",
    "#### **Output**:\n",
    "- Predicted Scores: `[3.8, 2.9]`\n",
    "\n",
    "#### **Loss**:\n",
    "- MSE Loss = `0.025`\n",
    "\n",
    "---\n",
    "\n",
    "### Key Idea:\n",
    "The NCF model tries to minimize the difference between the **true ratings** and the **predicted scores**. It learns the embeddings and neural network parameters during training to improve its predictions.\n",
    "\n",
    "This example simplifies the process, but the same principle applies to larger datasets where the NCF model processes many user-item pairs to learn patterns in user preferences and item characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use Linear Layers?\n",
    "While embeddings alone encode important features, the linear layers provide:\n",
    "\n",
    "- **Non-linear Transformations:** Capture complex relationships between embeddings.\n",
    "- **Fexibility:** Allow the network to model interactions in ways that simple dot products (traditional matrix factorization) cannot.\n",
    "\n",
    "In short, embeddings capture **what** the user and item represent, and the linear layers determine **how** much the user is likely to interact with or rate the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a NFC with **content blending** and use the enriched data (including genres, tags, plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class StaticEmbeddingNCF(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_users, \n",
    "        n_items, \n",
    "        embedding_dim=32, \n",
    "        hidden_layers=[64, 32, 16], \n",
    "        content_dim=768, \n",
    "        content_tower_hidden_dim=128,\n",
    "        use_scaling=False, \n",
    "        use_multihead_attention=False, \n",
    "        num_heads=4,\n",
    "        alpha_reg_strength=0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Neural Collaborative Filtering Model with a Hybrid Content Tower + Gating Mechanism\n",
    "        for both users and items.\n",
    "\n",
    "        Args:\n",
    "            n_users (int): Number of unique users.\n",
    "            n_items (int): Number of unique items.\n",
    "            embedding_dim (int): Dimension of the user/item embeddings.\n",
    "            hidden_layers (list): List defining the size of hidden layers.\n",
    "            content_dim (int): Dimension of the content embeddings (e.g., from a pretrained LM).\n",
    "            content_tower_hidden_dim (int): Hidden dimension of the content tower.\n",
    "            use_scaling (bool): If True, introduce learnable scaling parameters for user/item content embeddings.\n",
    "            use_multihead_attention (bool): If True, use multiple attention heads for gating.\n",
    "            num_heads (int): Number of attention heads if multi-head attention is enabled.\n",
    "            alpha_reg_strength (float): Strength of alpha regularization (0.0 to disable).\n",
    "        \"\"\"\n",
    "        super(StaticEmbeddingNCF, self).__init__()\n",
    "\n",
    "        self.use_scaling = use_scaling\n",
    "        self.use_multihead_attention = use_multihead_attention\n",
    "        self.num_heads = num_heads\n",
    "        self.alpha_reg_strength = alpha_reg_strength\n",
    "\n",
    "        # Embedding layers for users and items\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "\n",
    "        # Content-only tower for user features\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(content_dim, content_tower_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(content_tower_hidden_dim, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Content-only tower for item features\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(content_dim, content_tower_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(content_tower_hidden_dim, embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Scaling parameters (if enabled)\n",
    "        if self.use_scaling:\n",
    "            self.user_scale = nn.Parameter(torch.tensor(1.0))\n",
    "            self.user_content_scale = nn.Parameter(torch.tensor(1.0))\n",
    "            self.item_scale = nn.Parameter(torch.tensor(1.0))\n",
    "            self.content_scale = nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "        # Attention layers for users\n",
    "        if self.use_multihead_attention:\n",
    "            self.user_attention_layers = nn.ModuleList([\n",
    "                nn.Linear(embedding_dim * 2, 1) for _ in range(num_heads)\n",
    "            ])\n",
    "        else:\n",
    "            self.user_attention_layer = nn.Linear(embedding_dim * 2, 1)\n",
    "        \n",
    "        # Attention bias for users\n",
    "        self.user_attention_bias = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        # Attention layers for items\n",
    "        if self.use_multihead_attention:\n",
    "            self.item_attention_layers = nn.ModuleList([\n",
    "                nn.Linear(embedding_dim * 2, 1) for _ in range(num_heads)\n",
    "            ])\n",
    "        else:\n",
    "            self.item_attention_layer = nn.Linear(embedding_dim * 2, 1)\n",
    "        \n",
    "        # Attention bias for items\n",
    "        self.item_attention_bias = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "        print(f\"User/Item Embedding dimension: {embedding_dim}, Original Content dimension: {content_dim}, Content tower hidden dimension: {content_tower_hidden_dim}\")\n",
    "        print(f\"Use Scaling: {self.use_scaling}, Use Multi-Head Attention: {self.use_multihead_attention}, Num Heads: {self.num_heads}, Alpha Reg: {self.alpha_reg_strength}\")\n",
    "\n",
    "        # Fully connected layers receive user_final_embed + item_final_embed = 2 * embedding_dim\n",
    "        input_dim = embedding_dim * 2\n",
    "        layers = []\n",
    "        for hd in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hd))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hd\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_layers[-1], 1)  # Predict a single rating score\n",
    "\n",
    "    def _gating_mechanism(self, base_embed, content_embed, attention_layers_or_layer, attention_bias):\n",
    "        \"\"\"\n",
    "        A helper function to handle either multi-head or single-head gating.\n",
    "        base_embed and content_embed should be [batch_size, embedding_dim].\n",
    "        \"\"\"\n",
    "        if self.use_multihead_attention:\n",
    "            attn_outputs = []\n",
    "            alpha_values = []\n",
    "            for attention_layer in attention_layers_or_layer:\n",
    "                attn_input = torch.cat([base_embed, content_embed], dim=1)  # [batch_size, 2 * embedding_dim]\n",
    "                head_alpha = torch.sigmoid(attention_layer(attn_input) + attention_bias)  # [batch_size, 1]\n",
    "                alpha_values.append(head_alpha)\n",
    "                head_output = head_alpha * base_embed + (1 - head_alpha) * content_embed\n",
    "                attn_outputs.append(head_output)\n",
    "            final_embed = torch.mean(torch.stack(attn_outputs, dim=0), dim=0)\n",
    "            alpha = torch.mean(torch.stack(alpha_values, dim=0), dim=0)\n",
    "        else:\n",
    "            attn_input = torch.cat([base_embed, content_embed], dim=1)  # [batch_size, 2 * embedding_dim]\n",
    "            alpha = torch.sigmoid(attention_layers_or_layer(attn_input) + attention_bias)  # [batch_size, 1]\n",
    "            final_embed = alpha * base_embed + (1 - alpha) * content_embed\n",
    "        return final_embed, alpha\n",
    "\n",
    "    def forward(self, user_indices, item_indices, user_content_features, item_content_features, return_alpha=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            user_indices (Tensor): Batch of user indices.\n",
    "            item_indices (Tensor): Batch of item indices.\n",
    "            user_content_features (Tensor): User content features embeddings.\n",
    "            item_content_features (Tensor): Item content features embeddings.\n",
    "            return_alpha (bool): If True, also return the alpha (attention) values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted scores.\n",
    "            Tensor (optional): The alpha values for user and item gating (alpha_user, alpha_item) if return_alpha is True.\n",
    "        \"\"\"\n",
    "        # Embedding lookup\n",
    "        user_embed = self.user_embedding(user_indices)  # [batch_size, embedding_dim]\n",
    "        item_embed = self.item_embedding(item_indices)  # [batch_size, embedding_dim]\n",
    "\n",
    "        # Process user and item content features through their towers\n",
    "        user_content_embed = self.user_tower(user_content_features)    # [batch_size, embedding_dim]\n",
    "        item_content_embed = self.item_tower(item_content_features)    # [batch_size, embedding_dim]\n",
    "\n",
    "        # Apply scaling if enabled\n",
    "        if self.use_scaling:\n",
    "            user_embed = self.user_scale * user_embed\n",
    "            user_content_embed = self.user_content_scale * user_content_embed\n",
    "            item_embed = self.item_scale * item_embed\n",
    "            item_content_embed = self.content_scale * item_content_embed\n",
    "\n",
    "        # Gating for user embeddings\n",
    "        if self.use_multihead_attention:\n",
    "            user_final, alpha_user = self._gating_mechanism(\n",
    "                user_embed, user_content_embed, self.user_attention_layers, self.user_attention_bias\n",
    "            )\n",
    "        else:\n",
    "            user_final, alpha_user = self._gating_mechanism(\n",
    "                user_embed, user_content_embed, self.user_attention_layer, self.user_attention_bias\n",
    "            )\n",
    "\n",
    "        # Gating for item embeddings\n",
    "        if self.use_multihead_attention:\n",
    "            item_final, alpha_item = self._gating_mechanism(\n",
    "                item_embed, item_content_embed, self.item_attention_layers, self.item_attention_bias\n",
    "            )\n",
    "        else:\n",
    "            item_final, alpha_item = self._gating_mechanism(\n",
    "                item_embed, item_content_embed, self.item_attention_layer, self.item_attention_bias\n",
    "            )\n",
    "\n",
    "        # Combine user_final and item_final embeddings\n",
    "        combined = torch.cat([user_final, item_final], dim=1)  # [batch_size, 2 * embedding_dim]\n",
    "        \n",
    "        # Pass through fully connected layers\n",
    "        x = self.fc_layers(combined)\n",
    "        \n",
    "        # Predict the output score\n",
    "        output = self.output_layer(x).squeeze()  # [batch_size]\n",
    "\n",
    "        if return_alpha:\n",
    "            # Return both user and item alpha values for diagnostics\n",
    "            return output, (alpha_user.squeeze(), alpha_item.squeeze())\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "\n",
    "def ridge_loss(predictions, targets, model, lambda_reg=1e-4):\n",
    "    mse_loss = F.mse_loss(predictions, targets)\n",
    "    l2_reg = sum(param.norm(2) for param in model.parameters())\n",
    "    return mse_loss + lambda_reg * l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_mse_with_content(model, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance by computing the true sample-wise MSE.\n",
    "\n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        data_loader: DataLoader for the dataset to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: Sample-wise MSE over the entire dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_squared_error = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for user_indices, item_indices, user_content_features, item_content_features, ratings in data_loader:\n",
    "        user_indices = user_indices.to(device)\n",
    "        item_indices = item_indices.to(device)\n",
    "        user_content_features = user_content_features.to(device)\n",
    "        item_content_features = item_content_features.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        predictions = model(user_indices, item_indices, user_content_features, item_content_features)\n",
    "        \n",
    "        # Compute the squared errors for this batch without averaging\n",
    "        squared_errors = (predictions - ratings) ** 2\n",
    "        batch_size = len(ratings)\n",
    "\n",
    "        # Accumulate total squared error and count of samples\n",
    "        total_squared_error += squared_errors.sum().item()\n",
    "        total_samples += batch_size\n",
    "\n",
    "    # Compute true sample-wise MSE across all batches\n",
    "    mse = total_squared_error / total_samples\n",
    "    return mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs=50, patience=5):\n",
    "    writer = SummaryWriter(log_dir=\"runs/my_experiment\")  # TensorBoard log directory\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for user_indices, item_indices, user_content_features, item_content_features, ratings in train_loader:\n",
    "            user_indices = user_indices.to(device)\n",
    "            item_indices = item_indices.to(device)\n",
    "            user_content_features = user_content_features.to(device)\n",
    "            item_content_features = item_content_features.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions, (alpha_items, alpha_users) = model(\n",
    "                user_indices, item_indices, user_content_features, item_content_features, return_alpha=True\n",
    "            )\n",
    "            base_loss = criterion(predictions, ratings)\n",
    "            \n",
    "            # Alpha Regularization (encouraging alpha near 0.5 for both items and users)\n",
    "            if model.alpha_reg_strength > 0:\n",
    "                alpha_reg_items = model.alpha_reg_strength * ((alpha_items - 0.5)**2).mean()\n",
    "                alpha_reg_users = model.alpha_reg_strength * ((alpha_users - 0.5)**2).mean()\n",
    "                alpha_reg = alpha_reg_items + alpha_reg_users\n",
    "                loss = base_loss + alpha_reg\n",
    "            else:\n",
    "                loss = base_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log alpha statistics\n",
    "            writer.add_scalar(\"Alpha_Items/Mean\", alpha_items.mean().item(), global_step)\n",
    "            writer.add_scalar(\"Alpha_Items/Std\", alpha_items.std().item(), global_step)\n",
    "            writer.add_scalar(\"Alpha_Users/Mean\", alpha_users.mean().item(), global_step)\n",
    "            writer.add_scalar(\"Alpha_Users/Std\", alpha_users.std().item(), global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loss = evaluate_mse_with_content(model, val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {total_loss/len(train_loader):.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Log validation loss to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch + 1)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs.\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User/Item Embedding dimension: 64, Original Content dimension: 768, Content tower hidden dimension: 384\n",
      "Use Scaling: False, Use Multi-Head Attention: False, Num Heads: 4, Alpha Reg: 0.0\n",
      "Epoch [1/100], Train Loss: 2.6278, Validation Loss: 1.0010\n",
      "Epoch [2/100], Train Loss: 0.9624, Validation Loss: 0.9255\n",
      "Epoch [3/100], Train Loss: 0.9106, Validation Loss: 0.8866\n",
      "Epoch [4/100], Train Loss: 0.8719, Validation Loss: 0.8560\n",
      "Epoch [5/100], Train Loss: 0.8403, Validation Loss: 0.8514\n",
      "Epoch [6/100], Train Loss: 0.8170, Validation Loss: 0.8416\n",
      "Epoch [7/100], Train Loss: 0.7967, Validation Loss: 0.8177\n",
      "Epoch [8/100], Train Loss: 0.7802, Validation Loss: 0.8171\n",
      "Epoch [9/100], Train Loss: 0.7671, Validation Loss: 0.8020\n",
      "Epoch [10/100], Train Loss: 0.7543, Validation Loss: 0.7974\n",
      "Epoch [11/100], Train Loss: 0.7426, Validation Loss: 0.8005\n",
      "No improvement for 1 epochs.\n",
      "Epoch [12/100], Train Loss: 0.7339, Validation Loss: 0.8035\n",
      "No improvement for 2 epochs.\n",
      "Epoch [13/100], Train Loss: 0.7237, Validation Loss: 0.8048\n",
      "No improvement for 3 epochs.\n",
      "Epoch [14/100], Train Loss: 0.7131, Validation Loss: 0.7942\n",
      "Epoch [15/100], Train Loss: 0.7063, Validation Loss: 0.7926\n",
      "Epoch [16/100], Train Loss: 0.6974, Validation Loss: 0.8039\n",
      "No improvement for 1 epochs.\n",
      "Epoch [17/100], Train Loss: 0.6881, Validation Loss: 0.7929\n",
      "No improvement for 2 epochs.\n",
      "Epoch [18/100], Train Loss: 0.6805, Validation Loss: 0.7969\n",
      "No improvement for 3 epochs.\n",
      "Epoch [19/100], Train Loss: 0.6711, Validation Loss: 0.7920\n",
      "Epoch [20/100], Train Loss: 0.6637, Validation Loss: 0.7993\n",
      "No improvement for 1 epochs.\n",
      "Epoch [21/100], Train Loss: 0.6543, Validation Loss: 0.8009\n",
      "No improvement for 2 epochs.\n",
      "Epoch [22/100], Train Loss: 0.6464, Validation Loss: 0.7934\n",
      "No improvement for 3 epochs.\n",
      "Epoch [23/100], Train Loss: 0.6370, Validation Loss: 0.8012\n",
      "No improvement for 4 epochs.\n",
      "Epoch [24/100], Train Loss: 0.6297, Validation Loss: 0.8100\n",
      "No improvement for 5 epochs.\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Best Hyperparameters (Optuna):\n",
    "# finished with value: 0.7780352973761171 and parameters: {'embedding_dim': 64, 'hidden_layers': [128, 64, 32], 'lr': 0.0001920012036309971, 'lambda_reg': 4.162785620739764e-05, 'content_tower_hidden_dim': 384, 'use_scaling': False, 'use_multihead_attention': False}. Best is trial 36 with value: 0.7780352973761171.\n",
    "embedding_dim = 64\n",
    "hidden_layers = [128, 64, 32]\n",
    "learning_rate = 0.0001920012036309971\n",
    "lambda_reg = 4.162785620739764e-05\n",
    "content_tower_hidden_dim = 384\n",
    "use_scaling = False\n",
    "use_multihead_attention = False\n",
    "alpha_reg_strength = 0.00\n",
    "num_heads = 4\n",
    "\n",
    "model = StaticEmbeddingNCF(\n",
    "    n_users=n_users, \n",
    "    n_items=n_items, \n",
    "    embedding_dim=embedding_dim, \n",
    "    hidden_layers=hidden_layers, \n",
    "    content_dim=CONTENT_DIM, \n",
    "    content_tower_hidden_dim=content_tower_hidden_dim,\n",
    "    alpha_reg_strength=alpha_reg_strength,\n",
    "    use_scaling=use_scaling,\n",
    "    use_multihead_attention=use_multihead_attention,\n",
    "    num_heads=num_heads\n",
    "    ).to(device)\n",
    "\n",
    "criterion = lambda predictions, targets: ridge_loss(predictions, targets, model, lambda_reg=lambda_reg)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, train_loader, val_loader, criterion, optimizer, epochs=100, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to saved_weights/ncf/best_model_with_content.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model state dictionary\n",
    "model_path = \"saved_weights/ncf/best_model_with_content.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'embedding_dim': embedding_dim,\n",
    "    'hidden_layers': hidden_layers,\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'content_dim': CONTENT_DIM,\n",
    "    'content_tower_hidden_dim': content_tower_hidden_dim,\n",
    "    'use_scaling': use_scaling,\n",
    "    'use_multihead_attention': use_multihead_attention,\n",
    "    'num_heads': num_heads,\n",
    "    'alpha_reg_strength': alpha_reg_strength\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User/Item Embedding dimension: 64, Original Content dimension: 768, Content tower hidden dimension: 384\n",
      "Use Scaling: False, Use Multi-Head Attention: False, Num Heads: 4, Alpha Reg: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290320/2542680590.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EnhancedNCF(\n",
       "  (user_embedding): Embedding(610, 64)\n",
       "  (item_embedding): Embedding(9724, 64)\n",
       "  (user_tower): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=384, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (item_tower): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=384, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (user_attention_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (item_attention_layer): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reinitialize the model architecture\n",
    "model_path = \"saved_weights/ncf/best_model_with_content.pth\"\n",
    "checkpoint = torch.load(model_path)\n",
    "\n",
    "loaded_model = StaticEmbeddingNCF(\n",
    "    checkpoint['n_users'], \n",
    "    checkpoint['n_items'], \n",
    "    checkpoint['embedding_dim'], \n",
    "    checkpoint['hidden_layers'],\n",
    "    checkpoint['content_dim'],\n",
    "    checkpoint['content_tower_hidden_dim'],\n",
    "    alpha_reg_strength=checkpoint['alpha_reg_strength'],\n",
    "    use_scaling=checkpoint['use_scaling'],\n",
    "    use_multihead_attention=checkpoint['use_multihead_attention'],\n",
    "    num_heads=checkpoint['num_heads']\n",
    "    ).to(device)\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter search with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-20 15:48:28,863] A new study created in memory with name: no-name-cc8bba79-f197-4075-83e7-d8bc430fa076\n",
      "/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [128, 64, 32] which is of type list.\n",
      "  warnings.warn(message)\n",
      "/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [256, 128, 64] which is of type list.\n",
      "  warnings.warn(message)\n",
      "/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [512, 256, 128] which is of type list.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_250109/1516410752.py:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)  # Learning rate\n",
      "/tmp/ipykernel_250109/1516410752.py:22: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_reg = trial.suggest_loguniform(\"lambda_reg\", 1e-6, 1e-4)  # L2 regularization\n",
      "/tmp/ipykernel_250109/1516410752.py:29: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha_reg_strength = trial.suggest_loguniform(\"alpha_reg_strength\", 1e-4, 1e-2) if use_scaling or use_multihead_attention else 0.0  # Alpha regularization strength\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User/Item Embedding dimension: 448, Original Content dimension: 768, Content tower hidden dimension: 256\n",
      "Use Scaling: False, Use Multi-Head Attention: True, Num Heads: 4, Alpha Reg: 0.004067850372561103\n",
      "Epoch [1/25], Train Loss: 2.7990, Validation Loss: 0.9115\n",
      "Epoch [2/25], Train Loss: 1.0731, Validation Loss: 0.8646\n",
      "Epoch [3/25], Train Loss: 1.0335, Validation Loss: 0.8431\n",
      "Epoch [4/25], Train Loss: 1.0047, Validation Loss: 0.8349\n",
      "Epoch [5/25], Train Loss: 0.9778, Validation Loss: 0.8295\n",
      "Epoch [6/25], Train Loss: 0.9502, Validation Loss: 0.8201\n",
      "Epoch [7/25], Train Loss: 0.9194, Validation Loss: 0.8205\n",
      "No improvement for 1 epochs.\n",
      "Epoch [8/25], Train Loss: 0.8848, Validation Loss: 0.8189\n",
      "Epoch [9/25], Train Loss: 0.8470, Validation Loss: 0.8325\n",
      "No improvement for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-12-20 15:50:02,733] Trial 0 failed with parameters: {'embedding_dim': 448, 'hidden_layers': [256, 128, 64], 'lr': 9.268181387955211e-05, 'lambda_reg': 8.94977830722816e-05, 'content_tower_hidden_dim': 256, 'use_scaling': False, 'use_multihead_attention': True, 'num_heads': 4, 'alpha_reg_strength': 0.004067850372561103} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_250109/1516410752.py\", line 52, in objective\n",
      "    train(model, train_loader, val_loader, criterion, optimizer, epochs=25, patience=5)\n",
      "  File \"/tmp/ipykernel_250109/2136064172.py\", line 36, in train\n",
      "    optimizer.step()\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/adam.py\", line 524, in _multi_tensor_adam\n",
      "    torch._foreach_add_(\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-20 15:50:02,737] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Set up Optuna study\u001b[39;00m\n\u001b[1;32m     66\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Minimize validation loss\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Output the best hyperparameters\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     49\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Evaluate the best model on the validation set\u001b[39;00m\n\u001b[1;32m     55\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate_mse_with_content(model, val_loader)\n",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience)\u001b[0m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Log alpha statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/torch/optim/adam.py:524\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mis_cpu:\n\u001b[0;32m--> 524\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    528\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_foreach_add_(device_state_steps, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Initialize variable to track the best model\n",
    "best_val_loss = float('inf')  # Best validation loss initialized to infinity\n",
    "best_model_path = \"saved_weights/ncf/best_model_with_content_optuna.pth\"  # Path to save the best model\n",
    "\n",
    "# Define the Optuna objective function\n",
    "def objective(trial):\n",
    "    global best_val_loss  # Use the global variable to track the best validation loss\n",
    "\n",
    "    # Define hyperparameter search space\n",
    "    # Define hyperparameter search space\n",
    "    embedding_dim = trial.suggest_int(\"embedding_dim\", 64, 512, step=64)  # Embedding dimension\n",
    "    hidden_layers = trial.suggest_categorical(\"hidden_layers\", [\n",
    "        [128, 64, 32],\n",
    "        [256, 128, 64],\n",
    "        [512, 256, 128]\n",
    "    ])  # Hidden layer configurations\n",
    "\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-3)  # Learning rate\n",
    "    lambda_reg = trial.suggest_loguniform(\"lambda_reg\", 1e-6, 1e-4)  # L2 regularization\n",
    "    content_tower_hidden_dim = trial.suggest_int(\"content_tower_hidden_dim\", 256, 512, step=64)  # Content tower hidden dim\n",
    "\n",
    "    # Additional parameters\n",
    "    use_scaling = trial.suggest_categorical(\"use_scaling\", [True, False])  # Toggle scaling\n",
    "    use_multihead_attention = trial.suggest_categorical(\"use_multihead_attention\", [True, False])  # Toggle multi-head attention\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 2, 8, step=2) if use_multihead_attention else 1  # Number of attention heads\n",
    "    alpha_reg_strength = trial.suggest_loguniform(\"alpha_reg_strength\", 1e-4, 1e-2) if use_scaling or use_multihead_attention else 0.0  # Alpha regularization strength\n",
    "\n",
    "    # Prepare the dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = StaticEmbeddingNCF(\n",
    "        n_users, \n",
    "        n_items, \n",
    "        embedding_dim, \n",
    "        hidden_layers, \n",
    "        content_dim=CONTENT_DIM, \n",
    "        content_tower_hidden_dim=content_tower_hidden_dim,\n",
    "        use_multihead_attention=use_multihead_attention,\n",
    "        use_scaling=use_scaling,\n",
    "        num_heads=num_heads,\n",
    "        alpha_reg_strength=alpha_reg_strength).to(device)\n",
    "    \n",
    "    criterion = lambda predictions, targets: ridge_loss(predictions, targets, model, lambda_reg=lambda_reg)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Train the model\n",
    "    train(model, train_loader, val_loader, criterion, optimizer, epochs=25, patience=5)\n",
    "    \n",
    "    # Evaluate the best model on the validation set\n",
    "    val_loss = evaluate_mse_with_content(model, val_loader)\n",
    "\n",
    "    # Save the model if it has the best validation loss so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Set up Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize validation loss\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n",
    "\n",
    "# Visualize Optuna optimization process (optional)\n",
    "try:\n",
    "    import optuna.visualization as vis\n",
    "    vis.plot_optimization_history(study).show()\n",
    "    vis.plot_param_importances(study).show()\n",
    "except ImportError:\n",
    "    print(\"Install optuna.visualization for plotting features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_precision_at_k(model, test_loader, k=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance using Precision@K.\n",
    "\n",
    "    This function assumes the test_loader yields batches of (user_indices, item_indices, user_features, item_features, ratings).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to eval mode\n",
    "\n",
    "    user_to_items = {}  # user_to_items: { user_id: { 'predictions': [], 'relevances': [] } }\n",
    "\n",
    "    for user_indices, item_indices, user_features, item_features, ratings in test_loader:\n",
    "        user_indices = user_indices.to(device)\n",
    "        item_indices = item_indices.to(device)\n",
    "        user_features = user_features.to(device)\n",
    "        item_features = item_features.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        predictions, _ = model(user_indices, item_indices, user_features, item_features, return_alpha=True)\n",
    "\n",
    "        # Aggregate predictions by user\n",
    "        for u, pred, r in zip(user_indices.cpu().numpy(), predictions.cpu().numpy(), ratings.cpu().numpy()):\n",
    "            if u not in user_to_items:\n",
    "                user_to_items[u] = {'predictions': [], 'relevances': []}\n",
    "            user_to_items[u]['predictions'].append(pred)\n",
    "            user_to_items[u]['relevances'].append(1.0 if r >= threshold else 0.0)\n",
    "\n",
    "    # Compute Precision@K per user\n",
    "    total_precision = 0.0\n",
    "    total_users = 0\n",
    "\n",
    "    for u, data in user_to_items.items():\n",
    "        user_preds = torch.tensor(data['predictions'])\n",
    "        user_relevances = torch.tensor(data['relevances'])\n",
    "\n",
    "        # Get top-k predictions for this user\n",
    "        k_user = min(len(user_preds), k)\n",
    "        _, top_indices = torch.topk(user_preds, k_user)\n",
    "\n",
    "        # Compute how many of the top-k items are relevant\n",
    "        relevant_in_top_k = user_relevances[top_indices].sum().item()\n",
    "        precision = relevant_in_top_k / k_user\n",
    "\n",
    "        total_precision += precision\n",
    "        total_users += 1\n",
    "\n",
    "    # Average precision across all users\n",
    "    return total_precision / total_users if total_users > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_recall_at_k(model, test_loader, k=10, threshold=4.0):\n",
    "    \"\"\"\n",
    "    Evaluate the model's performance using Recall@K.\n",
    "\n",
    "    This function assumes the test_loader yields batches of (user_indices, item_indices, user_features, item_features, ratings).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set to eval mode\n",
    "\n",
    "    user_to_items = {}  # user_to_items: { user_id: { 'predictions': [], 'relevances': [] } }\n",
    "\n",
    "    for user_indices, item_indices, user_features, item_features, ratings in test_loader:\n",
    "        user_indices = user_indices.to(device)\n",
    "        item_indices = item_indices.to(device)\n",
    "        user_features = user_features.to(device)\n",
    "        item_features = item_features.to(device)\n",
    "        ratings = ratings.to(device)\n",
    "\n",
    "        # Forward pass to get predictions\n",
    "        predictions, _ = model(user_indices, item_indices, user_features, item_features, return_alpha=True)\n",
    "\n",
    "        # Aggregate predictions by user\n",
    "        for u, pred, r in zip(user_indices.cpu().numpy(), predictions.cpu().numpy(), ratings.cpu().numpy()):\n",
    "            if u not in user_to_items:\n",
    "                user_to_items[u] = {'predictions': [], 'relevances': []}\n",
    "            user_to_items[u]['predictions'].append(pred)\n",
    "            user_to_items[u]['relevances'].append(1.0 if r >= threshold else 0.0)\n",
    "\n",
    "    total_recall = 0.0\n",
    "    total_users = 0\n",
    "\n",
    "    for u, data in user_to_items.items():\n",
    "        user_preds = torch.tensor(data['predictions'])\n",
    "        user_relevances = torch.tensor(data['relevances'])\n",
    "\n",
    "        # Number of all relevant items for this user\n",
    "        total_relevant = user_relevances.sum().item()\n",
    "\n",
    "        if total_relevant == 0:\n",
    "            continue  # Skip if the user has no relevant items\n",
    "\n",
    "        # Get top-k predictions for this user\n",
    "        k_user = min(len(user_preds), k)\n",
    "        _, top_indices = torch.topk(user_preds, k_user)\n",
    "\n",
    "        # Compute how many of the top-k items are relevant\n",
    "        relevant_in_top_k = user_relevances[top_indices].sum().item()\n",
    "        recall = relevant_in_top_k / total_relevant\n",
    "\n",
    "        total_recall += recall\n",
    "        total_users += 1\n",
    "\n",
    "    # Average recall across all users\n",
    "    return total_recall / total_users if total_users > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.8405\n"
     ]
    }
   ],
   "source": [
    "mse = evaluate_mse_with_content(loaded_model, test_loader)\n",
    "print(f\"Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision@10: 0.6043\n"
     ]
    }
   ],
   "source": [
    "precision_at_10 = evaluate_precision_at_k(loaded_model, test_loader, k=10, threshold=4.0)\n",
    "print(f\"Test Precision@10: {precision_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test recall@10: 0.7272\n"
     ]
    }
   ],
   "source": [
    "recall_at_10 = evaluate_recall_at_k(loaded_model, test_loader, k=10, threshold=4.0)\n",
    "print(f\"Test recall@10: {recall_at_10:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## **Static embedding: When Was It Used in Real Life?**\n",
    "\n",
    "1. **Early Collaborative Filtering Systems**:\n",
    "   - Early systems like **Matrix Factorization (e.g., SVD)** or **NCF** rely on fixed user and item embedding dimensions.\n",
    "   - These systems work well in environments where:\n",
    "     - The user base and catalog are relatively static.\n",
    "     - Retraining can be done frequently enough to include new users/items.\n",
    "\n",
    "2. **Small-Scale Systems**:\n",
    "   - In applications with a stable number of users/items (e.g., internal company systems or niche platforms), fixed embeddings might still be practical.\n",
    "\n",
    "3. **Static Datasets**:\n",
    "   - In benchmarks like **MovieLens**, the user/item base is fixed, so this approach is used for simplicity in academic research.\n",
    "\n",
    "---\n",
    "\n",
    "## **Dependencies and Limitations of Static Embedding Approach**\n",
    "\n",
    "### **1. Dataset Structure**\n",
    "- **All Users Must Be Present in Training, Validation, and Test Sets**:\n",
    "  - Since the embedding table for users is tied to the user IDs present during training, all users in the dataset must be included in the training, validation, and test sets. Otherwise, embeddings for unseen users will not exist, leading to errors during inference.\n",
    "  - This implies no truly \"cold-start\" users are handled natively in this approach.\n",
    "\n",
    "### **2. Temporal Splits**\n",
    "- **Validation and Test Ratings Must Be Newer Than Training Ratings**:\n",
    "  - To evaluate the model's ability to generalize over time, validation and test splits should only contain interactions (ratings) that occur after the training interactions.\n",
    "  - This is a **temporal train-test split**, where the training set contains older ratings, and validation/test sets contain newer ratings for the same set of users.\n",
    "\n",
    "### **3. Batch Training**\n",
    "- **Feeding Users in Batches**:\n",
    "  - During training, the model learns embeddings incrementally, batch by batch. However, the **embedding table is shared across all batches**.\n",
    "  - Each batch contains a subset of users and items, but the model updates the global embeddings based on the interactions in the current batch.\n",
    "\n",
    "### **4. Inference**\n",
    "- **Same Number of Users During Inference**:\n",
    "  - During inference, the model expects the same user count as in training (due to the fixed embedding table size).\n",
    "  - If fewer users are provided, the missing embeddings will not exist, causing errors.\n",
    "  - If more users are provided, the embedding table will not have enough entries, also causing errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **Best Practices with Static Embedding Approaches**\n",
    "\n",
    "### **1. Ensure Temporal Train-Test Split**\n",
    "- Split the data based on timestamps:\n",
    "  - Training set: Older interactions.\n",
    "  - Validation set: Intermediate interactions.\n",
    "  - Test set: Newest interactions.\n",
    "  \n",
    "```python\n",
    "# Example Temporal Split\n",
    "ratings = ratings.sort_values(\"timestamp\")\n",
    "train_data = ratings[ratings[\"timestamp\"] < val_start_time]\n",
    "val_data = ratings[(ratings[\"timestamp\"] >= val_start_time) & (ratings[\"timestamp\"] < test_start_time)]\n",
    "test_data = ratings[ratings[\"timestamp\"] >= test_start_time]\n",
    "```\n",
    "\n",
    "### **2. Include All Users in Train, Val, and Test**\n",
    "- Ensure that all users have some interactions in the training set.\n",
    "- Validation and test sets should only add new interactions for the same users.\n",
    "\n",
    "### **3. Use Content Features for Cold-Start Scenarios**\n",
    "- Introduce additional features for users and items (e.g., demographics, metadata).\n",
    "- Use these features to generate embeddings for new users or items when the static approach fails.\n",
    "\n",
    "### **4. Consider Hybrid Approaches**\n",
    "- Combine the static embedding approach with feature-based methods to handle new users/items dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## **Questions & Answers**\n",
    "\n",
    "1. **Does the static approach depend on having all users in train, val, and test?**\n",
    "   - Yes, all users must be present in the dataset because their embeddings are pre-trained during training.\n",
    "\n",
    "2. **Must train, val, and test be split temporally?**\n",
    "   - Yes, temporal splits are necessary to ensure that the model only predicts future interactions based on past data.\n",
    "\n",
    "3. **Do we feed all users to the net during training and inference?**\n",
    "   - During training, users are fed in mini-batches, but the embeddings for all users are globally updated across batches.\n",
    "   - During inference, the same number of users must be provided because the embedding table size is fixed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Flexible Alternatives**\n",
    "To address these limitations:\n",
    "1. Use **content-based features** to derive embeddings dynamically.\n",
    "2. Adopt hybrid models that combine collaborative and content-based approaches.\n",
    "3. Transition to models that are **agnostic to user count**, as discussed in earlier responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
