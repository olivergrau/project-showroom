{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding $R = U \\cdot V^T$ with a Simple Example\n",
    "\n",
    "In Matrix Factorization, we approximate the User-Item Interaction Matrix $R$ as the product of two smaller matrices:\n",
    "$\n",
    "R \\approx U \\cdot V^T\n",
    "$\n",
    "- $U$: User latent feature matrix ($n_{users} \\times k$).\n",
    "- $V$: Item latent feature matrix ($n_{items} \\times k$).\n",
    "- $k$: Number of latent features (a hyperparameter).\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "\n",
    "### Given\n",
    "Assume we have:\n",
    "- 2 users ($n_{users} = 2$).\n",
    "- 3 items ($n_{items} = 3$).\n",
    "- 2 latent features ($k = 2$).\n",
    "\n",
    "### Latent Matrices\n",
    "1. **User Latent Matrix ($U$)**:\n",
    "   Each row represents a user in the latent feature space.\n",
    "   $$\n",
    "   U = \\begin{bmatrix}\n",
    "   0.5 & 1.0 \\\\\\\\\n",
    "   1.5 & 0.8\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   - User 1: Latent feature vector $[0.5, 1.0]$.\n",
    "   - User 2: Latent feature vector $[1.5, 0.8]$.\n",
    "\n",
    "2. **Item Latent Matrix ($V$)**:\n",
    "   Each row represents an item in the latent feature space.\n",
    "   $$\n",
    "   V = \\begin{bmatrix}\n",
    "   0.6 & 0.9 \\\\\\\\\n",
    "   1.2 & 0.7 \\\\\\\\\n",
    "   0.4 & 1.1\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "   - Item 1: Latent feature vector $[0.6, 0.9]$.\n",
    "   - Item 2: Latent feature vector $[1.2, 0.7]$.\n",
    "   - Item 3: Latent feature vector $[0.4, 1.1]$.\n",
    "\n",
    "---\n",
    "\n",
    "### Compute $R = U \\cdot V^T$\n",
    "\n",
    "To compute $R$, take the dot product of $U$ (size $2 \\times 2$) with $V^T$ (size $2 \\times 3$):\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "0.5 & 1.0 \\\\\\\\\n",
    "1.5 & 0.8\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.6 & 1.2 & 0.4 \\\\\\\\\n",
    "0.9 & 0.7 & 1.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### Step-by-Step Calculation\n",
    "1. For User 1 and Item 1:\n",
    "   $$\n",
    "   R_{1,1} = (0.5 \\cdot 0.6) + (1.0 \\cdot 0.9) = 0.3 + 0.9 = 1.2\n",
    "   $$\n",
    "\n",
    "2. For User 1 and Item 2:\n",
    "   $$\n",
    "   R_{1,2} = (0.5 \\cdot 1.2) + (1.0 \\cdot 0.7) = 0.6 + 0.7 = 1.3\n",
    "   $$\n",
    "\n",
    "3. For User 1 and Item 3:\n",
    "   $$\n",
    "   R_{1,3} = (0.5 \\cdot 0.4) + (1.0 \\cdot 1.1) = 0.2 + 1.1 = 1.3\n",
    "   $$\n",
    "\n",
    "4. For User 2 and Item 1:\n",
    "   $$\n",
    "   R_{2,1} = (1.5 \\cdot 0.6) + (0.8 \\cdot 0.9) = 0.9 + 0.72 = 1.62\n",
    "   $$\n",
    "\n",
    "5. For User 2 and Item 2:\n",
    "   $$\n",
    "   R_{2,2} = (1.5 \\cdot 1.2) + (0.8 \\cdot 0.7) = 1.8 + 0.56 = 2.36\n",
    "   $$\n",
    "\n",
    "6. For User 2 and Item 3:\n",
    "   $$\n",
    "   R_{2,3} = (1.5 \\cdot 0.4) + (0.8 \\cdot 1.1) = 0.6 + 0.88 = 1.48\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### Resulting Matrix\n",
    "The predicted User-Item Interaction Matrix $R$:\n",
    "$$\n",
    "R = \\begin{bmatrix}\n",
    "1.2 & 1.3 & 1.3 \\\\\\\\\n",
    "1.62 & 2.36 & 1.48\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Explanation of $k$\n",
    "- **What is $k$?**\n",
    "  - $k$ is the number of latent features (dimensions) used to represent users and items.\n",
    "  - It determines the size of the latent feature space.\n",
    "\n",
    "- **How does $k$ affect the model?**\n",
    "  - **Small $k$**:\n",
    "    - Captures only the most dominant patterns.\n",
    "    - Risk of underfitting (missing finer details of user preferences).\n",
    "  - **Large $k$**:\n",
    "    - Can capture more granular relationships.\n",
    "    - Risk of overfitting (learning noise in the data).\n",
    "\n",
    "In this example, $k = 2$ means each user and item is represented by two latent features.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "1. $R = U \\cdot V^T$ approximates the interaction matrix based on learned latent features.\n",
    "2. $k$ is a hyperparameter that balances the model’s complexity and its ability to generalize.\n",
    "3. The dot product combines user and item latent features to predict interactions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How a Smaller $k$ than the Rank of $M$ Can Reconstruct the Matrix Form\n",
    "\n",
    "Matrix Factorization with $k < \\text{rank}(M)$ cannot reconstruct the **exact values** of the original matrix, but it can approximate its **shape or structure** by capturing the most significant patterns or features. Here’s a detailed explanation:\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Rank of a Matrix**:\n",
    "   - The rank of a matrix $M$ represents the maximum number of linearly independent rows or columns.\n",
    "   - For exact reconstruction, the number of latent features $k$ must be equal to the rank of $M$.\n",
    "\n",
    "2. **When $k < \\text{rank}(M)$**:\n",
    "   - Using fewer latent features ($k < \\text{rank}(M)$), the factorization approximates the matrix.\n",
    "   - The lower $k$ captures only the **dominant patterns** or relationships in the data, ignoring minor details or noise.\n",
    "\n",
    "3. **Approximation with SVD**:\n",
    "   - SVD (Singular Value Decomposition) allows us to decompose $M$ as:\n",
    "     $$\n",
    "     M = U \\cdot \\Sigma \\cdot V^T\n",
    "     $$\n",
    "     - $U$: User latent matrix.\n",
    "     - $\\Sigma$: Diagonal matrix of singular values, representing the importance of each latent feature.\n",
    "     - $V$: Item latent matrix.\n",
    "   - Truncating $\\Sigma$ to the top-$k$ singular values retains the most significant latent features and produces a low-rank approximation of $M$:\n",
    "     $$\n",
    "     M_k = U_k \\cdot \\Sigma_k \\cdot V_k^T\n",
    "     $$\n",
    "   - $M_k$ approximates the shape or structure of $M$.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Matrix Approximation with $k < \\text{rank}(M)$\n",
    "\n",
    "### Original Matrix ($M$)\n",
    "Let’s assume we have a $4 \\times 4$ matrix $M$ with rank 4:\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "5 & 4 & 2 & 1 \\\\\\\\\n",
    "4 & 5 & 3 & 2 \\\\\\\\\n",
    "1 & 3 & 5 & 4 \\\\\\\\\n",
    "2 & 1 & 4 & 5\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### SVD Decomposition\n",
    "Using SVD, we decompose $M$ into:\n",
    "$$\n",
    "M = U \\cdot \\Sigma \\cdot V^T\n",
    "$$\n",
    "Where:\n",
    "- $U$: $4 \\times 4$ orthogonal matrix.\n",
    "- $\\Sigma$: $4 \\times 4$ diagonal matrix of singular values:\n",
    "  $$\n",
    "  \\Sigma = \\begin{bmatrix}\n",
    "  14 & 0 & 0 & 0 \\\\\\\\\n",
    "  0 & 6 & 0 & 0 \\\\\\\\\n",
    "  0 & 0 & 3 & 0 \\\\\\\\\n",
    "  0 & 0 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- $V$: $4 \\times 4$ orthogonal matrix.\n",
    "\n",
    "### Truncating to $k = 2$\n",
    "To approximate $M$ with $k = 2$, we keep only the top 2 singular values in $\\Sigma$:\n",
    "$$\n",
    "\\Sigma_k = \\begin{bmatrix}\n",
    "14 & 0 & 0 & 0 \\\\\\\\\n",
    "0 & 6 & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The truncated matrices $U_k$ and $V_k^T$ are:\n",
    "- $U_k$: $4 \\times 2$ matrix containing the first 2 columns of $U$.\n",
    "- $V_k^T$: $2 \\times 4$ matrix containing the first 2 rows of $V^T$.\n",
    "\n",
    "### Approximation ($M_k$)\n",
    "The low-rank approximation of $M$ is:\n",
    "$$\n",
    "M_k = U_k \\cdot \\Sigma_k \\cdot V_k^T\n",
    "$$\n",
    "This produces:\n",
    "$$\n",
    "M_k = \\begin{bmatrix}\n",
    "5.1 & 3.9 & 2.1 & 1.2 \\\\\\\\\n",
    "3.8 & 4.8 & 3.2 & 2.1 \\\\\\\\\n",
    "1.2 & 2.9 & 4.9 & 4.1 \\\\\\\\\n",
    "2.1 & 1.3 & 3.8 & 5.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- $M_k$ retains the **overall structure** of $M$ but not the exact values.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Does This Work?\n",
    "1. **Latent Features Capture Dominant Patterns**:\n",
    "   - The top-$k$ singular values and their corresponding singular vectors capture the most significant relationships in the data.\n",
    "   - For example, in a movie recommendation system:\n",
    "     - Latent features might represent genres, popularity, or user preferences.\n",
    "\n",
    "2. **Dimensionality Reduction**:\n",
    "   - By using $k < \\text{rank}(M)$, we project users and items into a lower-dimensional space, simplifying the data while retaining its essential structure.\n",
    "\n",
    "3. **Trade-Off**:\n",
    "   - A smaller $k$ sacrifices precision for generality, filtering out noise and less significant patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- When $k < \\text{rank}(M)$, the reconstructed matrix $M_k$ is an **approximation** of $M$.\n",
    "- $k$ determines the level of detail retained:\n",
    "  - Small $k$ captures only dominant patterns.\n",
    "  - Larger $k$ retains finer details but risks overfitting.\n",
    "- The shape of the original matrix (number of rows and columns) is preserved, but the exact values are approximated.\n",
    "\n",
    "This explains how smaller $k$ can still capture the overall structure of the matrix while simplifying the data representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Matrix Factorization Handles Sparse Matrices\n",
    "\n",
    "When we perform **Matrix Factorization**, the sizes of the latent matrices $U$ and $V$ are determined by the dimensions of the original matrix $M$ and the number of latent features $k$. However, the latent matrices $U$ and $V$ themselves are **not sparse**. Here's an explanation:\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Points**\n",
    "\n",
    "1. **Original Matrix $M$**:\n",
    "   - Dimensions: $n_{users} \\times n_{items}$.\n",
    "   - Often sparse: Most real-world User-Item Matrices have many missing values (interactions that didn’t happen).\n",
    "\n",
    "2. **Latent Matrices $U$ and $V$**:\n",
    "   - **$U$ (User Latent Matrix)**:\n",
    "     - Size: $n_{users} \\times k$.\n",
    "     - Each row represents a user’s latent feature vector.\n",
    "   - **$V$ (Item Latent Matrix)**:\n",
    "     - Size: $n_{items} \\times k$.\n",
    "     - Each row represents an item’s latent feature vector.\n",
    "   - These matrices are **dense**, meaning all positions typically have values (no zeros unless they are part of the learned representation).\n",
    "\n",
    "3. **Matrix Reconstruction**:\n",
    "   - The reconstructed matrix $M_k = U \\cdot V^T$ will have the same size as the original matrix $M$ ($n_{users} \\times n_{items}$).\n",
    "   - $M_k$ is dense, but we only compare its predicted values to the observed (non-zero) entries in $M$.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Aren’t $U$ and $V$ Sparse?**\n",
    "\n",
    "1. **Learning Latent Features**:\n",
    "   - The purpose of $U$ and $V$ is to learn **latent representations** that summarize user preferences and item characteristics.\n",
    "   - These features are shared across users and items, making the matrices dense (all features contribute to the prediction).\n",
    "\n",
    "2. **Prediction for All User-Item Pairs**:\n",
    "   - Even for unobserved entries in $M$, $U$ and $V$ can generate predictions because the latent factors generalize across the data.\n",
    "\n",
    "3. **Sparse Original Matrix, Dense Latent Matrices**:\n",
    "   - The sparsity of $M$ is handled by training only on observed entries, but $U$ and $V$ themselves are dense to enable predictions for all potential interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "### Given:\n",
    "Original matrix $M$ (sparse, $4 \\times 3$):\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "5 & 4 & 0 \\\\\\\\\n",
    "4 & 0 & 3 \\\\\\\\\n",
    "0 & 3 & 5 \\\\\\\\\n",
    "2 & 0 & 4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- $M$ is sparse, with missing values represented by 0.\n",
    "\n",
    "Latent matrices $U$ and $V$ (dense, $k = 2$):\n",
    "- $U$ (User Latent Matrix, $4 \\times 2$):\n",
    "$$\n",
    "U = \\begin{bmatrix}\n",
    "0.5 & 1.0 \\\\\\\\\n",
    "1.2 & 0.8 \\\\\\\\\n",
    "0.7 & 0.6 \\\\\\\\\n",
    "0.9 & 1.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "- $V$ (Item Latent Matrix, $3 \\times 2$):\n",
    "$$\n",
    "V = \\begin{bmatrix}\n",
    "0.6 & 0.9 \\\\\\\\\n",
    "1.1 & 0.8 \\\\\\\\\n",
    "0.5 & 1.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Reconstructed Matrix ($M_k = U \\cdot V^T$):\n",
    "$$\n",
    "M_k = \\begin{bmatrix}\n",
    "1.35 & 1.65 & 1.55 \\\\\\\\\n",
    "1.68 & 2.08 & 2.01 \\\\\\\\\n",
    "1.02 & 1.26 & 1.23 \\\\\\\\\n",
    "1.74 & 2.07 & 2.06\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $M_k$ is dense: All user-item pairs have predicted values.\n",
    "- Only observed entries in $M$ are used during training, but $U$ and $V$ generalize to predict missing values.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "\n",
    "- $U$ and $V$ are **dense** because they represent the latent feature vectors of users and items, which are used to predict interactions for all pairs.\n",
    "- $M$ is typically **sparse** in real-world data, but $M_k$ is dense after reconstruction.\n",
    "- The factorization handles sparsity by training only on observed entries in $M$, without directly imposing sparsity on $U$ or $V$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are Predicted Values Used for Recommendations?\n",
    "\n",
    "Yes! The **non-zero values** in the reconstructed matrix $M_k$ (which were originally missing or zero in the sparse matrix $M$) are the **predicted ratings**. These values are used to estimate what a user might like. Here's how it works:\n",
    "\n",
    "---\n",
    "\n",
    "## **How the Reconstructed Matrix $M_k$ is Used for Predictions**\n",
    "\n",
    "1. **Observed Values in $M$**:\n",
    "   - The original matrix $M$ contains observed interactions (e.g., ratings) and many missing values (represented as zeros or NaN).\n",
    "   - During training, the model learns from the **non-zero entries** in $M$.\n",
    "\n",
    "2. **Predicted Values in $M_k$**:\n",
    "   - The reconstructed matrix $M_k$ contains **predicted values** for all user-item pairs, including those that were missing in $M$.\n",
    "   - These values represent the model's **best guess** of how a user would interact with an item based on:\n",
    "     - Latent user preferences (from $U$).\n",
    "     - Latent item characteristics (from $V$).\n",
    "\n",
    "3. **Recommendation Candidates**:\n",
    "   - For each user:\n",
    "     - The rows of $M_k$ contain predicted ratings for all items.\n",
    "     - The **missing interactions** (which were zero in $M$) are now filled with non-zero predicted ratings in $M_k$.\n",
    "   - Items with **high predicted ratings** in $M_k$ are considered good candidates for recommendations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Example**\n",
    "\n",
    "### Original Matrix $M$ (Sparse)\n",
    "$$\n",
    "M = \\begin{bmatrix}\n",
    "5 & 4 & 0 \\\\\\\\\n",
    "4 & 0 & 3 \\\\\\\\\n",
    "0 & 3 & 5 \\\\\\\\\n",
    "2 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Rows: Users\n",
    "- Columns: Items\n",
    "- Observed ratings: Non-zero values\n",
    "- Missing interactions: Represented as zeros.\n",
    "\n",
    "### Reconstructed Matrix $M_k$\n",
    "$$\n",
    "M_k = \\begin{bmatrix}\n",
    "5.0 & 4.1 & 3.8 \\\\\\\\\n",
    "4.1 & 3.7 & 3.2 \\\\\\\\\n",
    "3.6 & 3.1 & 4.9 \\\\\\\\\n",
    "2.1 & 2.8 & 3.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $M_k$ now contains predicted ratings for all user-item pairs.\n",
    "- For example:\n",
    "  - For User 1 (row 1), the predicted rating for Item 3 is **3.8**.\n",
    "  - For User 4 (row 4), the predicted ratings for Items 2 and 3 are **2.8** and **3.4**, respectively.\n",
    "\n",
    "---\n",
    "\n",
    "## **Making Recommendations**\n",
    "\n",
    "1. **Filter Out Observed Interactions**:\n",
    "   - When generating recommendations, ignore items that the user has already interacted with in the original matrix $M$.\n",
    "   - Example for User 1:\n",
    "     - Observed items: Item 1 (rating 5) and Item 2 (rating 4).\n",
    "     - Focus on Item 3 for recommendations.\n",
    "\n",
    "2. **Sort Predicted Ratings**:\n",
    "   - For each user, sort the unobserved items by their predicted ratings.\n",
    "   - Example for User 1:\n",
    "     - Predicted ratings for unobserved items: Item 3 → **3.8**.\n",
    "     - Recommend Item 3 to User 1.\n",
    "\n",
    "3. **Top-N Recommendations**:\n",
    "   - Select the top $N$ items with the highest predicted ratings.\n",
    "   - Example:\n",
    "     - For User 4, recommend Items 3 (rating 3.4) and 2 (rating 2.8).\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Considerations**\n",
    "\n",
    "1. **Missing Values Are Predictions**:\n",
    "   - The previously missing values in $M$ (zeros or NaNs) are now populated with predicted ratings in $M_k$.\n",
    "   - These predictions indicate the likelihood of a user liking an item.\n",
    "\n",
    "2. **Thresholding**:\n",
    "   - In some cases, you might only recommend items with predicted ratings above a certain threshold (e.g., $M_k[u, i] > 4$).\n",
    "\n",
    "3. **Ranking**:\n",
    "   - Recommendations are typically ranked by predicted ratings, ensuring the top-rated items are suggested.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "- The predicted values in $M_k$ (corresponding to originally zero or missing values in $M$) are used to recommend items to users.\n",
    "- Items with high predicted ratings are strong candidates for recommendations.\n",
    "- The model generalizes from observed interactions to make predictions for all user-item pairs, even those with no prior data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is $\\Sigma$ Not Used in the Training?\n",
    "\n",
    "In traditional **Singular Value Decomposition (SVD)**, the matrix $M$ is factorized as:\n",
    "$$\n",
    "M = U \\cdot \\Sigma \\cdot V^T\n",
    "$$\n",
    "- $U$: Orthogonal matrix representing users.\n",
    "- $\\Sigma$: Diagonal matrix of singular values, representing the importance of each latent feature.\n",
    "- $V$: Orthogonal matrix representing items.\n",
    "\n",
    "However, in the **Matrix Factorization approach for recommendation systems**, we do not explicitly use $\\Sigma$ during training. Here’s why:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Role of $\\Sigma$ in SVD**\n",
    "\n",
    "- In SVD, $\\Sigma$ contains the singular values of $M$ (eigenvalues), which measure the contribution of each latent feature to the overall structure of $M$.\n",
    "- Truncating $\\Sigma$ to the top $k$ singular values corresponds to keeping the most significant latent features and discarding less important ones.\n",
    "- The truncated SVD reconstruction is:\n",
    "  $$\n",
    "  M_k = U_k \\cdot \\Sigma_k \\cdot V_k^T\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## **2. How Matrix Factorization Handles This**\n",
    "\n",
    "In recommendation systems, **Matrix Factorization** approximates $M$ as:\n",
    "$$\n",
    "M \\approx U \\cdot V^T\n",
    "$$\n",
    "- $U$: Learned user latent matrix ($n_{users} \\times k$).\n",
    "- $V$: Learned item latent matrix ($n_{items} \\times k$).\n",
    "- There is **no explicit $\\Sigma$ matrix** because:\n",
    "\n",
    "### **Why $\\Sigma$ is Not Used Directly**\n",
    "1. **Learning the Latent Space**:\n",
    "   - In SVD, $\\Sigma$ comes from a deterministic mathematical decomposition of $M$.\n",
    "   - In Matrix Factorization, we \"learn\" the latent space through optimization (e.g., SGD), rather than directly computing $\\Sigma$.\n",
    "\n",
    "2. **Implicit Role of $\\Sigma$**:\n",
    "   - During training, the magnitude of the learned embeddings in $U$ and $V$ indirectly captures the importance of the latent features, similar to $\\Sigma$ in SVD.\n",
    "   - The optimization process balances the contributions of each latent feature, mimicking the effect of truncating $\\Sigma$ in traditional SVD.\n",
    "\n",
    "3. **Truncation is Handled by $k$**:\n",
    "   - The hyperparameter $k$ (number of latent features) determines the dimensionality of $U$ and $V$.\n",
    "   - Choosing a smaller $k$ effectively truncates the latent feature space, similar to keeping the top-$k$ singular values in $\\Sigma$.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Advantages of Not Using $\\Sigma$ Explicitly**\n",
    "\n",
    "- **Flexibility**:\n",
    "  - In SVD, $\\Sigma$ is fixed once $M$ is factorized. In contrast, Matrix Factorization allows $U$ and $V$ to adapt dynamically during training to minimize the loss function.\n",
    "\n",
    "- **Efficiency**:\n",
    "  - Computing $\\Sigma$ explicitly would require factorizing the entire matrix $M$, which is infeasible for large, sparse matrices.\n",
    "\n",
    "- **Compatibility with Sparse Matrices**:\n",
    "  - Matrix Factorization is trained only on observed values of $M$, while traditional SVD requires $M$ to be dense.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary: Does $k$ Represent $\\Sigma$?**\n",
    "\n",
    "Yes, the truncation of $k$ in Matrix Factorization serves a similar role to $\\Sigma$ in SVD:\n",
    "- $k$ limits the number of latent features, akin to keeping the top-$k$ singular values in $\\Sigma$.\n",
    "- The magnitude of the embeddings in $U$ and $V$ implicitly captures the importance of latent features, replacing the explicit need for $\\Sigma$.\n",
    "\n",
    "By learning $U$ and $V$ through optimization, Matrix Factorization achieves similar results to truncated SVD but with added flexibility and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
