{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "I tried first a baseline decoder implementation with a very basic architecture: a vector embedding, a LSTM and a linear layer to map the probabilities to the vocabular. You can find the implementation in the model.py (see BaseLineDecoderCNN). The EncoderCNN was predefined. But I read that it would be better to not remove the spatial information nearly completely of the ResNet50 layers, so I disabled the last average pool and fc layer. I then started to train with a simple combination of such an encoder (with more spatial information) and a LSTM based decoder. But then I read that this is not state of the art anymore so I researched a bit more. An stumbled about Transformer Architectures with Attention. Sadly those transformer architectures have nothing to do with RNN, so I decided for a compromise: \n",
    "\n",
    "I decided to implement something more fancy than a standard LSTM based decoder and found a paper which suggests using an attention mechanism (Bahdanau Attention) with GRU based units. GRU is still RNN based, so I think this should be ok. You (the reviewer) can find the implementation in the model.py. There is an attention module and a DecoderGRU implementation.\n",
    "\n",
    "How did I choose the hyper parameters:\n",
    "\n",
    "batch_size = 256          # because my laptop GPU has 8 GB of ram and this was ok for my hardware (I always monitor my hardware when I train a net)\n",
    "vocab_threshold = 5        # I kept it to a default value\n",
    "embed_size = 256           # see 1)\n",
    "hidden_size = 512          # see 2) \n",
    "num_epochs = 3             # one epoch took about 2h, I kept my laptop powered on over night for 3 epoch\n",
    "\n",
    "1) Many pre-trained models (e.g., ResNet) output feature maps with high channel counts (often 512, 1024, or 2048). Reducing these to an embed_size of 256 is a good compromise for creating manageable embeddings that still retain significant information. An embed size of 256 helps reduce the dimensions while still maintaining a rich representation, making it easier to process the embeddings in the attention mechanism and the RNN.\n",
    "\n",
    "2) With a hidden_size of 512, the GRU has enough memory to maintain a strong representation of the sentenceâ€™s context as it is generated.\n",
    "    For sequential tasks like captioning, larger hidden sizes can help the RNN retain important information over longer sequences, which is essential in generating coherent captions.\n",
    "    \n",
    "I started with those values and because one epoch took more than 2 hours, I didn't have much room for empirical experiments. \n",
    "\n",
    "My source: https://arxiv.org/pdf/2203.01594\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "I have left the transformations as they were. The COCO dataset has already good photos, so standard transformations (like crop and resize) seem ok for me. By the way, because of the amount of time one epoch took to train, I didn't have much room for experiments. \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because I wanted to leverage the pretrained ResNet50 weights I only activated the gradients for the newly added layer in the encoder: \n",
    "\n",
    "# Map the ResNet feature map to the desired embedding size\n",
    "self.embed = nn.Conv2d(2048, embed_size, kernel_size=1)  # 2048 channels in ResNet50's final conv layer\n",
    "\n",
    "So for my decoder I only activated the gradients for the newly added (replaced the last two layers of ResNet50) embed layer which produces an embedding for the features.\n",
    "As I said before, I removed the last two layers (avg pool and fc) to keep more spatial information for the decoder.\n",
    "\n",
    "And of course all gradients of the decoder were activated because we wanted it to train and learn anything new.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "I googled and again the ADAM optimizer is a good choice (as always, because its integrated momentum). And I say it again, I did not have much room for experiments because of the time an epoch took to train (2h/epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:21:48.370872Z",
     "start_time": "2024-11-13T18:20:41.968720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 591753/591753 [00:55<00:00, 10619.58it/s]\n",
      "/tmp/ipykernel_881778/1686264058.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load('models/encoder-3.pkl', map_location=device))\n",
      "/tmp/ipykernel_881778/1686264058.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load('models/decoder-3.pkl', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "#sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, BaseLineDecoderCNN, DecoderGRU\n",
    "import math\n",
    "import torch.optim as optim\n",
    "\n",
    "# watch for any changes in model.py, if it changes, re-load it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "## Select appropriate values for the Python variables below.\n",
    "batch_size = 256          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 2             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file,\n",
    "                         num_workers=10)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderGRU(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# 'models/encoder-1.pkl', 'models/decoder-1.pkl'\n",
    "encoder.load_state_dict(torch.load('models/encoder-3.pkl', map_location=device))\n",
    "decoder.load_state_dict(torch.load('models/decoder-3.pkl', map_location=device))\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# Specify the learnable parameters of the model.\n",
    "params = []\n",
    "for name, param in encoder.named_parameters():\n",
    "    if \"embed\" in name:\n",
    "        param.requires_grad = True\n",
    "        params.append(param)\n",
    "\n",
    "# Add all parameters of the Decoder\n",
    "params += list(decoder.parameters())\n",
    "\n",
    "# Define the optimizer.\n",
    "learning_rate = 1e-3  # You can start with this learning rate and adjust based on results\n",
    "optimizer = optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T01:28:36.609800Z",
     "start_time": "2024-11-12T20:39:14.494440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Epoch [1/2], Step [100/2312], Loss: 2.3420, Perplexity: 10.4017\n",
      "Epoch [1/2], Step [200/2312], Loss: 2.3748, Perplexity: 10.7488\n",
      "Epoch [1/2], Step [300/2312], Loss: 2.7663, Perplexity: 15.89945\n",
      "Epoch [1/2], Step [400/2312], Loss: 3.3749, Perplexity: 29.2206\n",
      "Epoch [1/2], Step [500/2312], Loss: 2.3255, Perplexity: 10.2319\n",
      "Epoch [1/2], Step [600/2312], Loss: 2.3154, Perplexity: 10.1291\n",
      "Epoch [1/2], Step [700/2312], Loss: 2.2872, Perplexity: 9.84718\n",
      "Epoch [1/2], Step [800/2312], Loss: 2.2652, Perplexity: 9.63280\n",
      "Epoch [1/2], Step [900/2312], Loss: 2.2661, Perplexity: 9.64184\n",
      "Epoch [1/2], Step [1000/2312], Loss: 2.2079, Perplexity: 9.0966\n",
      "Epoch [1/2], Step [1100/2312], Loss: 2.3775, Perplexity: 10.7783\n",
      "Epoch [1/2], Step [1200/2312], Loss: 2.2434, Perplexity: 9.42545\n",
      "Epoch [1/2], Step [1300/2312], Loss: 2.1875, Perplexity: 8.91286\n",
      "Epoch [1/2], Step [1400/2312], Loss: 2.1131, Perplexity: 8.27402\n",
      "Epoch [1/2], Step [1500/2312], Loss: 2.7132, Perplexity: 15.0767\n",
      "Epoch [1/2], Step [1600/2312], Loss: 2.2669, Perplexity: 9.649518\n",
      "Epoch [1/2], Step [1700/2312], Loss: 2.5799, Perplexity: 13.1957\n",
      "Epoch [1/2], Step [1800/2312], Loss: 2.8100, Perplexity: 16.6105\n",
      "Epoch [1/2], Step [1900/2312], Loss: 2.3196, Perplexity: 10.1717\n",
      "Epoch [1/2], Step [2000/2312], Loss: 2.1142, Perplexity: 8.28280\n",
      "Epoch [1/2], Step [2100/2312], Loss: 2.6015, Perplexity: 13.4836\n",
      "Epoch [1/2], Step [2200/2312], Loss: 2.2663, Perplexity: 9.64395\n",
      "Epoch [1/2], Step [2300/2312], Loss: 2.3311, Perplexity: 10.2894\n",
      "Epoch [2/2], Step [100/2312], Loss: 2.3261, Perplexity: 10.23815\n",
      "Epoch [2/2], Step [200/2312], Loss: 2.2826, Perplexity: 9.80185\n",
      "Epoch [2/2], Step [300/2312], Loss: 2.3529, Perplexity: 10.5161\n",
      "Epoch [2/2], Step [400/2312], Loss: 2.1739, Perplexity: 8.79225\n",
      "Epoch [2/2], Step [500/2312], Loss: 2.1787, Perplexity: 8.83462\n",
      "Epoch [2/2], Step [600/2312], Loss: 2.1878, Perplexity: 8.91547\n",
      "Epoch [2/2], Step [700/2312], Loss: 2.2699, Perplexity: 9.67832\n",
      "Epoch [2/2], Step [800/2312], Loss: 2.2656, Perplexity: 9.63726\n",
      "Epoch [2/2], Step [900/2312], Loss: 2.2558, Perplexity: 9.54263\n",
      "Epoch [2/2], Step [1000/2312], Loss: 2.2414, Perplexity: 9.4066\n",
      "Epoch [2/2], Step [1100/2312], Loss: 2.3099, Perplexity: 10.0739\n",
      "Epoch [2/2], Step [1200/2312], Loss: 2.1350, Perplexity: 8.45680\n",
      "Epoch [2/2], Step [1300/2312], Loss: 2.5823, Perplexity: 13.2276\n",
      "Epoch [2/2], Step [1400/2312], Loss: 2.1690, Perplexity: 8.74955\n",
      "Epoch [2/2], Step [1500/2312], Loss: 2.1947, Perplexity: 8.97697\n",
      "Epoch [2/2], Step [1600/2312], Loss: 2.2415, Perplexity: 9.40708\n",
      "Epoch [2/2], Step [1700/2312], Loss: 2.1031, Perplexity: 8.19125\n",
      "Epoch [2/2], Step [1800/2312], Loss: 2.4273, Perplexity: 11.3279\n",
      "Epoch [2/2], Step [1900/2312], Loss: 2.2915, Perplexity: 9.88942\n",
      "Epoch [2/2], Step [2000/2312], Loss: 2.2369, Perplexity: 9.36384\n",
      "Epoch [2/2], Step [2100/2312], Loss: 2.1104, Perplexity: 8.25195\n",
      "Epoch [2/2], Step [2200/2312], Loss: 2.2400, Perplexity: 9.39344\n",
      "Epoch [2/2], Step [2300/2312], Loss: 2.2432, Perplexity: 9.42383\n",
      "Epoch [2/2], Step [2312/2312], Loss: 2.1685, Perplexity: 8.74472"
     ]
    }
   ],
   "source": [
    "# watch for any changes in model.py, if it changes, re-load it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        \n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "\n",
    "        # Calculate the batch loss.        \n",
    "        # Using reshape (more flexible tensor does not need to be contigous)\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
    "\n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:46:20.121742Z",
     "start_time": "2024-11-13T09:46:20.081759Z"
    }
   },
   "outputs": [],
   "source": [
    "# Debugging for the poor. My debugger isn't working in my PyCharm. \n",
    "def inspect_data_types(data, depth=2, current_depth=0):\n",
    "    \"\"\"\n",
    "    Recursively inspects data types within a nested structure up to a specified depth.\n",
    "    \n",
    "    Args:\n",
    "    - data: The data structure to inspect (e.g., dict, list, tuple).\n",
    "    - depth: The maximum depth to inspect.\n",
    "    - current_depth: The current depth in recursion (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    - A nested structure representing data types.\n",
    "    \"\"\"\n",
    "    if current_depth >= depth:\n",
    "        return type(data)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        return {key: inspect_data_types(value, depth, current_depth + 1) for key, value in data.items()}\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        return [inspect_data_types(item, depth, current_depth + 1) for item in data]\n",
    "\n",
    "    elif isinstance(data, tuple):\n",
    "        return tuple(inspect_data_types(item, depth, current_depth + 1) for item in data)\n",
    "\n",
    "    else:\n",
    "        return type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Only Five Captions Were Delivered by `DataLoader`\n",
    "\n",
    "I encountered an issue witht the DataLoader â€” where only 5 captions were delivered by the batch loaderâ€”was due to how PyTorch's `DataLoader` handles batching and the structure of the captions in the dataset.\n",
    "\n",
    "### Original Issue with `DataLoader` and Batching\n",
    "\n",
    "1. **DataLoader's Expectation of Consistent Data Shapes**:\n",
    "   - By default, `DataLoader` expects each item in a batch to have a **consistent tensor shape**. This means that all elements (e.g., images or labels) should have the same structure, allowing them to be stacked into a single tensor for efficient processing.\n",
    "   - In your original setup, each image had a list of captions, which caused variability in the data structure. Some images might have 5 captions, others fewer, creating a **list of lists** (variable-length lists) that cannot be directly stacked into a tensor.\n",
    "\n",
    "2. **Automatic Collation Issue**:\n",
    "   - Without a custom collate function, `DataLoader` tries to collate the data automatically.\n",
    "   - When `DataLoader` encountered the list of captions, it interpreted it as a **sequence of individual items**, resulting in only the first 5 captions being included in the batch.\n",
    "   - Essentially, `DataLoader` treated the list of captions as a single dimension rather than treating each list as a whole, so only 5 caption items were delivered across the batch instead of 5 captions per image.\n",
    "\n",
    "### Solution with `collate_fn`\n",
    "\n",
    "The custom `collate_fn` resolves this by:\n",
    "1. **Separating Images and Captions Explicitly**:\n",
    "   - The custom function `collate_fn` separates the images and captions at the batch level. It stacks images into a tensor and keeps captions as a list of lists, where each sublist corresponds to the captions for a single image.\n",
    "   \n",
    "2. **Preserving Caption Structure**:\n",
    "   - By preserving each list of captions as a whole, the `collate_fn` avoids the automatic collation issue. `DataLoader` no longer tries to interpret the captions in the wrong way.\n",
    "   - The result is a batch where the images are neatly stacked into a tensor, and the captions are correctly grouped by image, with each imageâ€™s captions kept in a separate list.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The custom `collate_fn` prevented `DataLoader` from trying to automatically stack lists of captions, which it had mistakenly flattened. Instead, the function preserved the intended structure by handling each image and its associated captions as a distinct pair, resulting in the correct number of captions per image in each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T10:01:48.475840Z",
     "start_time": "2024-11-13T10:01:48.399188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the COCO validation dataset\n",
    "val_coco = COCO(annotation_file='datasets/COCO/annotations/captions_val2017.json')\n",
    "val_images_dir = 'datasets/COCO/images/val2017/'\n",
    "\n",
    "# we have to intervene in the batching process to handle the caption list of lists problem\n",
    "def collate_fn(batch):\n",
    "    # Separate images and captions\n",
    "    images, captions = zip(*batch)\n",
    "\n",
    "    # Stack images into a tensor\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    return images, list(captions)  # List of captions for each image\n",
    "\n",
    "# Define a Dataset for the validation images and captions\n",
    "class COCOValidationDataset(Dataset):\n",
    "    def __init__(self, coco, images_dir, transform=None):\n",
    "        self.coco = coco\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_info = self.coco.loadImgs(image_id)[0]\n",
    "        image_path = os.path.join(self.images_dir, image_info['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Retrieve captions for the image\n",
    "        captions_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        captions = [self.coco.anns[c]['caption'] for c in captions_ids]\n",
    "\n",
    "        return image, captions\n",
    "\n",
    "\n",
    "# Create a DataLoader for the validation dataset\n",
    "val_dataset = COCOValidationDataset(val_coco, val_images_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T09:56:53.249472Z",
     "start_time": "2024-11-13T09:56:53.153510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images: torch.Size([7, 3, 224, 224])\n",
      "Batch of captions: [['A man is in a kitchen making pizzas.', 'Man in apron standing on front of oven with pans and bakeware', 'A baker is working in the kitchen rolling dough.', 'A person standing by a stove in a kitchen.', 'A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.'], ['The dining table near the kitchen has a bowl of fruit on it.', 'A small kitchen has various appliances and a table.', 'The kitchen is clean and ready for us to see.', 'A kitchen and dining area decorated in white.', 'A kitchen that has a bowl of fruit on the table.'], ['a person with a shopping cart on a city street ', 'City dwellers walk by as a homeless man begs for cash.', 'People walking past a homeless man begging on a city street', 'a homeless man holding a cup and standing next to a shopping cart on a street', 'People are walking on the street by a homeless person.'], ['A person on a skateboard and bike at a skate park.', 'A man on a skateboard performs a trick at the skate park', 'A skateboarder jumps into the air as he performs a skateboard trick.', 'Athletes performing tricks on a BMX bicycle and a skateboard.', 'a man falls off his skateboard in a skate park.'], ['a blue bike parked on a side walk ', 'A bicycle is chained to a fixture on a city street', 'A blue bicycle sits on a sidewalk near a street.', 'A bicycle is locked up to a post', 'a bike sits parked next to a street '], ['A bathroom that has a broken wall in the shower.', 'A bathroom looks clean but is missing tile at the shower stall.', 'A view of a bathroom that needs to be fixed up.', 'a shower toilet and sink in a basement bathroom', 'A very big whit rest room with a shabby looking shower.'], ['a couple of buckets in a white room', 'A bathroom with no toilets and a red and green bucket. ', 'a shower room with two buckets, tolet paper holder and soap.', 'A standing toilet in a bathroom next to a window.', 'This picture looks like a janitors closet with buckets on the floor.']]\n",
      "Image 1 has 5 captions.\n",
      " - A man is in a kitchen making pizzas.\n",
      " - Man in apron standing on front of oven with pans and bakeware\n",
      " - A baker is working in the kitchen rolling dough.\n",
      " - A person standing by a stove in a kitchen.\n",
      " - A table with pies being made and a person standing near a wall with pots and pans hanging on the wall.\n",
      "Image 2 has 5 captions.\n",
      " - The dining table near the kitchen has a bowl of fruit on it.\n",
      " - A small kitchen has various appliances and a table.\n",
      " - The kitchen is clean and ready for us to see.\n",
      " - A kitchen and dining area decorated in white.\n",
      " - A kitchen that has a bowl of fruit on the table.\n",
      "Image 3 has 5 captions.\n",
      " - a person with a shopping cart on a city street \n",
      " - City dwellers walk by as a homeless man begs for cash.\n",
      " - People walking past a homeless man begging on a city street\n",
      " - a homeless man holding a cup and standing next to a shopping cart on a street\n",
      " - People are walking on the street by a homeless person.\n",
      "Image 4 has 5 captions.\n",
      " - A person on a skateboard and bike at a skate park.\n",
      " - A man on a skateboard performs a trick at the skate park\n",
      " - A skateboarder jumps into the air as he performs a skateboard trick.\n",
      " - Athletes performing tricks on a BMX bicycle and a skateboard.\n",
      " - a man falls off his skateboard in a skate park.\n",
      "Image 5 has 5 captions.\n",
      " - a blue bike parked on a side walk \n",
      " - A bicycle is chained to a fixture on a city street\n",
      " - A blue bicycle sits on a sidewalk near a street.\n",
      " - A bicycle is locked up to a post\n",
      " - a bike sits parked next to a street \n",
      "Image 6 has 5 captions.\n",
      " - A bathroom that has a broken wall in the shower.\n",
      " - A bathroom looks clean but is missing tile at the shower stall.\n",
      " - A view of a bathroom that needs to be fixed up.\n",
      " - a shower toilet and sink in a basement bathroom\n",
      " - A very big whit rest room with a shabby looking shower.\n",
      "Image 7 has 5 captions.\n",
      " - a couple of buckets in a white room\n",
      " - A bathroom with no toilets and a red and green bucket. \n",
      " - a shower room with two buckets, tolet paper holder and soap.\n",
      " - A standing toilet in a bathroom next to a window.\n",
      " - This picture looks like a janitors closet with buckets on the floor.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Test the DataLoader to ensure it works as expected\n",
    "for images, captions in val_loader:\n",
    "    print(\"Batch of images:\", images.shape)\n",
    "    print(\"Batch of captions:\", captions)\n",
    "\n",
    "    # Check each image and its associated captions\n",
    "    for i, (image, caption_list) in enumerate(zip(images, captions)):\n",
    "        print(f\"Image {i + 1} has {len(caption_list)} captions.\")\n",
    "        for caption in caption_list:\n",
    "            print(f\" - {caption}\")\n",
    "\n",
    "    # Break after first batch for demonstration purposes\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T11:01:10.023685Z",
     "start_time": "2024-11-13T11:01:09.812955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# watch for any changes in model.py, if it changes, re-load it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from predict_model import ImageCaptioningPredictor\n",
    "\n",
    "# (Optional) Validate your model.\n",
    "model = ImageCaptioningPredictor(encoder, decoder, data_loader.dataset.vocab, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T11:01:13.699165Z",
     "start_time": "2024-11-13T11:01:11.980090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "features shape: torch.Size([1, 49, 256])\n",
      "Average BLEU Score on Validation Set: 0.0893\n"
     ]
    }
   ],
   "source": [
    "# watch for any changes in model.py, if it changes, re-load it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch\n",
    "\n",
    "# Initialize lists to store generated captions and ground truth captions\n",
    "generated_captions = []\n",
    "ground_truth_captions = []\n",
    "\n",
    "exit_after_batch = 1\n",
    "batch = 0\n",
    "\n",
    "# Iterate over the validation DataLoader\n",
    "for images, captions in val_loader:    \n",
    "    # Generate caption for each image in the batch\n",
    "    for i in range(images.size(0)):  # Iterate through each image in the batch\n",
    "        with torch.no_grad():\n",
    "            generated_caption = model.generate_caption(images[i].unsqueeze(0))  # Pass a single image with batch dimension\n",
    "\n",
    "        # Append the generated caption and ground truth captions\n",
    "        generated_captions.append(generated_caption)\n",
    "\n",
    "        # Convert the ground truth list of captions to strings\n",
    "        ground_truth_captions.append([caption.lower() for caption in captions[i]])\n",
    "\n",
    "        # Optionally, print or log the result for the first few samples\n",
    "        if len(generated_captions) <= 5:  # Print first 5 samples\n",
    "            print(f\"Generated Caption: {generated_caption}\")\n",
    "            print(f\"Ground Truth Captions: {captions[i]}\\n\")\n",
    "        \n",
    "    batch += 1\n",
    "    \n",
    "    if batch >= exit_after_batch:\n",
    "        break\n",
    "\n",
    "# After generating all captions, calculate BLEU scores\n",
    "bleu_scores = []\n",
    "for gen_caption, gt_captions in zip(generated_captions, ground_truth_captions):\n",
    "    # Compute BLEU score against all ground truth captions for the image\n",
    "    bleu_score = sentence_bleu([c.split() for c in gt_captions], gen_caption.split())\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# Print average BLEU score over the validation set\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU Score on Validation Set: {average_bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, only 10%, but I only trained for 3 epochs and one epoch took roughly 2h. And the bleu score is not the best metric because it only checks for engram overlaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Alternatives to BLEU for Caption Validation\n",
    "\n",
    "For image captioning, several metrics provide a more comprehensive evaluation than BLEU, as they consider aspects beyond simple n-gram overlap. Here are some alternatives that are often considered better suited to the task:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**\n",
    "\n",
    "- **Description**: METEOR improves upon BLEU by considering **synonyms, stemming,** and **paraphrases**. It evaluates word matches not only for exact n-gram matches but also for related words, which allows it to better handle the diversity of language.\n",
    "- **Advantages**: \n",
    "  - **Semantic Sensitivity**: Recognizes variations in phrasing and synonyms.\n",
    "  - **Higher Correlation with Human Judgments**: Studies have shown METEOR correlates better with human evaluations than BLEU.\n",
    "- **Consideration**: Slower to compute than BLEU, as it requires stemming, synonym checking, and paraphrase evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **CIDEr (Consensus-based Image Description Evaluation)**\n",
    "\n",
    "- **Description**: CIDEr was specifically developed for image captioning tasks. It measures the similarity between generated captions and reference captions using **term frequency-inverse document frequency (TF-IDF)** weighting, which emphasizes relevant and unique content.\n",
    "- **Advantages**:\n",
    "  - **Focus on Salient Information**: By weighting terms based on TF-IDF, CIDEr emphasizes uncommon or unique words that are more likely to describe important aspects of an image.\n",
    "  - **Strong Suitability for Image Captioning**: CIDEr tends to align well with human judgment on captioning tasks because it captures essential descriptive content.\n",
    "- **Consideration**: Typically better suited than BLEU for image captioning, and now one of the most widely accepted metrics in this domain.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **SPICE (Semantic Propositional Image Caption Evaluation)**\n",
    "\n",
    "- **Description**: SPICE evaluates the **semantic content** of captions by parsing both the generated and reference captions into a semantic graph, where it compares objects, attributes, and relationships.\n",
    "- **Advantages**:\n",
    "  - **Focus on Semantic Content**: SPICE directly evaluates whether the generated caption includes the key entities and relationships present in the reference captions.\n",
    "  - **Strong Correlation with Human Judgments**: Particularly strong in assessing high-level content and relations, SPICE correlates well with human evaluations on complex captions.\n",
    "- **Consideration**: Computationally intensive and doesnâ€™t penalize incorrect word order or grammar, so itâ€™s best used alongside other metrics.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "\n",
    "- **Description**: ROUGE is widely used for summarization but is also applicable to image captioning. It measures recall and precision based on **overlapping n-grams** and **longest common subsequence**.\n",
    "- **Advantages**:\n",
    "  - **Recall-Based**: ROUGE scores are often useful in ensuring that key phrases and words are present in the generated caption, making it sensitive to missing important details.\n",
    "  - **Good Complementary Metric**: ROUGE can be used alongside CIDEr or SPICE for a more rounded evaluation.\n",
    "- **Consideration**: ROUGE is less suited for image captioning than CIDEr or SPICE but can still be a useful additional metric.\n",
    "\n",
    "---\n",
    "\n",
    "### Recommended Combination for Image Captioning\n",
    "\n",
    "In practice, itâ€™s often best to use **multiple metrics** together to get a balanced view of caption quality. For image captioning tasks, a combination like **CIDEr, SPICE,** and **METEOR** generally provides a robust evaluation:\n",
    "\n",
    "1. **CIDEr** to measure the relevance and uniqueness of content in the captions.\n",
    "2. **SPICE** to evaluate semantic correctness (e.g., objects, attributes, relationships).\n",
    "3. **METEOR** to capture synonyms, stemming, and phrasing variations for a broader evaluation of language quality.\n",
    "\n",
    "This combination gives a comprehensive view of both content accuracy and linguistic quality, aligning well with human judgment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
