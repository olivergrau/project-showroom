{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "  - **LoRA (Low-Rank Adaptation)**: Efficient fine-tuning method that reduces memory and computation requirements by adding lightweight, trainable matrices to the model's weights.\n",
    "\n",
    "* Model: \n",
    "  - **BERT (bert-base-uncased)**: A foundational transformer model, suitable for text classification tasks and compatible with my GPU (laptop 4070).\n",
    "\n",
    "* Evaluation approach: \n",
    "  - **Accuracy and F1-Score**: Evaluate performance using these metrics. Accuracy provides an overall measure of correctness, while the F1-score balances precision and recall, especially useful for imbalanced datasets.\n",
    "  - **Cross-validation**: Perform k-fold cross-validation to ensure robust evaluation and reduce overfitting risks.\n",
    "\n",
    "* Fine-tuning dataset: \n",
    "  - **IMDb Movie Reviews Dataset**: A benchmark dataset for sentiment analysis. It consists of 50,000 movie reviews categorized into \"positive\" and \"negative\" sentiment labels, suitable for sequence classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217b0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define model and tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Here I use the AutoModelForSequenceClassification method to get a model with a classification head (2 labels).\n",
    "# This classification head is used on the [CLS] Token, which represents the entire input sequence.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)  # Binary classification\n",
    "model.to(device)\n",
    "\n",
    "# Load IMDb Dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "encoded_test = test_dataset.map(preprocess_function, batched=True)\n",
    "encoded_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Create DataLoader for evaluation\n",
    "test_dataloader = DataLoader(encoded_test, batch_size=16)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)  # Ensure the key is 'labels'\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            # No need for softmax here because the logits have the same relative values\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            # Alternatively, if you want to use softmax:\n",
    "            # preds = torch.argmax(F.softmax(logits, dim=1), dim=1)\n",
    "\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            true_labels.extend(labels.cpu().tolist())\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, predictions, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc51f041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the pre-trained model on IMDb test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████| 1563/1563 [04:39<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before fine-tuning: 0.4469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "model.to(device)\n",
    "\n",
    "print(\"Evaluating the pre-trained model on IMDb (large) test set...\")\n",
    "accuracy, predictions, true_labels = evaluate(model, test_dataloader)\n",
    "print(f\"Accuracy before fine-tuning: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92010f",
   "metadata": {},
   "source": [
    "We can see that the default accuracy is not very high. Only less 50%. This needs fine-tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size train: 25000\n",
      "Original size test: 25000\n",
      "PEFT model initialized.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the dataset\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "orig_size_train = len(encoded_dataset[\"train\"])\n",
    "orig_size_test = len(encoded_dataset[\"test\"])\n",
    "\n",
    "print(f\"Original size train: {orig_size_train}\")\n",
    "print(f\"Original size test: {orig_size_test}\")\n",
    "\n",
    "subset_train_size = 5000 # Use a small subset for quicker training\n",
    "subset_test_size = 2500 # we only take n test records otherwise training would take forever\n",
    "\n",
    "# Split the dataset for training and evaluation\n",
    "train_dataset = encoded_dataset[\"train\"].shuffle(seed=42).select(range(subset_train_size))\n",
    "eval_dataset = encoded_dataset[\"test\"].select(range(subset_test_size))\n",
    "\n",
    "# Configure LoRA for PEFT\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=8,  # Rank of the low-rank adapters\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],  # Apply LoRA to attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Wrap the base model with PEFT using LoRA\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "print(\"PEFT model initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliver/anaconda3/envs/pytorch_env/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                 | 0/313 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████| 313/313 [02:41<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 157/157 [00:31<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3882, Validation accuracy: 0.8344\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████| 313/313 [02:42<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 157/157 [00:29<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2714, Validation accuracy: 0.8836\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████████████████████████████████████████████████████| 313/313 [02:37<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████| 157/157 [00:30<00:00,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2766, Validation accuracy: 0.8804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "peft_model.to(device)\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(eval_dataset, batch_size=16)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(peft_model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")\n",
    "\n",
    "# Define loss function\n",
    "# Defining a loss function is not needed here because the peft model returns a loss when it gets the labels\n",
    "\n",
    "# Training loop (I do this manually instead of the Trainer because here I have fine control over what is printed out and what is done)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    peft_model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = peft_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss  # Model outputs the loss when labels are provided\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    peft_model.eval()\n",
    "    total_val_loss = 0\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = peft_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(logits, dim=-1) # fim=-1 last dimension of tensor\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    val_accuracy = accuracy_score(true_labels, preds)\n",
    "    print(f\"Validation loss: {avg_val_loss:.4f}, Validation accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model weights saved.\n"
     ]
    }
   ],
   "source": [
    "# Save the PEFT model\n",
    "peft_model.save_pretrained(\"./peft_lora_model\")\n",
    "print(\"PEFT model weights saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the base model and the PEFT model with saved weights\n",
    "base_model_name = \"bert-base-uncased\"\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"./peft_lora_model\")\n",
    "peft_model.to(device)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the saved PEFT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████| 1563/1563 [05:01<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy, predictions, true_labels\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the saved PEFT model...\")\n",
    "accuracy, _, _ = evaluate(peft_model, test_dataloader)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
