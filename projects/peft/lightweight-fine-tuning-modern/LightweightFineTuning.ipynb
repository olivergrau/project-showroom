{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lightweight Fine-Tuning â€” LoRA + QLoRA (IMDb Sentiment)\n",
        "\n",
        "This notebook is a job-ready refresh of your original PEFT demo. It includes:\n",
        "- **Baseline** evaluation of a frozen model\n",
        "- **LoRA** fine-tuning path (rank/configurable)\n",
        "- **Optional QLoRA** path (4-bit base model with `bitsandbytes`)\n",
        "- **Evaluation** with Accuracy and Macro-F1 (+ confusion matrix)\n",
        "- **Save/Reload** adapters and **Merge** for export\n",
        "- **Deployment-ready** inference helpers\n",
        "\n",
        "You can toggle LoRA vs QLoRA via a config flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "Config(base_model='bert-base-uncased', task_name='imdb', max_length=256, sample_train=5000, sample_test=2500, batch_size=16, num_epochs=3, lr=5e-05, lora_r=8, lora_alpha=32, lora_dropout=0.1, use_qlora=False, output_dir='outputs_peft')\n"
          ]
        }
      ],
      "source": [
        "# %% [setup]\n",
        "import os, sys, math, time\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding, TrainingArguments, Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', DEVICE)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    base_model: str = 'bert-base-uncased'  # keep BERT to align with original notebook\n",
        "    task_name: str = 'imdb'\n",
        "    max_length: int = 256\n",
        "    sample_train: Optional[int] = 5000  # None for full\n",
        "    sample_test: Optional[int] = 2500\n",
        "    batch_size: int = 16\n",
        "    num_epochs: int = 3\n",
        "    lr: float = 5e-5\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 32\n",
        "    lora_dropout: float = 0.1\n",
        "    target_modules = ['query','key','value']  # include key as upgrade\n",
        "    use_qlora: bool = False  # toggle here\n",
        "    output_dir: str = 'outputs_peft'\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "print(cfg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Data â€” IMDb\n",
        "We keep the dataset and pre-processing nearly identical to your original notebook for continuity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ae6f81c0ddf4b078b0ce0f665217e31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 2500\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['label', 'input_ids', 'attention_mask'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# %% [data]\n",
        "raw = load_dataset('imdb')\n",
        "if cfg.sample_train:\n",
        "    raw['train'] = raw['train'].select(range(cfg.sample_train))\n",
        "if cfg.sample_test:\n",
        "    raw['test'] = raw['test'].select(range(cfg.sample_test))\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(cfg.base_model, use_fast=True)\n",
        "def tokenize_fn(ex):\n",
        "    return tok(ex['text'], truncation=True, max_length=cfg.max_length)\n",
        "\n",
        "tok_ds = raw.map(tokenize_fn, batched=True)\n",
        "tok_ds = tok_ds.remove_columns([c for c in tok_ds['train'].column_names if c not in ['input_ids','attention_mask','label']])\n",
        "tok_ds.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "\n",
        "collator = DataCollatorWithPadding(tok)\n",
        "num_labels = 2\n",
        "print(tok_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Baseline â€” frozen model evaluation\n",
        "We evaluate the base model without fine-tuning to establish a reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipykernel_262483/3850293427.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  baseline_trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='157' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [157/157 00:15]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgrausoft-net\u001b[0m (\u001b[33mgrausoft-net-it-freelancer-oliver-grau\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/oliver/project-showroom/projects/peft/lightweight-fine-tuning/wandb/run-20250810_113657-df4ldwth</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/grausoft-net-it-freelancer-oliver-grau/huggingface/runs/df4ldwth' target=\"_blank\">outputs_peft/baseline</a></strong> to <a href='https://wandb.ai/grausoft-net-it-freelancer-oliver-grau/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/grausoft-net-it-freelancer-oliver-grau/huggingface' target=\"_blank\">https://wandb.ai/grausoft-net-it-freelancer-oliver-grau/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/grausoft-net-it-freelancer-oliver-grau/huggingface/runs/df4ldwth' target=\"_blank\">https://wandb.ai/grausoft-net-it-freelancer-oliver-grau/huggingface/runs/df4ldwth</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.7940624952316284,\n",
              " 'eval_model_preparation_time': 0.0023,\n",
              " 'eval_accuracy': 0.0948,\n",
              " 'eval_f1_macro': 0.0865911582024114,\n",
              " 'eval_runtime': 16.0773,\n",
              " 'eval_samples_per_second': 155.499,\n",
              " 'eval_steps_per_second': 9.765}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %% [baseline]\n",
        "baseline_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    cfg.base_model, num_labels=num_labels\n",
        ").to(DEVICE)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    return {'accuracy': acc, 'f1_macro': f1}\n",
        "\n",
        "baseline_args = TrainingArguments(\n",
        "    output_dir=os.path.join(cfg.output_dir, 'baseline'),\n",
        "    per_device_eval_batch_size=cfg.batch_size,\n",
        "    do_train=False,\n",
        "    do_eval=True,\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "baseline_trainer = Trainer(\n",
        "    model=baseline_model,\n",
        "    args=baseline_args,\n",
        "    eval_dataset=tok_ds['test'],\n",
        "    tokenizer=tok,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "baseline_metrics = baseline_trainer.evaluate()\n",
        "baseline_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) LoRA / QLoRA setup\n",
        "Toggle `cfg.use_qlora` to switch between classic LoRA and QLoRA. QLoRA loads the base model in 4-bit and prepares it for k-bit training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 443,906 || all params: 109,927,684 || trainable%: 0.4038\n"
          ]
        }
      ],
      "source": [
        "# %% [peft-setup]\n",
        "if cfg.use_qlora:\n",
        "    peft_base = AutoModelForSequenceClassification.from_pretrained(\n",
        "        cfg.base_model,\n",
        "        num_labels=num_labels,\n",
        "        load_in_4bit=True,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    peft_base = prepare_model_for_kbit_training(peft_base)\n",
        "else:\n",
        "    peft_base = AutoModelForSequenceClassification.from_pretrained(\n",
        "        cfg.base_model, num_labels=num_labels\n",
        "    )\n",
        "peft_base.to(DEVICE)\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=cfg.lora_r,\n",
        "    lora_alpha=cfg.lora_alpha,\n",
        "    lora_dropout=cfg.lora_dropout,\n",
        "    bias='none',\n",
        "    task_type='SEQ_CLS',\n",
        "    target_modules=cfg.target_modules,\n",
        ")\n",
        "peft_model = get_peft_model(peft_base, lora_cfg)\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train adapters\n",
        "We fine-tune only the LoRA parameters. Metrics include Accuracy and Macro-F1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/oliver/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/tmp/ipykernel_262483/840817049.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='939' max='939' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [939/939 04:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.00023359589977189898,\n",
              " 'eval_accuracy': 1.0,\n",
              " 'eval_f1_macro': 1.0,\n",
              " 'eval_runtime': 17.2576,\n",
              " 'eval_samples_per_second': 144.864,\n",
              " 'eval_steps_per_second': 9.097,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %% [train]\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=os.path.join(cfg.output_dir, 'lora' if not cfg.use_qlora else 'qlora'),\n",
        "    num_train_epochs=cfg.num_epochs,\n",
        "    per_device_train_batch_size=cfg.batch_size,\n",
        "    per_device_eval_batch_size=cfg.batch_size,\n",
        "    learning_rate=cfg.lr,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=train_args,\n",
        "    train_dataset=tok_ds['train'],\n",
        "    eval_dataset=tok_ds['test'],  # quick turnaround; swap in a real val set if desired\n",
        "    tokenizer=tok,\n",
        "    data_collator=collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "eval_metrics = trainer.evaluate()\n",
        "eval_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Confusion matrix\n",
        "A quick confusion matrix for additional signal on misclassifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/oliver/anaconda3/envs/pytorch_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:409: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2500]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %% [confusion-matrix]\n",
        "preds = trainer.predict(tok_ds['test'])\n",
        "y_true = preds.label_ids\n",
        "y_pred = preds.predictions.argmax(axis=-1)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Save adapters, reload, and merge for export\n",
        "We save the LoRA adapters, show how to reload them onto the base model, and optionally merge for a single exportable model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved adapters to outputs_peft/adapters_lora\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged model saved to outputs_peft/merged_lora_model\n"
          ]
        }
      ],
      "source": [
        "# %% [save-reload-merge]\n",
        "adapters_dir = os.path.join(cfg.output_dir, 'adapters_lora' if not cfg.use_qlora else 'adapters_qlora')\n",
        "trainer.model.save_pretrained(adapters_dir)\n",
        "tok.save_pretrained(adapters_dir)\n",
        "print('Saved adapters to', adapters_dir)\n",
        "\n",
        "# Reload for inference\n",
        "reload_base = AutoModelForSequenceClassification.from_pretrained(\n",
        "    cfg.base_model, num_labels=num_labels\n",
        ").to(DEVICE)\n",
        "from peft import PeftModel\n",
        "reload_peft = PeftModel.from_pretrained(reload_base, adapters_dir).to(DEVICE)\n",
        "reload_peft.eval()\n",
        "\n",
        "# Optional: merge and save a single safetensors file (only for non-quantized base)\n",
        "if not cfg.use_qlora:\n",
        "    merged = reload_peft.merge_and_unload()\n",
        "    merged_dir = os.path.join(cfg.output_dir, 'merged_lora_model')\n",
        "    merged.save_pretrained(merged_dir)\n",
        "    print('Merged model saved to', merged_dir)\n",
        "else:\n",
        "    print('Merging is disabled for QLoRA since the base is 4-bit quantized.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Inference helper\n",
        "A quick function that runs text â†’ label using the saved adapters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction (0=neg,1=pos): 0\n"
          ]
        }
      ],
      "source": [
        "# %% [inference]\n",
        "def classify_text(text: str, model, tokenizer, max_length: int = 256):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        enc = tokenizer(text, truncation=True, max_length=max_length, return_tensors='pt').to(DEVICE)\n",
        "        out = model(**enc)\n",
        "        pred = out.logits.argmax(dim=-1).item()\n",
        "    return int(pred)\n",
        "\n",
        "sample = \"This movie was surprisingly good and kept me engaged.\"\n",
        "label = classify_text(sample, reload_peft, tok, max_length=cfg.max_length)\n",
        "print('Prediction (0=neg,1=pos):', label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Notes for MLOps / production\n",
        "- Track runs with MLflow or W&B (plug into `Trainer` callbacks).\n",
        "- Keep adapters separate for small artifacts; merge only if you need a single file.\n",
        "- Add a red-team prompt set when moving from classification to instruction SFT.\n",
        "- For on-prem clusters, parameterize hyperparams via env/CLI and store artifacts to shared storage.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
