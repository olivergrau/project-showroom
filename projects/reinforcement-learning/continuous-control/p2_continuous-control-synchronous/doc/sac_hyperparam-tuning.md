# **üìú Soft Actor-Critic (SAC) Hyperparameters Cheat Sheet üöÄ**
This cheat sheet provides a **comprehensive overview** of **SAC hyperparameters** and their effects on **loss curves, alpha, log probabilities, actor loss, critic loss, and Q-values**.

---

## **1Ô∏è‚É£ Learning Rates**
| **Hyperparameter** | **Effect** | **Observations in Metrics** | **Fix if Wrong** |
|-------------------|-----------|-----------------------------|-----------------|
| **`lr_actor`** *(Actor Learning Rate)* | Controls how fast the policy (actor) updates. | **Too high** ‚Üí Actor loss unstable. <br> **Too low** ‚Üí Slow convergence. | **If unstable:** Lower it (e.g., `1e-3 ‚Üí 3e-4`). <br> **If learning is too slow:** Increase slightly. |
| **`lr_critic`** *(Critic Learning Rate)* | Affects how quickly the critic learns Q-values. | **Too high** ‚Üí Q-values oscillate, loss unstable. <br> **Too low** ‚Üí Q-values increase slowly, critic overfits. | If **critic loss is unstable**, lower it (`1e-3 ‚Üí 3e-4`). |
| **`lr_alpha`** *(Temperature Learning Rate)* | Controls how fast entropy (Œ±) adapts. | **Too high** ‚Üí Alpha changes too quickly, unstable log prob. <br> **Too low** ‚Üí Alpha updates too slowly. | Lower to `1e-4` if Œ± is changing too fast. |

---

## **2Ô∏è‚É£ Entropy & Exploration (Œ± - Temperature Parameter)**
| **Hyperparameter** | **Effect** | **Observations in Metrics** | **Fix if Wrong** |
|-------------------|-----------|-----------------------------|-----------------|
| **`target_entropy`** | Encourages exploration. Set to `-action_dim` by default. | **If too high** ‚Üí The agent explores too much, slow convergence. <br> **If too low** ‚Üí The agent becomes deterministic too quickly. | Try `-0.5 * action_dim` to reduce excessive randomness. |
| **`alpha` (if fixed)** | Controls trade-off between exploration & exploitation. | **High Œ±** ‚Üí More exploration, higher log_probs. <br> **Low Œ±** ‚Üí Less exploration, overfits to exploitation. | If **log_probs are too high**, lower Œ±. If **eval rewards are poor**, increase Œ±. |
| **`log_alpha`** *(learned Œ± parameter)* | If Œ± is learned, `log_alpha` determines its update. | **If decreasing too much** ‚Üí Check `target_entropy`. <br> **If increasing too much** ‚Üí Training might be too stochastic. | Clamp `log_alpha` to prevent extreme changes. |

üîπ **How it affects metrics:**
- **Increasing Œ±** ‚Üí **Higher log_probs**, encourages exploration.
- **Decreasing Œ±** ‚Üí **Lower log_probs**, reduces randomness.
- **Wrong Œ±** ‚Üí Actor loss oscillates, critic loss diverges.

---

## **3Ô∏è‚É£ Critic & Q-Value Stability**
| **Hyperparameter** | **Effect** | **Observations in Metrics** | **Fix if Wrong** |
|-------------------|-----------|-----------------------------|-----------------|
| **`gamma` (Discount Factor)** | Balances short-term & long-term rewards. | **High (0.99)** ‚Üí Stable learning but slow updates. <br> **Low (0.9)** ‚Üí Faster updates but less stability. | Use `0.99` unless rewards are highly delayed. |
| **`tau` (Soft Update Rate)** | Controls how quickly target Q-networks update. | **High (0.01)** ‚Üí Target Q-values update fast, might diverge. <br> **Low (0.005)** ‚Üí Smoother learning, more stability. | Try `0.005` for stability, `0.01` for faster adaptation. |
| **`critic_loss` (MSE Loss on Q-values)** | Indicates how well Q-values approximate true returns. | **If loss is too high** ‚Üí Q-values might be overestimated. <br> **If loss is too low** ‚Üí Critic might not generalize well. | Check reward scaling and batch size. |

üîπ **How it affects metrics:**
- **Q-values too high?** ‚Üí Reduce `lr_critic`, adjust reward scaling.
- **Q-values too low?** ‚Üí Increase `lr_critic`, improve critic training.
- **Critic loss unstable?** ‚Üí Adjust `tau`, lower learning rate.

---

## **4Ô∏è‚É£ Training Efficiency**
| **Hyperparameter** | **Effect** | **Observations in Metrics** | **Fix if Wrong** |
|-------------------|-----------|-----------------------------|-----------------|
| **`batch_size`** | Number of samples per update. | **Small batch (128)** ‚Üí Noisy updates. <br> **Large batch (512)** ‚Üí Smoother training, slower updates. | Use `256-512` for stability. |
| **`replay_buffer_size`** | Stores past transitions for off-policy training. | **Too small** ‚Üí Overfitting, poor generalization. <br> **Too large** ‚Üí Old transitions hurt learning. | Keep `100,000 - 1,000,000` for SAC. |
| **`env_steps_per_update`** | How often updates occur relative to env steps. | **If too high** ‚Üí Delayed updates, policy lags behind. <br> **If too low** ‚Üí Wastes computation, slow training. | Use `50` (common for SAC). |
| **`updates_per_block`** | Number of updates per training step. | **If too high** ‚Üí Overfitting, policy updates too fast. <br> **If too low** ‚Üí Training is too slow. | Use `10-20` per update cycle. |

üîπ **How it affects metrics:**
- **If training is too slow** ‚Üí Increase `updates_per_block`, `batch_size`.
- **If overfitting occurs** ‚Üí Reduce `updates_per_block`, use `target_entropy`.

---

## **5Ô∏è‚É£ Actor-Critic Synchronization**
| **Hyperparameter** | **Effect** | **Observations in Metrics** | **Fix if Wrong** |
|-------------------|-----------|-----------------------------|-----------------|
| **`soft_update()` (`tau`)** | Controls the target Q-network updates. | **If too low** ‚Üí Q-values lag behind, slow training. <br> **If too high** ‚Üí Unstable Q-values. | `tau = 0.005` is standard. |
| **`gradient_clipping`** | Prevents exploding gradients. | **If too low** ‚Üí Gradients explode, loss spikes. <br> **If too high** ‚Üí Training slows down. | Clip at `5.0` if needed. |

üîπ **How it affects metrics:**
- **Diverging Q-values?** ‚Üí Reduce `tau`, clip gradients.
- **Slow Q-updates?** ‚Üí Increase `tau`.

---

## **6Ô∏è‚É£ Reward Scaling**
| **Hyperparameter** | **Effect** | **Observations in Metrics** | **Fix if Wrong** |
|-------------------|-----------|-----------------------------|-----------------|
| **`reward_scaling_factor`** | Scales rewards before training. | **If too high** ‚Üí Exploding Q-values, high critic loss. <br> **If too low** ‚Üí Small updates, slow learning. | Use `1-10` for SAC. |
| **`normalize_rewards`** | Ensures consistent reward values. | **If not normalized** ‚Üí Q-values might be inconsistent. | Normalize rewards if they vary significantly. |

üîπ **How it affects metrics:**
- **Q-values too high?** ‚Üí Reduce `reward_scaling_factor`.
- **Q-values too low?** ‚Üí Increase `reward_scaling_factor`.

---

## **üîë Final SAC Hyperparameter Recommendations**
| **Hyperparameter** | **Recommended Value** |
|-------------------|----------------------|
| **Actor LR (`lr_actor`)** | `3e-4` |
| **Critic LR (`lr_critic`)** | `3e-4` |
| **Alpha LR (`lr_alpha`)** | `1e-4` |
| **Target Entropy (`target_entropy`)** | `-action_dim` or `-0.5 * action_dim` |
| **Tau (`tau`)** | `0.005` |
| **Gamma (`gamma`)** | `0.99` |
| **Batch Size (`batch_size`)** | `256-512` |
| **Replay Buffer Size** | `100,000 - 1,000,000` |
| **Updates per Block** | `10-20` |
| **Env Steps per Update** | `50` |

---

## **üöÄ Summary**
- **Too much exploration?** ‚Üí Lower `target_entropy`, reduce `alpha`.
- **Unstable Q-values?** ‚Üí Reduce `tau`, lower `lr_critic`.
- **Slow learning?** ‚Üí Increase `updates_per_block`, batch size.
- **Poor evaluation performance?** ‚Üí Train with deterministic actions sometimes.