{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Monkey patch missing attributes for newer numpy versions\n",
    "if not hasattr(np, \"float_\"):\n",
    "    np.float_ = np.float64\n",
    "    \n",
    "if not hasattr(np, \"int_\"):\n",
    "    np.int_ = np.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/oliver/project-showroom/projects/reinforcement-learning/continuous-control/p2_continuous-control-synchronous/Reacher_Linux_Single/Reacher.x86_64\n",
      "Mono path[0] = '/home/oliver/project-showroom/projects/reinforcement-learning/continuous-control/p2_continuous-control-synchronous/Reacher_Linux_Single/Reacher_Data/Managed'\n",
      "Mono config path = '/home/oliver/project-showroom/projects/reinforcement-learning/continuous-control/p2_continuous-control-synchronous/Reacher_Linux_Single/Reacher_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "\tScreenSelector.so\n",
      "Logging to /home/oliver/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux_Single/Reacher.x86_64', no_graphics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "(20, 33)\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print(states.shape)\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_moving_observation_slice(env, brain_name, num_agents=20, action_size=4, steps=100):\n",
    "    print(\"ðŸ” Running motion scan...\")\n",
    "    obs_log = []\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "\n",
    "    for i in range(steps):\n",
    "        actions = np.random.uniform(-1, 1, (num_agents, action_size))\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        obs_log.append(state[0])  # only track agent 0\n",
    "\n",
    "    obs_array = np.array(obs_log)  # shape: (steps, obs_dim)\n",
    "\n",
    "    # Compute std per dimension\n",
    "    std_per_dim = np.std(obs_array, axis=0)\n",
    "    print(f\"ðŸ“ Observation vector dim = {obs_array.shape[1]}\")\n",
    "    \n",
    "    # Look for most moving 3D slices\n",
    "    for i in range(0, len(std_per_dim) - 2):\n",
    "        slice_std = np.sum(std_per_dim[i:i+3])\n",
    "        if slice_std > 1e-3:  # filter out noise\n",
    "            print(f\"Candidate slice [{i}:{i+3}] has std sum {slice_std:.4f} â†’ stds = {std_per_dim[i:i+3]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_moving_observation_slice(env, brain_name, num_agents=20, action_size=4, steps=1000):\n",
    "    \"\"\"\n",
    "    Runs the environment for a given number of steps, collects observations for agent 0,\n",
    "    and reports on the movement (std, min, max) of slices 29-31 (hand position).\n",
    "    \"\"\"\n",
    "    print(\"ðŸ” Running motion scan for hand-position slice [29:32]...\")\n",
    "    obs_log = []\n",
    "\n",
    "    # Reset environment and get initial observation\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations  # shape: (num_agents, obs_dim)\n",
    "\n",
    "    for i in range(steps):\n",
    "        # Random actions to explore\n",
    "        actions = np.random.uniform(-1, 1, (num_agents, action_size))\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        obs_log.append(state[0])  # track only agent 0's observation\n",
    "\n",
    "    obs_array = np.array(obs_log)  # shape: (steps, obs_dim)\n",
    "\n",
    "    # Compute std, min, max for the hand-position slice\n",
    "    hand_slice = obs_array[:, 29:32]  # dims 29, 30, 31\n",
    "    std_hand = np.std(hand_slice, axis=0)\n",
    "    min_hand = np.min(hand_slice, axis=0)\n",
    "    max_hand = np.max(hand_slice, axis=0)\n",
    "\n",
    "    print(f\"â–¶ Hand-position slice [29:32] stats over {steps} steps:\")\n",
    "    print(f\"  Std per dim: {std_hand}\")\n",
    "    print(f\"  Min per dim: {min_hand}\")\n",
    "    print(f\"  Max per dim: {max_hand}\")\n",
    "\n",
    "    if np.allclose(std_hand, 0.0, atol=1e-6):\n",
    "        print(\"âš ï¸ Indices 29-31 appear fixed (no change over time).\")\n",
    "    else:\n",
    "        print(\"âœ… Indices 29-31 vary over time (hand position is updating).\")\n",
    "    \n",
    "    return std_hand, min_hand, max_hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running motion scan for hand-position slice [29:32]...\n",
      "â–¶ Hand-position slice [29:32] stats over 1000 steps:\n",
      "  Std per dim: [0. 0. 0.]\n",
      "  Min per dim: [0. 1. 0.]\n",
      "  Max per dim: [0. 1. 0.]\n",
      "âš ï¸ Indices 29-31 appear fixed (no change over time).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0.]), array([0., 1., 0.]), array([0., 1., 0.]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"Reacher_Linux/Reacher.x86_64\", no_graphics=False)\n",
    "brain_name = env.brain_names[0]\n",
    "find_moving_observation_slice(env, brain_name, steps=1000, num_agents=1, action_size=4)\n",
    "#env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env_info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(train_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[brain_name]     \u001b[38;5;66;03m# reset the environment    \u001b[39;00m\n\u001b[1;32m      2\u001b[0m states \u001b[38;5;241m=\u001b[39m env_info\u001b[38;5;241m.\u001b[39mvector_observations                 \u001b[38;5;66;03m# get the current state (for each agent)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mnum_agents\u001b[49m)                         \u001b[38;5;66;03m# initialize the score (for each agent)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m initial_hand_pos \u001b[38;5;241m=\u001b[39m states[:, \u001b[38;5;241m29\u001b[39m:\u001b[38;5;241m32\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()            \u001b[38;5;66;03m# save initial hand positions\u001b[39;00m\n\u001b[1;32m      6\u001b[0m max_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m                                        \u001b[38;5;66;03m# track the max movement across any agent\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'num_agents' is not defined"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                 # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                         # initialize the score (for each agent)\n",
    "\n",
    "initial_hand_pos = states[:, 29:32].copy()            # save initial hand positions\n",
    "max_delta = 0.0                                        # track the max movement across any agent\n",
    "\n",
    "step_count = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size)  # select random actions\n",
    "    actions = np.clip(actions, -1, 1)                    # clip to [-1, 1]\n",
    "    \n",
    "    env_info = env.step(actions)[brain_name]            # step the environment\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    scores += rewards\n",
    "    states = next_states\n",
    "    \n",
    "    hand_pos = states[:, 29:32]\n",
    "    step_delta = np.linalg.norm(hand_pos - initial_hand_pos, axis=1)\n",
    "    max_delta = max(max_delta, np.max(step_delta))\n",
    "    \n",
    "    step_count += 1\n",
    "    if step_count % 10 == 0:\n",
    "        print(f\"Step {step_count}: max hand movement from start: {np.max(step_delta):.4f}\")\n",
    "        print(\"Full obs (agent 0):\", states[0])\n",
    "\n",
    "    if np.any(dones):\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal score (averaged over agents): {np.mean(scores):.2f}\")\n",
    "print(f\"Max hand movement during episode: {max_delta:.4f}\")\n",
    "\n",
    "if max_delta < 1e-3:\n",
    "    print(\"âš ï¸  WARNING: Hand position did not change. Unity environment may be ignoring actions.\")\n",
    "else:\n",
    "    print(\"âœ… Hand movement detected. Unity responds to actions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Architecture Decisions for TD3 Training with Unity ML-Agents**\n",
    "\n",
    "## **1ï¸âƒ£ Use Replay Buffer in Its Own Process**\n",
    "### âœ… Justification:\n",
    "- **Avoids race conditions** when both the training and data collection processes interact with the buffer.\n",
    "- **Prevents memory contention** by isolating replay storage operations from CPU/GPU workloads.\n",
    "- **Decouples storage from compute-intensive tasks**, making data access smoother.\n",
    "\n",
    "---\n",
    "\n",
    "## **2ï¸âƒ£ Replay Buffer as the Bridge Between Training and Data Collection**\n",
    "### âœ… Justification:\n",
    "- **Producer-Consumer Model**: The **data collection process (Unity environment)** produces experience data, while the **training process** consumes it for learning.\n",
    "- **Ensures non-blocking behavior**: Training can proceed independently while new data is collected.\n",
    "- **Avoids excessive synchronization overhead**, allowing each process to run at its own pace.\n",
    "\n",
    "---\n",
    "\n",
    "## **3ï¸âƒ£ Three Separate Processes for Efficiency**\n",
    "### âœ… Justification:\n",
    "1. **Replay Buffer Process**:\n",
    "   - Manages stored experience and handles sampling/insertion requests efficiently.\n",
    "   - Runs in a **dedicated process** to prevent contention with the training loop.\n",
    "\n",
    "2. **Training Process**:\n",
    "   - Fetches batches from the replay buffer asynchronously and updates the TD3 neural networks.\n",
    "   - Utilizes the **GPU fully without waiting** for new experience data.\n",
    "\n",
    "3. **Data Collection Process (Unity Environment)**:\n",
    "   - Steps the environment in parallel for **20 agents**, collecting `(state, action, reward, next_state, done)` tuples.\n",
    "   - Pushes data to the replay buffer **without blocking training**.\n",
    "   - Offloads simulation work to the **CPU**, allowing for efficient resource utilization.\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸŒŸ Summary**\n",
    "- **Replay Buffer runs independently** to mediate between training and data collection.\n",
    "- **Training and data collection processes operate in parallel**, preventing bottlenecks.\n",
    "- **Each process is optimized for its specific task**, ensuring full utilization of CPU & GPU resources.\n",
    "\n",
    "This architecture balances **parallelism, efficiency, and stability**, leading to **faster training times** while keeping the system modular and scalable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also decided to use the ReplayBuffer implementation from https://github.com/ShangtongZhang/DeepRL/. His code is superior modularized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of my decisions to utilize a producer-consumer model for the ReplayBuffer I have to place all the code in python files that are directly executed in a terminal. A jupyter notebook is not suitable for parallel / async code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity_env",
   "language": "python",
   "name": "unity_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
