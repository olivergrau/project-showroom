{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-left: 4px solid #d9534f; padding: 0.8em; background-color: #fbeaea; margin-bottom: 1em;\">\n",
    "  <strong>‚ö†Ô∏è Warning:</strong> <br><br>It is of utmost importance that you clearly understand the tennis environment you deal with. Here is an introduction to set you up. (I was really confused in the beginning...)\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéæ What Kind of Environment Is Unity Tennis?\n",
    "\n",
    "Unity‚Äôs **Tennis environment**, as provided in ML-Agents, is a **two-agent, competitive environment** ‚Äî but it‚Äôs also:\n",
    "\n",
    "* **Zero-sum**: one agent‚Äôs success is the other‚Äôs failure\n",
    "* Designed to **learn adversarial strategies** (i.e., win against opponent)\n",
    "\n",
    "However, **the way it‚Äôs trained here** is different:\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Two Possible Training Objectives\n",
    "\n",
    "| Objective          | Description                        | Common in                     | Reward Style                 |\n",
    "| ------------------ | ---------------------------------- | ----------------------------- | ---------------------------- |\n",
    "| ü•ä **Competitive** | Agent A tries to beat Agent B      | Self-play, real tennis, games | Win: +1, Lose: -1            |\n",
    "| üèì **Cooperative** | Agents try to keep the rally going | Skill learning, paddle-ball   | Each successful bounce: +0.1 |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ What Does Unity Tennis Actually Do?\n",
    "\n",
    "In our **Tennis environment**:\n",
    "\n",
    "* **Each agent gets +0.1** for a successful hit / rally\n",
    "* **No negative reward** for failing or beating the opponent\n",
    "* The environment resets **when the ball is lost by either agent**\n",
    "* So both agents benefit from **keeping the ball in play longer**\n",
    "\n",
    "‚û°Ô∏è **It‚Äôs a reward-sharing, cooperative task disguised as tennis.**\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ So what‚Äôs the true training goal?\n",
    "\n",
    "> Train **both agents** to collaboratively **maximize the number of time steps the ball stays in play**.\n",
    "\n",
    "#### Reward Function (per agent):\n",
    "\n",
    "$$\n",
    "r_i = \n",
    "\\begin{cases}\n",
    "+0.1 & \\text{if the rally continues successfully} \\\\\n",
    "-0.01 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Goal:\n",
    "\n",
    "$$\n",
    "\\max \\sum_t r_1^t + r_2^t\n",
    "$$\n",
    "\n",
    "‚Üí Maximize **joint cumulative reward** by **collaborative coordination**, not winning.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§Ø Why the Confusion?\n",
    "\n",
    "* The **name \"Tennis\"** suggests competition.\n",
    "* It's a **2-agent setup**, which is typical of adversarial games.\n",
    "* But the **reward structure promotes cooperation** ‚Äî it‚Äôs a **coordination task**, not a match.\n",
    "\n",
    "In reinforcement learning, the **reward function defines the goal**, not the scenario. That‚Äôs why you are 100% right to question it.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† How should this affect your implementation?\n",
    "\n",
    "1. **Treat it as a cooperative game** during training:\n",
    "\n",
    "   * It‚Äôs okay for both agents to **share parameters** or use similar actor structures.\n",
    "   * Prioritize **stability and coordination**, not competition.\n",
    "\n",
    "2. **Measure performance** by:\n",
    "\n",
    "   * Steps per episode\n",
    "   * Total reward per episode (both agents combined)\n",
    "   * Length of rally\n",
    "\n",
    "3. **Optional**: Of course it is possible to **modify** the environment later to make it truly adversarial by adjusting rewards (e.g., +1 for winner, -1 for loser).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Mental Model\n",
    "\n",
    "> Imagine it as a paddle game: **keep the ball alive**.\n",
    "> It‚Äôs not tennis, it‚Äôs a **longest-rally challenge**.\n",
    "> You are teaching agents not to win ‚Äî but **not to lose**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### A more detailed Explanation to cement your Intuition\n",
    "\n",
    "Let‚Äôs break it down:\n",
    "\n",
    "| Event                                | Reward | Who gets it     | Goal/Interpretation            |\n",
    "| ------------------------------------ | ------ | --------------- | ------------------------------ |\n",
    "| Agent hits the ball over net         | +0.1   | That agent only | Encourages **successful play** |\n",
    "| Agent lets the ball drop or hits out | -0.01  | That agent only | Penalizes **ending the rally** |\n",
    "\n",
    "#### üîÅ No win/lose dynamics:\n",
    "\n",
    "* **Each agent is rewarded or penalized based on its own behavior.**\n",
    "* There is **no opponent-based scoring** (e.g., Agent A +1 if Agent B fails ‚Äî this would be zero-sum).\n",
    "* The **rewards are not coupled** ‚Äî one agent can do well while the other performs poorly or even fails.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ù So What Is It?\n",
    "\n",
    "#### ‚úÖ It is a **cooperative**, **partially observable**, **continuous-control** multi-agent environment.\n",
    "\n",
    "* Agents share the **mutual incentive** to **keep the ball in play**.\n",
    "* They are not punished or rewarded based on each other‚Äôs success or failure.\n",
    "* **Maximizing episode length (rally)** is the emergent goal.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary\n",
    "\n",
    "| Feature                 | True for Unity Tennis |\n",
    "| ----------------------- | --------------------- |\n",
    "| Multi-Agent?            | ‚úÖ Yes                 |\n",
    "| Continuous control?     | ‚úÖ Yes                 |\n",
    "| Partially observable?   | ‚úÖ Yes (local obs)     |\n",
    "| Competitive (zero-sum)? | ‚ùå **No**              |\n",
    "| Cooperative dynamics?   | ‚úÖ Yes                 |\n",
    "| Coordination required?  | ‚úÖ Yes                 |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch for changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Monkey patch missing attributes for newer numpy versions\n",
    "if not hasattr(np, \"float_\"):\n",
    "    np.float_ = np.float64\n",
    "    \n",
    "if not hasattr(np, \"int_\"):\n",
    "    np.int_ = np.int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis.x86_64\n",
      "Mono path[0] = '/home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis_Data/Managed'\n",
      "Mono config path = '/home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "\tScreenSelector.so\n",
      "Logging to /home/oliver/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "Whait! What? The Udacity docs are saying something about 8 observations per step, why do we get here 24 observations per agent and per step?</summary>\n",
    "\n",
    "Excellent question ‚Äî the short answer is:\n",
    "\n",
    "> **Unity returns 24 elements (3 √ó 8)** because **observation stacking is a built-in feature** of the ML-Agents engine ‚Äî and it's enabled by default in the environment configuration.\n",
    "\n",
    "This behavior is **intentional**, not arbitrary. Here's why:\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why Unity (ML-Agents) Stacks Observations by Default\n",
    "\n",
    "#### 1. **Frame stacking provides temporal context**\n",
    "\n",
    "In environments like Tennis:\n",
    "\n",
    "* The agent gets **positions and velocities**, but not all dynamics are obvious from a single frame.\n",
    "* Without stacking, the agent might struggle to infer:\n",
    "\n",
    "  * Ball trajectory\n",
    "  * Paddle motion direction\n",
    "  * Speed changes over time\n",
    "\n",
    "With stacked frames:\n",
    "\n",
    "* Even a feedforward (non-recurrent) network gets **short-term memory** baked into the input.\n",
    "\n",
    "This is especially important for:\n",
    "\n",
    "* **Fast-moving objects** (like a bouncing ball)\n",
    "* **Sparse reward signals**\n",
    "* Agents with **no internal memory** (i.e., no RNN or GRU)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Stacking is configurable in the Unity Editor**\n",
    "\n",
    "The ML-Agent training config for the Tennis environment likely contains:\n",
    "\n",
    "```yaml\n",
    "vector_observation_stacks: 3\n",
    "```\n",
    "\n",
    "So instead of returning:\n",
    "\n",
    "```python\n",
    "obs_t = [paddle_pos_t, ball_pos_t, paddle_vel_t, ...]  # 8 values\n",
    "```\n",
    "\n",
    "Unity returns:\n",
    "\n",
    "```python\n",
    "obs_t = [obs_{t-2}, obs_{t-1}, obs_t]  # 3 √ó 8 = 24 values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Why Not Just Return 8?\n",
    "\n",
    "They *could* ‚Äî and in fact, you **can change this** by modifying the Unity environment YAML config and recompiling the binary. But that would:\n",
    "\n",
    "* Require you to **manually add recurrence or past frames** to your model\n",
    "* Reduce **generalization and stability**, especially in fast games\n",
    "\n",
    "So Unity makes a tradeoff:\n",
    "‚úÖ Slightly larger obs\n",
    "‚úÖ Better learning without extra effort\n",
    "‚ùå You need to know that it‚Äôs happening üòâ\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Question                      | Answer                                                       |\n",
    "| ----------------------------- | ------------------------------------------------------------ |\n",
    "| Why 24 elements instead of 8? | Because Unity stacks 3 √ó 8 obs frames for temporal context   |\n",
    "| Does env step = 3 steps?      | ‚ùå No ‚Äî stacking is in observation, not simulation timestep   |\n",
    "| Can I change it?              | ‚úÖ Yes ‚Äî via Unity ML config (`vector_observation_stacks: 1`) |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Number of steps in episode 1: 15\n",
      "Score (max over agents) from episode 2: 0.0\n",
      "Number of steps in episode 2: 13\n",
      "Score (max over agents) from episode 3: 0.0\n",
      "Number of steps in episode 3: 15\n",
      "Score (max over agents) from episode 4: 0.0\n",
      "Number of steps in episode 4: 14\n",
      "Score (max over agents) from episode 5: 0.09000000171363354\n",
      "Number of steps in episode 5: 31\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    count_steps = 0\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "\n",
    "        count_steps += 1\n",
    "        if np.all(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))\n",
    "    print('Number of steps in episode {}: {}'.format(i, count_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating the Experience Buffer (Replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.shape:       torch.Size([4, 2, 24])\n",
      "actions.shape:      torch.Size([4, 2, 2])\n",
      "rewards.shape:      torch.Size([4, 2])\n",
      "next_states.shape:  torch.Size([4, 2, 24])\n",
      "dones.shape:        torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from codebase.experience.replay import MultiAgentReplayBuffer\n",
    "\n",
    "buffer = MultiAgentReplayBuffer(\n",
    "    num_agents=2,\n",
    "    obs_size=24,           # e.g., 24-dim observation vector per agent\n",
    "    action_size=2,         # e.g., 2 continuous actions per agent\n",
    "    buffer_size=100000,\n",
    "    batch_size=4,\n",
    "    seed=42,\n",
    "    device=torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# One time step, 2 agents\n",
    "states = [np.random.rand(24), np.random.rand(24)]       # List of 2 arrays, shape (24,)\n",
    "actions = [np.random.rand(2), np.random.rand(2)]         # List of 2 arrays, shape (2,)\n",
    "rewards = [0.5, -0.5]                                    # One reward per agent\n",
    "next_states = [np.random.rand(24), np.random.rand(24)]\n",
    "dones = [False, False]\n",
    "\n",
    "buffer.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "# Add more samples to exceed batch size\n",
    "for _ in range(10):\n",
    "    buffer.add(\n",
    "        [np.random.rand(24), np.random.rand(24)],               # states (8 variables x 3 dimensions)\n",
    "        [np.random.rand(2), np.random.rand(2)],                 # actions (2 actions per agent)\n",
    "        [np.random.uniform(-1, 1), np.random.uniform(-1, 1)],   # rewards\n",
    "        [np.random.rand(24), np.random.rand(24)],               # next_states\n",
    "    [False, False]                                              # dones\n",
    "    )\n",
    "\n",
    "# Sample a batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample()\n",
    "\n",
    "print(\"states.shape:      \", states.shape)       # (batch_size, 2, 24)\n",
    "print(\"actions.shape:     \", actions.shape)      # (batch_size, 2, 2)\n",
    "print(\"rewards.shape:     \", rewards.shape)      # (batch_size, 2)\n",
    "print(\"next_states.shape: \", next_states.shape)  # (batch_size, 2, 24)\n",
    "print(\"dones.shape:       \", dones.shape)        # (batch_size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Do we need a thread safe replay buffer?\n",
    "\n",
    "> **In our setup (Unity Tennis with 2 agents and a single-threaded trainer loop), the replay buffer does *not* need to be thread-safe.**\n",
    "> But if we later move to **parallel environments** or **asynchronous agents**, then **yes**, we‚Äôll need to make the buffer thread-safe.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Why It's Fine *Now*\n",
    "\n",
    "In the typical **Unity Tennis + MADDPG** training loop:\n",
    "\n",
    "* We step the environment **synchronously**: both agents act, and we get the joint experience in one call.\n",
    "* Then we **call `.add(...)` once** per step with this joint experience.\n",
    "* Similarly, `.sample()` is called only when we're ready to learn ‚Äî also inside a controlled loop.\n",
    "\n",
    "So:\n",
    "\n",
    "* All buffer interactions happen in the same thread ‚Üí **no race conditions**.\n",
    "* No async samplers or environment workers (unlike setups like `Ray RLlib` or `SEED RL`).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ When We *Do* Need Thread-Safety\n",
    "\n",
    "If we later extend your system to:\n",
    "\n",
    "* **Use multiple environments** (e.g. Unity envs in parallel workers)\n",
    "* **Have multiple trainers/workers pushing to the same buffer**\n",
    "* **Sample while adding (e.g. for prioritized replay with async training)**\n",
    "\n",
    "Then we'll need to:\n",
    "\n",
    "* Wrap the buffer with **a lock (e.g., `threading.Lock` in Python)**\n",
    "* Or use **concurrent queues** (e.g., `queue.Queue` or `multiprocessing.Queue`)\n",
    "* Or better: move to a framework like **ReplayBuffer from RLlib or TorchRL** if going big.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Actor-Critic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéæ Unity Tennis Environment Specs\n",
    "\n",
    "Each **agent** observes:\n",
    "\n",
    "* **State vector**: 24 floats\n",
    "  Each **agent** outputs:\n",
    "* **Action vector**: 2 floats (continuous control)\n",
    "\n",
    "In **MADDPG**, each agent has:\n",
    "\n",
    "* One **actor**: maps its own observation to action\n",
    "* One **critic**: maps **joint observations + joint actions** to Q-value\n",
    "\n",
    "So for two 2 agents:\n",
    "\n",
    "* Actor input: `obs_dim = 24`, output: `action_dim = 2`\n",
    "* Critic input: `2 * obs_dim + 2 * action_dim = 2 * (24 + 2) = 52`\n",
    "\n",
    "**Attention**: The Critic input is shared for both agents, thus the multiplication by factor 2!\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Design Choices\n",
    "\n",
    "| Component                | Details                              |\n",
    "| ------------------------ | ------------------------------------ |\n",
    "| Hidden sizes             | \\[128, 128]                          |\n",
    "| Activations              | ReLU (Actor uses `tanh` at output)   |\n",
    "| Layer norm or batch norm | Optional, not included yet           |\n",
    "| Weight init              | Small uniform for actor output layer |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codebase.maddpg.net.actor_critic import Actor, Critic\n",
    "\n",
    "actor = Actor(obs_size=24, action_size=2)\n",
    "critic = Critic(full_obs_size=2*24, full_action_size=2*2)\n",
    "\n",
    "obs_batch = torch.rand(10, 24)\n",
    "action_batch = actor(obs_batch)\n",
    "\n",
    "obs_all = torch.rand(10, 48)\n",
    "action_all = torch.rand(10, 4)\n",
    "q_value = critic(obs_all, action_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>CTDE - Centralized Training with Decentralized Execution</summary>\n",
    "\n",
    "> üí• **How can agents learn cooperatively or competitively in a non-stationary multi-agent world, yet still act independently at test time?**\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Problem in Multi-Agent RL\n",
    "\n",
    "In multi-agent reinforcement learning (MARL), each agent learns a policy in an environment where other agents are also learning. This makes the environment:\n",
    "\n",
    "* **Non-stationary** from each agent‚Äôs perspective\n",
    "* Hard for **value functions** (like Q-functions) to converge\n",
    "* Especially unstable if agents rely only on their **local** observations\n",
    "\n",
    "So we want:\n",
    "\n",
    "* Stability during training ‚úÖ\n",
    "* Independence at runtime ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### üí° Solution: Centralized Training, Decentralized Execution (CTDE)\n",
    "\n",
    "#### üîß During training:\n",
    "\n",
    "Each agent can access:\n",
    "\n",
    "* **Global state** (i.e., observations of all agents)\n",
    "* **Actions of all agents**\n",
    "* **Shared or opponent policies** (useful for computing target actions)\n",
    "* **Joint reward structure**\n",
    "\n",
    "This allows **centralized critics** to be trained:\n",
    "\n",
    "$$\n",
    "Q_i(o_1, o_2, \\dots, a_1, a_2, \\dots)\n",
    "$$\n",
    "\n",
    "Each agent's **critic** can see the whole world.\n",
    "\n",
    "---\n",
    "\n",
    "#### üßÉ During execution (deployment):\n",
    "\n",
    "Each agent acts **independently** using:\n",
    "\n",
    "* Only **its own local observation**\n",
    "* Its **actor**: $a_i = \\mu_{\\theta_i}(o_i)$\n",
    "\n",
    "No centralized info is needed. This makes the policy:\n",
    "\n",
    "* Portable ‚úÖ\n",
    "* Realistic ‚úÖ\n",
    "* Scalable ‚úÖ\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Example in MADDPG (Paper: Lowe et al., 2017)\n",
    "\n",
    "| Phase         | Actor Input     | Critic Input                              |\n",
    "| ------------- | --------------- | ----------------------------------------- |\n",
    "| **Training**  | Local obs $o_i$ | Global obs $o_1, o_2$, actions $a_1, a_2$ |\n",
    "| **Execution** | Local obs $o_i$ | ‚ùå No critic, only actor used              |\n",
    "\n",
    "So in code:\n",
    "\n",
    "```python\n",
    "# Actor uses only its own obs (decentralized)\n",
    "action = self.actor(obs_i)\n",
    "\n",
    "# Critic sees everything (centralized)\n",
    "q_value = self.critic(obs_1, obs_2, action_1, action_2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why It Works\n",
    "\n",
    "* Agents **can exploit additional info** to learn stable value estimates.\n",
    "* But they **don't need that info at test time** ‚Äî they learned how to act well using just local inputs.\n",
    "\n",
    "This removes the chicken-and-egg problem of co-learning agents constantly shifting each other‚Äôs dynamics during training.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Broader Use of CTDE\n",
    "\n",
    "This principle is used in many algorithms:\n",
    "\n",
    "* **MADDPG**: centralized critics, decentralized actors\n",
    "* **QMIX**: centralized mixing network, decentralized policies\n",
    "* **COMA**, **HAPPO**, **MAPPO**, etc.\n",
    "\n",
    "And even in real-world robotics, swarm coordination, autonomous driving, etc., where agents are trained in simulation with more info than they'll have at runtime.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Intuition Summary\n",
    "\n",
    "> *\"During training, let the agents be as smart and informed as they want. But when it‚Äôs time to deploy, they have to go solo.\"*\n",
    "\n",
    "This is **CTDE** in a nutshell ‚Äî a practical tradeoff between **stability** and **realism**.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The DDPGAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Purpose:\n",
    "\n",
    "This class defines the learning logic for a **single agent** using the **DDPG (Deep Deterministic Policy Gradient)** algorithm.\n",
    "\n",
    "### üß± Architecture:\n",
    "\n",
    "* **Actor**: Neural network mapping from local observation ‚Üí continuous action\n",
    "* **Critic**: Neural network estimating Q-values using **global** (joint) state and **joint** actions\n",
    "* **Target Networks**: Smoothed copies of actor and critic to stabilize learning (soft updates)\n",
    "* **Exploration**: Adds **Ornstein-Uhlenbeck (OU)** noise to promote correlated continuous exploration\n",
    "\n",
    "### üîÅ Algorithm (for one agent):\n",
    "\n",
    "1. **Action Selection**:\n",
    "\n",
    "   $$\n",
    "   a_t = \\mu(o_t) + \\text{OU noise}\n",
    "   $$\n",
    "2. **Critic Update**:\n",
    "\n",
    "   $$\n",
    "   L = \\text{MSE}\\left(Q(o_1, o_2, a_1, a_2), r_i + \\gamma Q'(o'_1, o'_2, a'_1, a'_2)\\right)\n",
    "   $$\n",
    "3. **Actor Update**:\n",
    "\n",
    "   $$\n",
    "   J = -Q(o_1, o_2, [a_1, a_2 \\mid a_i = \\mu(o_i)])\n",
    "   $$\n",
    "4. **Target Update**:\n",
    "\n",
    "   $$\n",
    "   \\theta_{\\text{target}} \\leftarrow \\tau \\theta + (1 - \\tau) \\theta_{\\text{target}}\n",
    "   $$\n",
    "\n",
    "### ü§î Why needed?\n",
    "\n",
    "Each agent must independently:\n",
    "\n",
    "* Learn to act based on its **own local view**\n",
    "* Consider the **global impact** of its actions (due to the opponent)\n",
    "  This modular design keeps each agent independent, yet trainable in a multi-agent setting.\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary> How the DDPGAgent works in detail:</summary>\n",
    "\n",
    "## ‚úÖ The Code (for reference)\n",
    "\n",
    "```python\n",
    "# --- Update Actor ---\n",
    "# b) build a full joint-action tensor in a differentiable way\n",
    "A = self.action_size\n",
    "i = self.agent_id\n",
    "start = i * A\n",
    "end   = start + A\n",
    "\n",
    "# actions_all: (B, N*A)\n",
    "actions_all_pred = torch.cat([\n",
    "   actions_all[:, :start],    # other agents before me\n",
    "   pred_action,               # my freshly predicted actions\n",
    "   actions_all[:, end:]       # other agents after me\n",
    "], dim=1)                       # -> (B, N*A)\n",
    "\n",
    "# c) freeze critic so only actor gets gradients\n",
    "for p in self.critic.parameters():\n",
    "   p.requires_grad = False\n",
    "\n",
    "# d) compute actor loss via the critic\n",
    "actor_loss = -self.critic(obs_all, actions_all_pred).mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Goal (What are we doing?)\n",
    "\n",
    "We are updating the **actor network** $\\mu_{\\theta_i}$ of agent $i$ to **improve its policy** by ascending the gradient of the expected return.\n",
    "\n",
    "In policy gradient algorithms, we do:\n",
    "\n",
    "$$\n",
    "\\theta_i \\leftarrow \\theta_i + \\alpha \\nabla_{\\theta_i} J_i\n",
    "$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$\n",
    "J_i = \\mathbb{E}_{o_1, o_2} \\left[ Q_i(o_1, o_2, \\mu_1(o_1), \\mu_2(o_2)) \\right]\n",
    "$$\n",
    "\n",
    "Since we want to **maximize** $J_i$, and PyTorch minimizes by default, we define:\n",
    "\n",
    "$$\n",
    "\\text{actor\\_loss} = -J_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Mapping the Code to the Math\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. **Predict this agent‚Äôs actions** using its own actor:\n",
    "\n",
    "   ```python\n",
    "   pred_action = self.actor(obs_batch)\n",
    "   ```\n",
    "\n",
    "   This gives:\n",
    "\n",
    "   $$\n",
    "   a_i = \\mu_{\\theta_i}(o_i)\n",
    "   $$\n",
    "\n",
    "2. **Rebuild the full joint action vector**:\n",
    "\n",
    "   * Take all other agents‚Äô actions from the buffer (unchanged)\n",
    "   * Replace **this agent‚Äôs** action with its new prediction\n",
    "\n",
    "   ```python\n",
    "   # b) build a full joint-action tensor in a differentiable way\n",
    "   A = self.action_size\n",
    "   i = self.agent_id\n",
    "   start = i * A\n",
    "   end   = start + A\n",
    "\n",
    "   # actions_all: (B, N*A)\n",
    "   actions_all_pred = torch.cat([\n",
    "      actions_all[:, :start],    # other agents before me\n",
    "      pred_action,               # my freshly predicted actions\n",
    "      actions_all[:, end:]       # other agents after me\n",
    "   ], dim=1)                       # -> (B, N*A)\n",
    "   ```\n",
    "\n",
    "   This forms:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{a}_{\\text{all}} = [a_1, a_2] \\quad \\text{with } a_i = \\mu_{\\theta_i}(o_i)\n",
    "   $$\n",
    "\n",
    "3. **Evaluate the Q-value** using the current critic:\n",
    "\n",
    "   ```python\n",
    "   actor_loss = -self.critic(obs_all, actions_all_pred).mean()\n",
    "   ```\n",
    "\n",
    "   This is:\n",
    "\n",
    "   $$\n",
    "   -Q_i(o_1, o_2, a_1, a_2) \\quad \\text{(maximize this by minimizing its negative)}\n",
    "   $$\n",
    "\n",
    "   So this actor is trying to **learn parameters** that output actions which **maximize Q-value** according to its own critic.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Intuition: What‚Äôs Really Happening?\n",
    "\n",
    "> *\"Given what the other agent is doing (as stored in this batch), how can I change **my** policy so that the critic says we'll get a higher reward?\"*\n",
    "\n",
    "You're freezing the other agent‚Äôs behavior (as if it‚Äôs non-learning) and just nudging **your own policy** to do better ‚Äî **according to your own value estimate**.\n",
    "\n",
    "This is a **local update** in a multi-agent world.\n",
    "\n",
    "* It‚Äôs centralized training (you can see joint state/action),\n",
    "* but decentralized execution (you learn only from your own obs and use your own actor).\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Visual Breakdown\n",
    "\n",
    "### Input:\n",
    "\n",
    "* $o_1, o_2$: joint obs\n",
    "* $a_1, a_2$: actions from the replay buffer\n",
    "* You‚Äôre training agent $i$\n",
    "\n",
    "### During actor update:\n",
    "\n",
    "* Replace $a_i \\leftarrow \\mu_{\\theta_i}(o_i)$\n",
    "* Evaluate $Q_i(o_1, o_2, a_1, a_2)$\n",
    "* Compute gradient w\\.r.t. $\\theta_i$ and apply optimizer step\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Mental Model Summary\n",
    "\n",
    "You‚Äôre asking:\n",
    "\n",
    "> *‚ÄúIf I had taken a different (and hopefully better) action in this past situation, holding the world and my opponent fixed, would my expected long-term return be higher?‚Äù*\n",
    "\n",
    "The **critic gives you that answer**, and the **actor updates its weights** so that in the future, it will **naturally choose** actions that lead to higher critic scores.\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>A simple Example with numbers:</summary>\n",
    "\n",
    "---\n",
    "\n",
    "## üèì Setup: 2-Agent MADDPG\n",
    "\n",
    "* Each agent observes **3 values** (`obs_size = 3`)\n",
    "* Each agent takes **2 actions** (`action_size = 2`)\n",
    "* Batch size is **2 transitions** (`B = 2`)\n",
    "\n",
    "Let‚Äôs say we‚Äôre inside **Agent 0**'s actor update step.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Given Inputs\n",
    "\n",
    "### `obs_batch` (Agent 0‚Äôs observations from replay buffer):\n",
    "\n",
    "| Step | Observation        |\n",
    "| ---- | ------------------ |\n",
    "| 0    | `[1.0, 0.0, -1.0]` |\n",
    "| 1    | `[0.5, -0.2, 0.3]` |\n",
    "\n",
    "‚Üí Shape: `(2, 3)`\n",
    "\n",
    "---\n",
    "\n",
    "### `actions_all` (Joint actions from buffer: Agent 0 and Agent 1):\n",
    "\n",
    "| Step | Agent 0 Action | Agent 1 Action |\n",
    "| ---- | -------------- | -------------- |\n",
    "| 0    | `[0.2, -0.1]`  | `[0.9, 0.4]`   |\n",
    "| 1    | `[0.5, 0.0]`   | `[1.0, 0.1]`   |\n",
    "\n",
    "‚Üí Shape: `(2, 4)`\n",
    "‚Üí Stored in buffer as:\n",
    "\n",
    "```python\n",
    "actions_all = [\n",
    "  [0.2, -0.1, 0.9, 0.4], # don't forget the batch dimension!!! Hence we have two rows (one row per step)\n",
    "  [0.5,  0.0, 1.0, 0.1]\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Step-by-Step\n",
    "\n",
    "### Step 1: Actor makes new prediction\n",
    "\n",
    "```python\n",
    "pred_action = self.actor(obs_batch)\n",
    "```\n",
    "\n",
    "Let‚Äôs say your actor returns:\n",
    "\n",
    "```python\n",
    "pred_action = [\n",
    "  [0.3, -0.3], # [+0.3, -0.3] ‚Üí x-movement: +0.3 (right), z-movement: -0.3 (lower)\n",
    "  [0.4, 0.1]\n",
    "]\n",
    "```\n",
    "\n",
    "‚Üí Meaning: ‚ÄúBased on my current policy, I would take these actions for those 2 observations.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Clone full joint actions\n",
    "\n",
    "```python\n",
    "actions_all_pred = actions_all.clone()\n",
    "```\n",
    "\n",
    "So `actions_all_pred` is initially:\n",
    "\n",
    "```python\n",
    "[\n",
    "  [0.2, -0.1, 0.9, 0.4],   # old agent0 + agent1 actions\n",
    "  [0.5,  0.0, 1.0, 0.1]\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Replace *my* part (Agent 0) with new actions\n",
    "\n",
    "```python\n",
    "actions_all_pred[:, 0:2] = pred_action\n",
    "```\n",
    "\n",
    "Now we update only **agent 0‚Äôs** portion of each row (cols 0 and 1):\n",
    "\n",
    "```python\n",
    "actions_all_pred = [\n",
    "  [0.3, -0.3, 0.9, 0.4],   # ‚Üê new agent0 action + old agent1 action\n",
    "  [0.4,  0.1, 1.0, 0.1]\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Final Result\n",
    "\n",
    "You now have a **joint action** tensor:\n",
    "\n",
    "| Step | Agent 0 Action (new) | Agent 1 Action (from buffer) |\n",
    "| ---- | -------------------- | ---------------------------- |\n",
    "| 0    | `[0.3, -0.3]`        | `[0.9, 0.4]`                 |\n",
    "| 1    | `[0.4, 0.1]`         | `[1.0, 0.1]`                 |\n",
    "\n",
    "That‚Äôs what you feed into the critic, together with the full joint observations, to compute:\n",
    "\n",
    "```python\n",
    "Q(obs_all, actions_all_pred)\n",
    "```\n",
    "\n",
    "Then the gradient of this Q-value with respect to **your actor's parameters** is used to improve your policy.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Mental Model\n",
    "\n",
    "> \"I take **my own observation**, predict a better action.\n",
    "> Then I **splice** that into the joint action vector\n",
    "> and ask the critic:\n",
    "> ‚ûú *‚ÄúIf I act this way, and the others act as they did, how good would that be?‚Äù*\"\n",
    "\n",
    "This lets each agent *individually* optimize its policy, even though the value is computed jointly.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The MADDPGAgent Coordinator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Purpose:\n",
    "\n",
    "This class **coordinates multiple `DDPGAgent`s**, handles joint learning logic, and enables **centralized training with decentralized execution**.\n",
    "\n",
    "### üß± Architecture:\n",
    "\n",
    "* Maintains a list of 2 `DDPGAgent`s (one per player)\n",
    "* Handles:\n",
    "\n",
    "  * Action collection\n",
    "  * Preparing **joint critic inputs**\n",
    "  * Triggering learning steps for each agent\n",
    "* Passes **global state & joint actions** to each critic\n",
    "\n",
    "### üîÅ Algorithm (for all agents):\n",
    "\n",
    "For each agent $i$:\n",
    "\n",
    "1. **Build joint input** from buffer:\n",
    "\n",
    "   $$\n",
    "   \\text{obs\\_all}, \\text{actions\\_all}, \\text{next\\_obs\\_all}\n",
    "   $$\n",
    "2. **Predict next actions using all target actors**:\n",
    "\n",
    "   $$\n",
    "   \\text{next\\_actions\\_all} = [\\mu_1'(o'_1), \\mu_2'(o'_2)]\n",
    "   $$\n",
    "3. **Train agent $i$** by calling:\n",
    "\n",
    "   ```python\n",
    "   agent[i].learn(...)\n",
    "   ```\n",
    "\n",
    "### ü§î Why needed?\n",
    "\n",
    "This class **decouples training logic** from individual agent mechanics. It ensures:\n",
    "\n",
    "* Each agent gets correct centralized info\n",
    "* Training stays **synchronized** across agents\n",
    "* Codebase is clean and scalable (e.g., for 3+ agents)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. The Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† Purpose:\n",
    "\n",
    "Drives the **outer training loop**:\n",
    "\n",
    "* Interfaces with Unity env\n",
    "* Handles exploration, reward tracking, logging\n",
    "* Calls `MADDPGAgent.step()` with sampled experience\n",
    "\n",
    "### üîÅ Algorithm:\n",
    "\n",
    "For each episode:\n",
    "\n",
    "1. **Reset** environment and agents\n",
    "2. Loop until terminal:\n",
    "\n",
    "   * Select actions: `agent.act(states)`\n",
    "   * Step env: `next_states, rewards, dones = env.step(actions)`\n",
    "   * Store in buffer\n",
    "   * If enough samples:\n",
    "\n",
    "     * Sample a batch\n",
    "     * Call `maddpg.step(batch)`\n",
    "3. Track score (e.g., max of 2 agents)\n",
    "4. Optionally decay exploration\n",
    "\n",
    "### üì¶ Output:\n",
    "\n",
    "* Prints episodic scores\n",
    "* Logs metrics to TensorBoard (if enabled)\n",
    "* Returns list of all episode scores\n",
    "\n",
    "### ü§î Why needed?\n",
    "\n",
    "The Trainer integrates all components:\n",
    "\n",
    "* Orchestrates training\n",
    "* Abstracts environment-specific quirks\n",
    "* Supports monitoring, logging, and clean exits\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Putting it all together\n",
    "\n",
    "### üîß Setup:\n",
    "\n",
    "1. **Environment**:\n",
    "\n",
    "   * Launch Unity executable (e.g., `Tennis.x86_64`)\n",
    "   * Wrap it in `BootstrappedEnvironment`\n",
    "2. **Replay Buffer**:\n",
    "\n",
    "   * Shared `MultiAgentReplayBuffer` storing joint transitions\n",
    "3. **Agent**:\n",
    "\n",
    "   * Create `MADDPGAgent` with two `DDPGAgent`s\n",
    "4. **Trainer**:\n",
    "\n",
    "   * Connect all parts and run `trainer.train(...)`\n",
    "\n",
    "### üìà Example Flow:\n",
    "\n",
    "```python\n",
    "maddpg = MADDPGAgent(...)\n",
    "buffer = MultiAgentReplayBuffer(...)\n",
    "trainer = Trainer(env_path=\"Tennis.x86_64\", maddpg_agent=maddpg, replay_buffer=buffer)\n",
    "scores = trainer.train(n_episodes=2000)\n",
    "```\n",
    "\n",
    "### üîÑ Execution Cycle:\n",
    "\n",
    "```\n",
    "trainer --> env --> agent.act() --> env.step() --> buffer --> agent.learn()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Summary Table\n",
    "\n",
    "| Component      | Responsibility                                     |\n",
    "| -------------- | -------------------------------------------------- |\n",
    "| `DDPGAgent`    | Learn policy & critic for one agent                |\n",
    "| `MADDPGAgent`  | Train all agents jointly using centralized critics |\n",
    "| `Trainer`      | Drive the full episode loop, learning, logging     |\n",
    "| `Environment`  | Unity simulation of multi-agent continuous task    |\n",
    "| `ReplayBuffer` | Store and sample joint experience                  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MADDPG] Hyperparameters:\n",
      "  - num_agents: 2\n",
      "  - obs_size: 24\n",
      "  - action_size: 2\n",
      "  - actor_hidden: [128, 128]\n",
      "  - critic_hidden: [128, 128]\n",
      "  - actor_lr: 0.001\n",
      "  - critic_lr: 0.0001\n",
      "  - tau: 0.001\n",
      "  - gamma: 0.99\n",
      "  - device: cuda\n",
      "  - use_action_noise: False\n",
      "  - ou_noise_sigma: 0.01\n",
      "  - ou_noise_theta: 0.15\n",
      "  - seed: 0\n",
      "[Agent 0] Hyperparameters:\n",
      "  - actor_hidden: [128, 128]\n",
      "  - critic_hidden: [128, 128]\n",
      "  - actor_lr: 0.001\n",
      "  - critic_lr: 0.0001\n",
      "  - tau: 0.001\n",
      "  - gamma: 0.99\n",
      "  - actor_use_layer_norm: True\n",
      "  - critic_use_batch_norm: True\n",
      "  - ou_noise_sigma: 0.01\n",
      "  - ou_noise_theta: 0.15\n",
      "  - seed: 0\n",
      "  - device: cuda\n",
      "  - debug: True\n",
      "[OUNoise] Hyperparameters:\n",
      "  - mu: 0.0\n",
      "  - theta: 0.15\n",
      "  - sigma: 0.01\n",
      "  - seed: 0\n",
      "  - size: 2\n",
      "[OUNoise] Initial state: [0. 0.]\n",
      "[Agent 1] Hyperparameters:\n",
      "  - actor_hidden: [128, 128]\n",
      "  - critic_hidden: [128, 128]\n",
      "  - actor_lr: 0.001\n",
      "  - critic_lr: 0.0001\n",
      "  - tau: 0.001\n",
      "  - gamma: 0.99\n",
      "  - actor_use_layer_norm: True\n",
      "  - critic_use_batch_norm: True\n",
      "  - ou_noise_sigma: 0.01\n",
      "  - ou_noise_theta: 0.15\n",
      "  - seed: 1\n",
      "  - device: cuda\n",
      "  - debug: True\n",
      "[OUNoise] Hyperparameters:\n",
      "  - mu: 0.0\n",
      "  - theta: 0.15\n",
      "  - sigma: 0.01\n",
      "  - seed: 1\n",
      "  - size: 2\n",
      "[OUNoise] Initial state: [0. 0.]\n",
      "[Trainer] Hyperparameters:\n",
      "  - env_path: Tennis_Linux/Tennis.x86_64\n",
      "  - num_agents: 2\n",
      "  - obs_size: 24\n",
      "  - action_size: 2\n",
      "  - max_steps: 2000\n",
      "  - batch_size: 128\n",
      "  - train_every: 1\n",
      "  - warmup_steps: 10000\n",
      "  - log_dir: runs/train_maddpg/2025-05-16_12-21-24\n",
      "  - worker_id: 0\n",
      "[BootstrappedEnvironment] Initializing Unity environment with exe_path: Tennis_Linux/Tennis.x86_64, worker_id: 0, use_graphics: False\n",
      "[BootstrappedEnvironment] Preprocess function: None, Max retries: 5, Retry delay: 2\n",
      "[BootstrappedEnvironment] Reward shaping function: <function reward_scaling_fn at 0x7f24696e8e50>\n",
      "Found path: /home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis.x86_64\n",
      "Mono path[0] = '/home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis_Data/Managed'\n",
      "Mono config path = '/home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "\tScreenSelector.so\n",
      "Logging to /home/oliver/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "969a75b9864e43bbab0b8ca98630b8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 358 | Updates 0\n",
      "Episode 50 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 719 | Updates 0\n",
      "Episode 75 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 1075 | Updates 0\n",
      "Episode 100 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 1430 | Updates 0\n",
      "Episode 125 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 1791 | Updates 0\n",
      "Episode 150 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 2158 | Updates 0\n",
      "Episode 175 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 2513 | Updates 0\n",
      "Episode 200 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 2881 | Updates 0\n",
      "Episode 225 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 3262 | Updates 0\n",
      "Episode 250 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 3624 | Updates 0\n",
      "Episode 275 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 3996 | Updates 0\n",
      "Episode 300 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 4359 | Updates 0\n",
      "Episode 325 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 4714 | Updates 0\n",
      "Episode 350 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.090 | Steps 5107 | Updates 0\n",
      "Episode 375 | Last Score: -0.01 (0.00/-0.01) | Moving Average 0.090 | Steps 5471 | Updates 0\n",
      "Episode 400 | Last Score: -0.01 (0.00/-0.01) | Moving Average 0.190 | Steps 5869 | Updates 0\n",
      "Episode 425 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.290 | Steps 6283 | Updates 0\n",
      "Episode 450 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.190 | Steps 6662 | Updates 0\n",
      "Episode 475 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.190 | Steps 7018 | Updates 0\n",
      "Episode 500 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.090 | Steps 7404 | Updates 0\n",
      "Episode 525 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.090 | Steps 7764 | Updates 0\n",
      "Episode 550 | Last Score: -0.01 (-0.01/0.00) | Moving Average 0.090 | Steps 8130 | Updates 0\n",
      "Episode 575 | Last Score: -0.01 (0.00/-0.01) | Moving Average 0.090 | Steps 8506 | Updates 0\n",
      "Episode 600 | Last Score: -0.01 (0.00/-0.01) | Moving Average 0.090 | Steps 8870 | Updates 0\n",
      "Episode 625 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 9250 | Updates 0\n",
      "Episode 650 | Last Score: -0.01 (-0.01/0.00) | Moving Average -0.010 | Steps 9624 | Updates 0\n",
      "Episode 675 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 9992 | Updates 0\n",
      "Episode 700 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 10352 | Updates 351\n",
      "Episode 725 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 10708 | Updates 707\n",
      "Episode 750 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 11073 | Updates 1072\n",
      "Episode 775 | Last Score: -0.01 (0.00/-0.01) | Moving Average -0.010 | Steps 11429 | Updates 1428\n",
      "[BootstrappedEnvironment] Waiting 3 seconds for closing the environment...\n",
      "[BootstrappedEnvironment] Successfully wiped Unity processes from OS.\n",
      "[BootstrappedEnvironment] Unity environment closed and processes wiped successfully.\n",
      "[Training] Completed. Unity closed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m\n\u001b[1;32m    113\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    114\u001b[0m     env_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTennis_Linux/Tennis.x86_64\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     maddpg_agent\u001b[38;5;241m=\u001b[39mmaddpg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     reward_shaping_fn\u001b[38;5;241m=\u001b[39mreward_scaling_fn\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# --- Training Loop ---\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Optional: Save models\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(maddpg\u001b[38;5;241m.\u001b[39magents):\n",
      "File \u001b[0;32m~/project-showroom/projects/reinforcement-learning/collab-and-compet/codebase/maddpg/trainer.py:170\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_episodes, offline)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# 6) train if ready\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup_steps\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 170\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     states_b, actions_b, rewards_b, next_states_b, dones_b \u001b[38;5;241m=\u001b[39m samples\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Update normalizer on large batch and normalize for learning\u001b[39;00m\n",
      "File \u001b[0;32m~/project-showroom/projects/reinforcement-learning/collab-and-compet/codebase/experience/replay.py:44\u001b[0m, in \u001b[0;36mMultiAgentReplayBuffer.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(batch\u001b[38;5;241m.\u001b[39mstates), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     43\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(batch\u001b[38;5;241m.\u001b[39mactions), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 44\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrewards\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(batch\u001b[38;5;241m.\u001b[39mnext_states), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     46\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(batch\u001b[38;5;241m.\u001b[39mdones), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Watch for changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Monkey patch missing attributes for newer numpy versions\n",
    "if not hasattr(np, \"float_\"):\n",
    "    np.float_ = np.float64\n",
    "    \n",
    "if not hasattr(np, \"int_\"):\n",
    "    np.int_ = np.int64\n",
    "\n",
    "from codebase.maddpg.maddpg_agent import MADDPGAgent\n",
    "from codebase.experience.replay import MultiAgentReplayBuffer\n",
    "from codebase.maddpg.trainer import Trainer\n",
    "\n",
    "def survival_reward_shaper(next_state, env_reward, total_steps):\n",
    "    \"\"\"\n",
    "    Give a small bonus on every step the rally continues,\n",
    "    plus the original env_reward at terminal.\n",
    "    \"\"\"\n",
    "    # env_reward is an array shape (n_agents,)\n",
    "    # survival bonus: +0.01 every step (even if env_reward == 0.0)\n",
    "    bonus       = 0.01\n",
    "    shaped      = env_reward + bonus\n",
    "    shaping_term = np.full_like(env_reward, bonus)\n",
    "    g_norm      = None\n",
    "    \n",
    "    return shaped, shaping_term, g_norm\n",
    "\n",
    "def reward_scaling_fn(next_state: np.ndarray,\n",
    "                      env_reward:   np.ndarray,\n",
    "                      total_steps:  int):\n",
    "    \"\"\"\n",
    "    Reward scaler for Unity Tennis that scales only positive rewards.\n",
    "\n",
    "    +0.1 ‚Üí +1.0 (scaled by `scale_factor`)\n",
    "     0.0 or -0.01 ‚Üí unchanged\n",
    "\n",
    "    Returns:\n",
    "      reward: the scaled reward (shape [n_agents])\n",
    "      shaped: the shaping term = reward - env_reward    \n",
    "    \"\"\"\n",
    "    scale_factor = 100.0  # scale +0.1 ‚Üí +1.0\n",
    "\n",
    "    # Copy to avoid modifying env_reward in place\n",
    "    reward = env_reward.copy()\n",
    "    mask = reward > 0\n",
    "    reward[mask] = reward[mask] * scale_factor\n",
    "\n",
    "    shaping = reward - env_reward\n",
    "    \n",
    "    return reward, shaping\n",
    "\n",
    "\n",
    "seed = 0\n",
    "\n",
    "# seeds\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark     = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "log_dir=os.path.join(\"runs/train_maddpg\", time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "num_agents = 2\n",
    "obs_size = 24 # 3 x 8 variables (3 steps)\n",
    "action_size = 2\n",
    "batch_size = 128\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "buffer = MultiAgentReplayBuffer(\n",
    "    num_agents=2,\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    buffer_size=int(1e5),\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- MADDPG Agent ---\n",
    "maddpg = MADDPGAgent(\n",
    "    num_agents=2,\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    actor_hidden=[128, 128],\n",
    "    critic_hidden=[128, 128],\n",
    "    actor_lr=1e-3,\n",
    "    critic_lr=1e-4,\n",
    "    tau=1e-3,\n",
    "    gamma=0.99,\n",
    "    device=device,\n",
    "    seed=seed,\n",
    "    use_action_noise=False,\n",
    "    ou_noise_sigma=0.03, # œÉ\n",
    "    ou_noise_theta=0.15, # Œ∏\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    env_path=\"Tennis_Linux/Tennis.x86_64\",\n",
    "    maddpg_agent=maddpg,\n",
    "    replay_buffer=buffer,\n",
    "    num_agents=2,\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    max_steps=2000,\n",
    "    batch_size=batch_size,\n",
    "    train_every=1,    \n",
    "    warmup_steps=10000, # no training before this   \n",
    "    early_training_steps=10000, # filters negative rewards\n",
    "    neg_sampling_ratio=0.2,\n",
    "    log_dir=log_dir,\n",
    "    use_state_norm=False,\n",
    "    reward_shaping_fn=reward_scaling_fn\n",
    ")\n",
    "\n",
    "# --- Training Loop ---\n",
    "scores = trainer.train(n_episodes=2000)\n",
    "\n",
    "# Optional: Save models\n",
    "for i, agent in enumerate(maddpg.agents):\n",
    "    torch.save(agent.actor.state_dict(), f\"checkpoint_actor_agent_{i}.pth\")\n",
    "    torch.save(agent.critic.state_dict(), f\"checkpoint_critic_agent_{i}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline synthetic buffer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BootstrappedEnvironment] Initializing Unity environment with exe_path: Tennis_Linux/Tennis.x86_64, worker_id: 0, use_graphics: False\n",
      "[BootstrappedEnvironment] Preprocess function: None, Max retries: 5, Retry delay: 2\n",
      "[BootstrappedEnvironment] Reward shaping function: None\n",
      "Found path: /home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis.x86_64\n",
      "Mono path[0] = '/home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis_Data/Managed'\n",
      "Mono config path = '/home/oliver/project-showroom/projects/reinforcement-learning/collab-and-compet/Tennis_Linux/Tennis_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "\tScreenSelector.so\n",
      "Logging to /home/oliver/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer pre-filled with random data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2834c3cc5adf440fa5df1ec377ea1e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Offline training:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Agent 0] Actor gradient dead for 500 steps. Injecting noise...\n",
      "[Agent 1] Actor gradient dead for 500 steps. Injecting noise...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuffer pre-filled with random data.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# --- Training Loop ---\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/project-showroom/projects/reinforcement-learning/collab-and-compet/codebase/maddpg/trainer.py:99\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, n_episodes, offline)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, n_episodes: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m, offline: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m offline:\n\u001b[0;32m---> 99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_from_prefilled_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     scores_deque \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    102\u001b[0m     all_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/project-showroom/projects/reinforcement-learning/collab-and-compet/codebase/maddpg/trainer.py:66\u001b[0m, in \u001b[0;36mTrainer._train_from_prefilled_buffer\u001b[0;34m(self, n_updates)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Log every LOG_FREQ updates using update_idx as step\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_idx \u001b[38;5;241m%\u001b[39m LOG_FREQ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/project-showroom/projects/reinforcement-learning/collab-and-compet/codebase/maddpg/maddpg_agent.py:120\u001b[0m, in \u001b[0;36mMADDPGAgent.step\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# 3) Let each agent learn\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents):\n\u001b[0;32m--> 120\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# (B, obs_size)\u001b[39;49;00m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m      \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdones\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m        \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m      \u001b[49m\u001b[43mobs_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\u001b[38;5;66;43;03m# (B, full_obs_size)\u001b[39;49;00m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[43mactions_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_obs_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_obs_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_actions_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnext_actions_all\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_logs[i]\u001b[38;5;241m.\u001b[39mappend(metrics)\n",
      "File \u001b[0;32m~/project-showroom/projects/reinforcement-learning/collab-and-compet/codebase/maddpg/ddpg_agent.py:214\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self, obs_batch, rewards, dones, obs_all, actions_all, next_obs_all, next_actions_all)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 214\u001b[0m         total_norm \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    216\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m total_norm \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# g) dead‚Äêgradient detection & recovery (unchanged)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Watch for changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Monkey patch missing attributes for newer numpy versions\n",
    "if not hasattr(np, \"float_\"):\n",
    "    np.float_ = np.float64\n",
    "    \n",
    "if not hasattr(np, \"int_\"):\n",
    "    np.int_ = np.int64\n",
    "\n",
    "from codebase.maddpg.maddpg_agent import MADDPGAgent\n",
    "from codebase.experience.replay import MultiAgentReplayBuffer\n",
    "from codebase.maddpg.trainer import Trainer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "log_dir=os.path.join(\"runs/train_maddpg\", time.strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "\n",
    "num_agents = 2\n",
    "obs_size = 24 # 3 x 8 variables (3 steps)\n",
    "action_size = 2\n",
    "batch_size = 256\n",
    "buffer_size = 100000\n",
    "\n",
    "# --- Replay Buffer ---\n",
    "buffer = MultiAgentReplayBuffer(\n",
    "    num_agents=2,\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    buffer_size=buffer_size,\n",
    "    batch_size=batch_size,\n",
    "    seed=0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- MADDPG Agent ---\n",
    "maddpg = MADDPGAgent(\n",
    "    num_agents=2,\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    actor_hidden=[256, 128],\n",
    "    critic_hidden=[256, 128],\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-3,\n",
    "    tau=1e-3,\n",
    "    gamma=0.0, #0.99,\n",
    "    device=device,\n",
    "    seed=0,\n",
    "    use_action_noise=False,\n",
    "    ou_noise_sigma=0.2, # œÉ\n",
    "    ou_noise_theta=0.15, # Œ∏\n",
    ")\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer = Trainer(\n",
    "    env_path=\"Tennis_Linux/Tennis.x86_64\",\n",
    "    maddpg_agent=maddpg,\n",
    "    replay_buffer=buffer,\n",
    "    num_agents=2,\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    max_steps=1000,\n",
    "    batch_size=batch_size,\n",
    "    train_every=1,    \n",
    "    warmup_steps=0,    \n",
    "    log_dir=log_dir,\n",
    ")\n",
    "\n",
    "# pre-fill the buffer with random data\n",
    "buffer_size = 100000\n",
    "for _ in range(buffer_size):\n",
    "    # 1) random states\n",
    "    states     = [np.random.uniform(-1,1,obs_size)    for _ in range(num_agents)]\n",
    "    # 2) random actions in full range\n",
    "    actions    = [np.random.uniform(-1,1,action_size) for _ in range(num_agents)]\n",
    "    # 3) *structured* reward: simply equal to each agent‚Äôs first action dim\n",
    "    rewards    = [a[0] for a in actions]\n",
    "    # 4) random next-states, no terminal\n",
    "    next_states = [np.random.uniform(-1,1,obs_size) for _ in range(num_agents)]\n",
    "    dones      = [False]*num_agents\n",
    "\n",
    "    buffer.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "print(\"Buffer pre-filled with random data.\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "scores = trainer.train(n_episodes=1000, offline=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ The Overall Code Respects the Cooperative Setting\n",
    "\n",
    "### 1. **Agents are trained independently but not adversarially**\n",
    "\n",
    "* Each agent‚Äôs `DDPGAgent.learn(...)` call updates only its own actor and critic.\n",
    "* There is **no attempt to minimize the opponent‚Äôs reward** (which would happen in competitive training via negated rewards or zero-sum Q functions).\n",
    "* The critic loss is based on:\n",
    "\n",
    "  $$\n",
    "  Q_i(o_1, o_2, a_1, a_2) \\rightarrow \\text{target} = r_i + \\gamma Q'_i(o'_1, o'_2, a'_1, a'_2)\n",
    "  $$\n",
    "\n",
    "  which uses **only the reward of agent $i$** ‚Äî not any form of relative score.\n",
    "\n",
    "### 2. **Experience is shared through the Replay Buffer**\n",
    "\n",
    "* A single `MultiAgentReplayBuffer` is used.\n",
    "* Both agents store their transitions in the same structure ‚Äî this enables **shared learning dynamics** even if the training is technically per-agent.\n",
    "* Actions and observations are jointly stored, allowing critics to **observe coordination** effects.\n",
    "\n",
    "### 3. **Decentralized Actors, Centralized Critics**\n",
    "\n",
    "* During training, critics have access to the full state and action space:\n",
    "\n",
    "  $$\n",
    "  Q_i(o_1, o_2, a_1, a_2)\n",
    "  $$\n",
    "\n",
    "  This allows them to learn **joint-value estimates** ‚Äî crucial for modeling inter-agent dynamics and helping agents learn to cooperate.\n",
    "* During execution, actors use only their **local observation**:\n",
    "\n",
    "  $$\n",
    "  a_i = \\mu_{\\theta_i}(o_i)\n",
    "  $$\n",
    "\n",
    "  which respects the decentralized nature of the environment.\n",
    "\n",
    "### 4. **Reward Handling**\n",
    "\n",
    "* Each agent receives its **own reward** from the environment (`rewards[i]`).\n",
    "* We **do not subtract** the other agent's reward or implement any competition.\n",
    "* The training goal, implicitly, is:\n",
    "\n",
    "  $$\n",
    "  \\text{maximize average/max score across both agents}\n",
    "  $$\n",
    "\n",
    "  ‚Äî which aligns with the projects definition of solving the environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Even though the architecture supports competitive training (e.g., centralized critics could be modified for zero-sum training), everything you‚Äôre doing right now:\n",
    "\n",
    "* Optimizing each agent **independently** using **only its own reward**\n",
    "* Enabling **joint context** via centralized critics\n",
    "* Measuring performance via max agent score or average rally length\n",
    "\n",
    "‚Ä¶is entirely appropriate for a **cooperative, coordination-based task** like Unity Tennis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Training Dynamics</summary>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Part 1: What *Should* Happen (Ideal Dynamics)\n",
    "\n",
    "### üîÅ Environment Setup Reminder:\n",
    "\n",
    "* 2 agents cooperatively **bounce a ball over the net**\n",
    "* Each agent gets **individual observations and rewards**\n",
    "* A positive reward (+0.1) for successful hits\n",
    "* A small penalty (‚àí0.01) for letting the ball drop or go out\n",
    "* Goal: learn to **rally the ball back and forth as long as possible**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Ideal Training Dynamics Step-by-Step\n",
    "\n",
    "#### üîÑ Phase 1: Warm-up (exploration)\n",
    "\n",
    "* OU noise dominates ‚Üí both agents try random actions\n",
    "* Ball rarely crosses the net\n",
    "* Short episodes, low reward\n",
    "* Replay buffer fills with diverse (but mostly bad) experiences\n",
    "\n",
    "#### üîÅ Phase 2: Early improvement\n",
    "\n",
    "* Occasionally a random action **succeeds** in returning the ball\n",
    "* Those rare **positive rewards** are captured\n",
    "* The **critic learns** that certain states + actions yield nonzero Q\n",
    "* Actors start **drifting toward those actions**\n",
    "\n",
    "#### üîÑ Phase 3: Self-reinforcing cooperation\n",
    "\n",
    "* Agents learn to **position and return** the ball more reliably\n",
    "* Q-values rise ‚Üí actor gradients stay alive\n",
    "* Buffer fills with **cooperative transitions**\n",
    "* Action outputs move away from saturation toward nuanced control\n",
    "* Episode lengths grow\n",
    "* Rewards per episode grow\n",
    "\n",
    "#### üîÑ Phase 4: Stabilization\n",
    "\n",
    "* OU noise has smaller effect (by clamping, or annealing if used)\n",
    "* Actors fine-tune behavior\n",
    "* Losses stabilize, Q-values converge\n",
    "* Each agent adapts to the **other agent's behavior** (emergent coordination)\n",
    "\n",
    "---\n",
    "\n",
    "## üß® Part 2: What‚Äôs Going Wrong in Your Case\n",
    "\n",
    "Your telemetry suggests the following **pathological divergence** instead.\n",
    "\n",
    "### üõë Phase 1: Exploration (normal)\n",
    "\n",
    "* Noise leads to random action\n",
    "* Very short episodes, mostly failures\n",
    "* Replay buffer gets filled ‚Üí so far still fine\n",
    "\n",
    "### ‚ö†Ô∏è Phase 2: Asymmetric early success\n",
    "\n",
    "* **Agent\\_1** hits the ball over once by luck ‚Üí gets a +0.1\n",
    "* **Agent\\_0** fails ‚Üí gets a -0.01\n",
    "* Buffer now has a skew: more **positive examples** for agent\\_1, negative for agent\\_0\n",
    "\n",
    "### üß® Phase 3: Divergence\n",
    "\n",
    "* Agent\\_1‚Äôs critic starts learning \"returning the ball ‚Üí good\"\n",
    "* Agent\\_0's critic starts learning \"bad things happen always\"\n",
    "* Agent\\_0‚Äôs Q-values drop ‚Üí gradients vanish ‚Üí actor dies\n",
    "* Agent\\_0 saturates (e.g. constant -0.9 output), stops changing\n",
    "* Now agent\\_1 has **no partner** to rally with\n",
    "\n",
    "### üîÅ Phase 4: Collapse\n",
    "\n",
    "* Agent\\_1 still explores briefly, but rallies are impossible\n",
    "* Q-values for both agents fall again\n",
    "* Reward per episode collapses\n",
    "* All actors saturate ‚Üí dead gradients\n",
    "* Critic loss stays \"alive\", but learning stalls\n",
    "* Policy stuck in local failure mode\n",
    "\n",
    "---\n",
    "\n",
    "## üî¨ Key Symptoms You Reported (Matching This Pattern)\n",
    "\n",
    "| Signal                | Interpretation                             |\n",
    "| --------------------- | ------------------------------------------ |\n",
    "| Gradients dead        | üö® Actor collapsed, no learning signal     |\n",
    "| Q-values flat/falling | ‚ùå Critic has nothing meaningful to predict |\n",
    "| Action saturation     | ‚ùå Policy learned rigid extremes            |\n",
    "| Reward mean falling   | Agents worsening ‚Üí failure loop            |\n",
    "| StateMean divergence  | One agent exploring, other stuck           |\n",
    "| Reward std ‚Üí 0        | No variability, no discovery               |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What You Ideally Want to See\n",
    "\n",
    "| Metric         | Ideal Signal                      |\n",
    "| -------------- | --------------------------------- |\n",
    "| Actor gradient | Fluctuating, non-zero             |\n",
    "| Q-values       | Rising early, then stabilizing    |\n",
    "| Actions        | Spanning mid-range values         |\n",
    "| Reward mean    | Increasing trend                  |\n",
    "| Reward std     | High early, reducing with mastery |\n",
    "| State mean/std | Symmetrical across agents         |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Final Summary\n",
    "\n",
    "Your agents are:\n",
    "\n",
    "* **Learning asymmetrically**\n",
    "* **Failing to bootstrap cooperation**\n",
    "* Getting trapped in a **local dead zone** where coordination never emerges\n",
    "\n",
    "\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unity_env",
   "language": "python",
   "name": "unity_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
