{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a61054e",
   "metadata": {},
   "source": [
    "# Pruning LLMs: Structured MLP Pruning and Evaluation\n",
    "\n",
    "Welcome to the pruning basics notebook. This notebook walks you through a practical, end‑to‑end workflow for pruning a small LLM (TinyLlama) and measuring the impact on speed, memory, and quality.\n",
    "\n",
    "## What You’ll Learn\n",
    "- Pruning concepts: how removing neurons/weights can shrink and speed up models.\n",
    "- Structured MLP pruning: zeroing entire rows/columns across `gate_proj`, `up_proj`, and `down_proj`.\n",
    "- Rebuilding layers: replacing pruned modules with smaller `nn.Linear` layers to make sparsity dense and actually cheaper.\n",
    "- Evaluation metrics:\n",
    "  - Latency (s) and Throughput (tokens/s) for generation.\n",
    "  - Peak GPU memory (MiB) during a forward pass.\n",
    "  - Perplexity (lower is better) as a proxy for model quality.\n",
    "- Practical trade‑offs: how pruning fraction affects performance vs. quality.\n",
    "\n",
    "## Notebook Flow\n",
    "1. Baseline: load model/tokenizer and measure latency, throughput, peak memory, and perplexity.\n",
    "2. Prune: apply structured pruning to MLP layers on CPU to avoid OOM.\n",
    "3. Rebuild: reconstruct smaller MLP layers to realize speed/memory benefits.\n",
    "4. Re‑measure: run the same metrics to compare against baseline.\n",
    "5. Save: write the rebuilt model to disk and report size.\n",
    "\n",
    "## Key Functions in This Notebook\n",
    "- `measure_latency_and_throughput(...)`: generation benchmarking.\n",
    "- `measure_peak_mem_and_perplexity(...)`: forward pass memory + loss→perplexity.\n",
    "- `prune_mlp_rows_and_cols(...)`: apply structured pruning masks on MLP.\n",
    "- `rebuild_mlp_blocks(...)`: build smaller dense layers using kept indices.\n",
    "- `start()`: ties everything together and prints results.\n",
    "\n",
    "## Metrics at a Glance\n",
    "- Latency: total time to produce `MAX_NEW_TOKENS` tokens.\n",
    "- Throughput: tokens per second = generated_tokens / latency.\n",
    "- Peak GPU Memory: maximum allocated memory during forward pass (MiB).\n",
    "- Perplexity: `exp(loss)`; lower suggests better next‑token prediction.\n",
    "\n",
    "## Configuration & Reproducibility\n",
    "- Model: `MODEL_NAME` (TinyLlama by default for quick runs).\n",
    "- Pruning strength: `MLP_PRUNE_FRAC` (e.g., 0.5 → keep ~50% of MLP neurons).\n",
    "- Generation length: `MAX_NEW_TOKENS`.\n",
    "- Device: auto‑selects CUDA if available, else CPU (FP16 requests on CPU are coerced to FP32 by libraries).\n",
    "\n",
    "## Requirements & Notes\n",
    "- Libraries: `torch`, `transformers`. A CUDA GPU is recommended but not required.\n",
    "- Internet: Hugging Face Hub downloads the model on first use.\n",
    "- Determinism: results can vary slightly across runs/hardware; consider seeding for tighter reproducibility.\n",
    "- Trade‑off: higher pruning fractions tend to boost speed/memory savings but may degrade perplexity/quality.\n",
    "\n",
    "## How to Use\n",
    "- Run the notebook top‑to‑bottom, or execute `start()` to perform the full pipeline.\n",
    "- Tweak `MLP_PRUNE_FRAC`, `MAX_NEW_TOKENS`, or `PROMPT`/`PERP_TEXT` to explore trade‑offs.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553eb726",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell:\n",
    "- Imports standard libraries for file handling, timing, and mathematical operations.\n",
    "- Imports PyTorch for deep learning operations and pruning utilities.\n",
    "- Imports Hugging Face Transformers for model and tokenizer handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851cb58b-1175-45d8-9113-9869ca6cfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7b2d8",
   "metadata": {},
   "source": [
    "# Define Model and Evaluation Settings\n",
    "This cell:\n",
    "- Specifies the model name to be used for pruning and evaluation.\n",
    "- Defines the fraction of neurons to prune in the MLP layers (`MLP_PRUNE_FRAC`).\n",
    "- Sets the maximum number of tokens to generate during inference.\n",
    "- Provides sample texts for benchmarking latency, throughput, and perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3b226d-9e0d-4ae9-984d-4e00e8006d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME     = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "MLP_PRUNE_FRAC = 0.2       # fraction of inner neurons to prune\n",
    "MAX_NEW_TOKENS = 50\n",
    "PROMPT = (\n",
    "    \"Over the next decade, sustainable energy solutions will revolutionize \"\n",
    "    \"global power grids, reducing carbon footprints and fostering resilient \"\n",
    "    \"communities through innovative storage and distribution technologies.\"\n",
    ")\n",
    "PERP_TEXT = (\n",
    "    \"Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast \"\n",
    "    \"to the natural intelligence displayed by humans and animals. Leading AI textbooks \"\n",
    "    \"define the field as the study of intelligent agents: any system that perceives \"\n",
    "    \"its environment and takes actions that maximize its chance of achieving its goals.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb75c67",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer\n",
    "This function:\n",
    "- Loads the tokenizer and model using Hugging Face Transformers.\n",
    "- Configures the model to use FP16 precision for faster inference.\n",
    "- Moves the model to the specified device (CPU or GPU).\n",
    "- Sets the model to evaluation mode to disable gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d36546f-6bbe-4d65-b952-2520b94ab986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      - Load AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "      - Load AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "      - Move model to `device` and set to .eval()\n",
    "      - Return tokenizer, model\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # load model in FP16 for faster inference\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    # move to device and set to eval\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e3bb63",
   "metadata": {},
   "source": [
    "# Measure Baseline Performance\n",
    "This function:\n",
    "- Measures the baseline performance of the model before pruning.\n",
    "- Evaluates:\n",
    "  - **Latency**: Time taken to generate tokens for a given prompt.\n",
    "  - **Throughput**: Tokens generated per second.\n",
    "  - **Peak GPU Memory Usage**: Maximum memory used during inference.\n",
    "  - **Perplexity**: A measure of how well the model predicts the given text.\n",
    "- Prints the baseline metrics for comparison with the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7273e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_latency_and_throughput(model, tokenizer, prompt: str, device: torch.device, max_new_tokens=50, runs=3):\n",
    "    \"\"\"\n",
    "    Measure latency and throughput for text generation.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to evaluate.\n",
    "        tokenizer: The tokenizer associated with the model.\n",
    "        prompt (str): Input prompt for text generation.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        runs (int): Number of runs for averaging metrics.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average latency (seconds) and throughput (tokens per second).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    # Warmup\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=8)\n",
    "\n",
    "    latencies = []\n",
    "    throughputs = []\n",
    "    for _ in range(runs):\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        gen_len = outputs.shape[1] - inputs[\"input_ids\"].shape[1]\n",
    "        latency = t1 - t0\n",
    "        throughput = gen_len / latency if latency > 0 else float(\"nan\")\n",
    "\n",
    "        latencies.append(latency)\n",
    "        throughputs.append(throughput)\n",
    "\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    avg_throughput = sum(throughputs) / len(throughputs)\n",
    "    return avg_latency, avg_throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e97ed4-3031-452c-b3d9-7d79afa84e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_peak_mem_and_perplexity(model, tokenizer, perp_text, device):\n",
    "    \"\"\"\n",
    "    Measure peak GPU memory (MiB) during a forward pass and compute perplexity\n",
    "    on the provided text. Returns (peak_mem_mib, perplexity).\n",
    "    \n",
    "    - Tokenizes `perp_text` and sets labels to input_ids to obtain loss.\n",
    "    - Resets and reads CUDA peak memory stats if on GPU; returns NaN on CPU.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and set labels for LM loss\n",
    "    enc = tokenizer(perp_text, return_tensors=\"pt\")\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].clone()\n",
    "    enc = {k: v.to(device) for k, v in enc.items()}\n",
    "    \n",
    "    # Prepare CUDA memory measurement if available\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Forward pass to get loss (no gradients)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**enc)\n",
    "    \n",
    "    # Read peak memory\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        peak_bytes = torch.cuda.max_memory_allocated(device)\n",
    "        peak_mib = peak_bytes / 1024**2\n",
    "    else:\n",
    "        peak_mib = float(\"nan\")\n",
    "    \n",
    "    # Convert loss to perplexity\n",
    "    loss = outputs.loss.item()\n",
    "    perplexity = math.exp(loss)\n",
    "    return peak_mib, perplexity\n",
    "\n",
    "def measure_baseline(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      - Warm up & measure generation latency & throughput on `prompt`\n",
    "      - Measure peak GPU memory & perplexity on `perp_text`\n",
    "      - Print or return these baseline metrics\n",
    "    \"\"\"\n",
    "    # 1) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(model, tokenizer, prompt, device)\n",
    "    # 2) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(model, tokenizer, perp_text, device)\n",
    "\n",
    "    # 3) Print baseline metrics\n",
    "    print(f\"[Baseline] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Baseline] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Baseline] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Baseline] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # Return in case caller wants to use them programmatically\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f5a15",
   "metadata": {},
   "source": [
    "# Prune MLP Rows and Columns\n",
    "This function:\n",
    "- Prunes the MLP layers in the model by:\n",
    "  - Zeroing out a fraction of rows in the `gate_proj` and `up_proj` layers.\n",
    "  - Zeroing out the corresponding columns in the `down_proj` layer.\n",
    "- Uses structured pruning to remove entire rows or columns.\n",
    "- Ensures pruning is performed on the CPU to avoid GPU memory issues.\n",
    "- Removes the pruning reparameterizations after applying the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d94de27-8bb4-4110-84f5-b9cac02dc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_mlp_rows_and_cols(model: nn.Module, prune_frac: float):\n",
    "    \"\"\"\n",
    "      - Move model to CPU\n",
    "      - For each layer in model.model.layers:\n",
    "          • Zero out `prune_frac` of rows in gate_proj and up_proj\n",
    "          • Zero out corresponding `prune_frac` of columns in down_proj\n",
    "      - Remove pruning reparameterizations\n",
    "    \"\"\"\n",
    "    # 1) Ensure we prune on CPU to avoid GPU OOM\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # 2) Iterate through each decoder layer’s MLP\n",
    "    for layer in model.model.layers:\n",
    "        gate = layer.mlp.gate_proj   # [inner, hidden]\n",
    "        up   = layer.mlp.up_proj     # [inner, hidden]\n",
    "        down = layer.mlp.down_proj   # [hidden, inner]\n",
    "\n",
    "        # 2a) Zero out rows in gate_proj and up_proj\n",
    "        for proj in (gate, up):\n",
    "            prune.ln_structured(\n",
    "                proj,\n",
    "                name=\"weight\",\n",
    "                amount=prune_frac,\n",
    "                n=1,\n",
    "                dim=0,           # prune entire rows\n",
    "            )\n",
    "            prune.remove(proj, \"weight\")\n",
    "\n",
    "        # 2b) Zero out corresponding columns in down_proj\n",
    "        prune.ln_structured(\n",
    "            down,\n",
    "            name=\"weight\",\n",
    "            amount=prune_frac,\n",
    "            n=1,\n",
    "            dim=1,               # prune entire columns\n",
    "        )\n",
    "        prune.remove(down, \"weight\")\n",
    "\n",
    "    # 3) Return the model (now with zeros in place)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1ba705",
   "metadata": {},
   "source": [
    "# Rebuild MLP Blocks\n",
    "This function:\n",
    "- Reconstructs the pruned MLP layers with reduced dimensions.\n",
    "- Identifies the neurons that were not pruned in the `gate_proj` layer.\n",
    "- Creates new `nn.Linear` modules for `gate_proj`, `up_proj`, and `down_proj` with updated dimensions.\n",
    "- Copies the weights and biases from the original layers to the new layers.\n",
    "- Replaces the old modules with the new ones in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d85619ae-1a0f-44f2-99f1-cb8cb3d7bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_mlp_blocks(model: nn.Module):\n",
    "    \"\"\"\n",
    "      - For each layer in model.model.layers:\n",
    "          1) Identify kept neuron indices in gate_proj\n",
    "          2) Construct new nn.Linear modules for gate_proj, up_proj, down_proj\n",
    "             with reduced dimensions\n",
    "          3) Copy over weights and biases\n",
    "          4) Replace the old modules on the model\n",
    "    \"\"\"\n",
    "    for layer in model.model.layers:\n",
    "        # original modules (still on CPU, dtype=original)\n",
    "        old_gate = layer.mlp.gate_proj\n",
    "        old_up   = layer.mlp.up_proj\n",
    "        old_down = layer.mlp.down_proj\n",
    "\n",
    "        # discover surviving rows in gate_proj\n",
    "        Wg = old_gate.weight.data     # [inner_orig, hidden], dtype say torch.half\n",
    "        keep_idx = (Wg.abs().sum(dim=1) != 0).nonzero(as_tuple=False).view(-1)\n",
    "        inner_new = keep_idx.numel()\n",
    "        hidden    = Wg.size(1)\n",
    "        dtype     = Wg.dtype\n",
    "        device    = Wg.device\n",
    "\n",
    "        # helper to build a new Linear with the same dtype/device\n",
    "        def make_linear(in_f, out_f, bias, old_weight, old_bias=None):\n",
    "            nl = nn.Linear(in_f, out_f, bias=bias)\n",
    "            # init in correct dtype & device\n",
    "            nl.weight.data = old_weight.clone().to(device=device, dtype=dtype)\n",
    "            if bias and old_bias is not None:\n",
    "                nl.bias.data = old_bias.clone().to(device=device, dtype=dtype)\n",
    "            return nl\n",
    "\n",
    "        # rebuild gate_proj: hidden -> inner_new\n",
    "        new_gate = make_linear(\n",
    "            hidden, inner_new, \n",
    "            bias=(old_gate.bias is not None),\n",
    "            old_weight=old_gate.weight.data[keep_idx],\n",
    "            old_bias=old_gate.bias.data[keep_idx] if old_gate.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild up_proj: hidden -> inner_new\n",
    "        new_up = make_linear(\n",
    "            hidden, inner_new,\n",
    "            bias=(old_up.bias is not None),\n",
    "            old_weight=old_up.weight.data[keep_idx],\n",
    "            old_bias=old_up.bias.data[keep_idx] if old_up.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # rebuild down_proj: inner_new -> hidden\n",
    "        new_down = make_linear(\n",
    "            inner_new, hidden,\n",
    "            bias=(old_down.bias is not None),\n",
    "            old_weight=old_down.weight.data[:, keep_idx],\n",
    "            old_bias=old_down.bias.data if old_down.bias is not None else None\n",
    "        )\n",
    "\n",
    "        # swap in-place\n",
    "        layer.mlp.gate_proj = new_gate\n",
    "        layer.mlp.up_proj   = new_up\n",
    "        layer.mlp.down_proj = new_down\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3b715",
   "metadata": {},
   "source": [
    "# Measure Performance After Rebuilding\n",
    "This function:\n",
    "- Evaluates the performance of the rebuilt model after pruning and reconstruction.\n",
    "- Measures:\n",
    "  - **Latency**: Time taken to generate tokens for a given prompt.\n",
    "  - **Throughput**: Tokens generated per second.\n",
    "  - **Peak GPU Memory Usage**: Maximum memory used during inference.\n",
    "  - **Perplexity**: A measure of how well the model predicts the given text.\n",
    "- Prints the metrics for comparison with the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f5c2f5-9541-42e1-8389-dcd1b216add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_rebuilt(model: nn.Module, tokenizer, prompt: str, perp_text: str, device: torch.device):\n",
    "    \"\"\"\n",
    "      - Move rebuilt model to `device` & .eval()\n",
    "      - Re-measure latency, throughput, peak memory, perplexity\n",
    "      - Print or return these metrics\n",
    "    \"\"\"\n",
    "    # 1) Move to device and set to eval\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) Measure latency & throughput\n",
    "    latency, throughput = measure_latency_and_throughput(\n",
    "        model, tokenizer, prompt, device\n",
    "    )\n",
    "\n",
    "    # 3) Measure peak GPU memory & perplexity\n",
    "    peak_mem, perplexity = measure_peak_mem_and_perplexity(\n",
    "        model, tokenizer, perp_text, device\n",
    "    )\n",
    "\n",
    "    # 4) Print results\n",
    "    print(f\"[Rebuilt] latency   = {latency:.3f}s\")\n",
    "    print(f\"[Rebuilt] throughput= {throughput:.1f} tok/s\")\n",
    "    print(f\"[Rebuilt] peak GPU  = {peak_mem:.1f} MiB\")\n",
    "    print(f\"[Rebuilt] perplexity= {perplexity:.3f}\")\n",
    "\n",
    "    # 5) Return for further use if needed\n",
    "    return {\n",
    "        \"latency\": latency,\n",
    "        \"throughput\": throughput,\n",
    "        \"peak_gpu_mem\": peak_mem,\n",
    "        \"perplexity\": perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d350c8c",
   "metadata": {},
   "source": [
    "# Save Model and Report Size\n",
    "This function:\n",
    "- Saves the pruned and rebuilt model to the specified output directory.\n",
    "- Calculates the total size of the saved model files on disk.\n",
    "- Prints the on-disk size of the model for comparison with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b5cde41-77ee-4339-a06c-6fe825e6ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_report_size(model: nn.Module, output_dir: str):\n",
    "    \"\"\"\n",
    "      - model.save_pretrained(output_dir)\n",
    "      - Walk `output_dir` to sum file sizes (in MiB)\n",
    "      - Print the on-disk size\n",
    "    \"\"\"\n",
    "    # 1) Save\n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    # 2) Sum file sizes\n",
    "    total_bytes = 0\n",
    "    for root, _, files in os.walk(output_dir):\n",
    "        for fname in files:\n",
    "            total_bytes += os.path.getsize(os.path.join(root, fname))\n",
    "\n",
    "    # 3) Convert to MiB and print\n",
    "    size_mb = total_bytes / 1024**2\n",
    "    print(f\"[Rebuilt] on-disk size = {size_mb:.1f} MiB\")\n",
    "\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab775d2",
   "metadata": {},
   "source": [
    "# Main Execution Flow\n",
    "This function:\n",
    "- Loads the model and tokenizer.\n",
    "- Measures the baseline performance of the model.\n",
    "- Applies structured pruning to the MLP layers.\n",
    "- Rebuilds the pruned MLP layers with reduced dimensions.\n",
    "- Measures the performance of the rebuilt model.\n",
    "- Saves the pruned and rebuilt model to disk and reports its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c3803f-9b33-42b3-a1cf-0891f0ea130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start():\n",
    "    device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    tokenizer, model = load_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # Baseline\n",
    "    print(\"Measuring baseline performance...\")\n",
    "    measure_baseline(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Prune on CPU\n",
    "    print(f\"Pruning {MLP_PRUNE_FRAC*100:.1f}% of MLP neurons...\")\n",
    "    prune_mlp_rows_and_cols(model, MLP_PRUNE_FRAC)\n",
    "\n",
    "    # Measure after pruning (before rebuild)\n",
    "    print(\"Measuring pruned (unrebuilt) model performance...\")\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "    \n",
    "    # Rebuild smaller MLPs\n",
    "    print(\"Rebuilding smaller MLP blocks...\")\n",
    "    rebuild_mlp_blocks(model)\n",
    "\n",
    "    # Re-benchmark rebuilt model\n",
    "    print(\"Measuring rebuilt model performance...\")\n",
    "    measure_rebuilt(model, tokenizer, PROMPT, PERP_TEXT, device)\n",
    "\n",
    "    # Save & report on-disk size\n",
    "    print(\"Saving rebuilt model and reporting on-disk size...\")\n",
    "    save_and_report_size(model, \"llama_pruned_rebuilt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0be5e3",
   "metadata": {},
   "source": [
    "# Start the Pruning and Evaluation Process\n",
    "This cell:\n",
    "- Calls the `start` function to execute the entire pruning and evaluation pipeline.\n",
    "- Outputs the baseline and post-pruning metrics, as well as the on-disk size of the pruned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2824316e-ff26-4dfa-b1f4-f7d3e4c3e2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring baseline performance...\n",
      "[Baseline] latency   = 1.186s\n",
      "[Baseline] throughput= 42.2 tok/s\n",
      "[Baseline] peak GPU  = 2129.4 MiB\n",
      "[Baseline] perplexity= 4.555\n",
      "Pruning 20.0% of MLP neurons...\n",
      "Measuring pruned (unrebuilt) model performance...\n",
      "[Rebuilt] latency   = 1.453s\n",
      "[Rebuilt] throughput= 34.6 tok/s\n",
      "[Rebuilt] peak GPU  = 2129.4 MiB\n",
      "[Rebuilt] perplexity= 66230.342\n",
      "Rebuilding smaller MLP blocks...\n",
      "Measuring rebuilt model performance...\n",
      "[Rebuilt] latency   = 1.289s\n",
      "[Rebuilt] throughput= 38.9 tok/s\n",
      "[Rebuilt] peak GPU  = 1839.9 MiB\n",
      "[Rebuilt] perplexity= 66363.558\n",
      "Saving rebuilt model and reporting on-disk size...\n",
      "[Rebuilt] on-disk size = 1807.9 MiB\n"
     ]
    }
   ],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa8a5b",
   "metadata": {},
   "source": [
    "A huge jump in perplexity after pruning **usually indicates damage**, but it does not automatically mean the model is “broken beyond usefulness”. It means the model’s probability distribution became extremely peaked in the wrong places, so the log likelihood collapses. Let me walk through the possibilities so you can pin down the cause without jumping to conclusions.\n",
    "\n",
    "I’ll keep it grounded and avoid oversimplified answers.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. What a jump from 4 point something to hundreds of thousands actually means\n",
    "\n",
    "Perplexity is:\n",
    "\n",
    "$$\n",
    "\\exp\\left(\\frac{1}{N}\\sum \\log p_\\theta(\\text{token}_i)\\right)\n",
    "$$\n",
    "\n",
    "If perplexity becomes enormous, this means that **one or more token probabilities collapsed toward zero** during evaluation.\n",
    "\n",
    "Not literally zero, but something so tiny that the log becomes a huge negative number.\n",
    "\n",
    "A single very improbable prediction can explode the average.\n",
    "\n",
    "So this does not imply the entire distribution is ruined, only that some part of it output a catastrophic likelihood for the evaluation sequence.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Usual reasons why pruning causes catastrophic perplexity\n",
    "\n",
    "There are three main failure modes in manual pruning and rebuilding.\n",
    "\n",
    "## 2.1 You pruned too aggressively\n",
    "\n",
    "If you drop half or more of the inner MLP neurons in a compact model, the remaining layers cannot compensate.\n",
    "LLaMA style models are sensitive to the width of the MLP.\n",
    "Collapse of some activations then causes near zero logits for some tokens.\n",
    "\n",
    "## 2.2 You rebuilt the layers incorrectly\n",
    "\n",
    "This is very common in manual rebuild code.\n",
    "\n",
    "Typical issues:\n",
    "\n",
    "* misaligned indices in the layer slicing\n",
    "* incorrect mapping between gate, up, and down projections\n",
    "* mismatch in bias slicing\n",
    "* wrong use of keep indices\n",
    "* accidentally mixing rows and columns\n",
    "* off by one errors in concatenated MLP structures\n",
    "* using the wrong shape when creating the new Linear layer\n",
    "\n",
    "Any structural mismatch will create a model that runs but no longer represents the original function.\n",
    "\n",
    "The forward pass produces outputs, but the mapping is basically scrambled.\n",
    "\n",
    "## 2.3 Pruned neurons that were not truly dead\n",
    "\n",
    "Magnitude pruning is crude.\n",
    "A small weight is not always an unimportant weight.\n",
    "\n",
    "If your prune fraction is high, you cut out functional pathways.\n",
    "Then the MLP no longer constructs the correct intermediate representation.\n",
    "Perplexity explodes.\n",
    "\n",
    "Magnitude pruning cannot distinguish useful low magnitude weights from noise.\n",
    "\n",
    "This is why sparsity aware training or iterative gradual pruning exists.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Does this mean the model is broken?\n",
    "\n",
    "A perplexity jump from 4 point something to over four hundred thousand is not mild damage.\n",
    "It indicates either:\n",
    "\n",
    "* a structural error in the rebuild\n",
    "  or\n",
    "* an extreme pruning fraction that destroyed semantic structure.\n",
    "\n",
    "A healthy pruned transformer rarely jumps beyond a factor of two or three in perplexity after moderate pruning. A jump by five orders of magnitude is not normal at all.\n",
    "\n",
    "So for practical purposes: **yes, the model in its current form is broken**.\n",
    "\n",
    "But this does not mean the idea of pruning itself is broken.\n",
    "\n",
    "It only means this particular pruning and rebuilding pass did not preserve the structure.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. How to diagnose which of the three failure modes happened\n",
    "\n",
    "Here is the easiest diagnostic approach.\n",
    "\n",
    "### Step 1: measure perplexity right after zeroing weights\n",
    "\n",
    "Before rebuilding, run perplexity evaluation.\n",
    "\n",
    "If perplexity is still sane (for example 4.5 goes to 6 or 7), then the pruning thresholds were fine.\n",
    "\n",
    "If perplexity explodes already here, pruning was too aggressive.\n",
    "\n",
    "### Step 2: measure perplexity right after rebuilding the first layer\n",
    "\n",
    "Rebuild only MLP block 0 and measure again.\n",
    "Then rebuild 0 and 1, measure again.\n",
    "Continue.\n",
    "\n",
    "If the explosion happens only after a particular layer rebuild, you found a structural error in that layer’s MLP reconstruction.\n",
    "\n",
    "This is the cleanest way to locate misalignment.\n",
    "\n",
    "### Step 3: instrument shapes and index mappings\n",
    "\n",
    "Print:\n",
    "\n",
    "* keep_idx\n",
    "* shape of old layers\n",
    "* shape of rebuilt layers\n",
    "* norms of pruned rows\n",
    "* a few random input–output comparisons\n",
    "\n",
    "Look for rows that were supposed to be all zero but are not.\n",
    "Look for columns that do not match.\n",
    "Look for mismatched shapes between gate, up and down.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f622f",
   "metadata": {},
   "source": [
    "# Outro: What We Achieved and What Actually Happened\n",
    "\n",
    "In this notebook we walked through magnitude based pruning at a low level, without relying on PyTorch’s pruning framework or Hugging Face Optimum. Every step was done manually so that the computational mechanics become transparent rather than hidden behind abstractions.\n",
    "\n",
    "The pruning process consisted of three essential parts:\n",
    "\n",
    "**First**, we identified low importance neurons inside the MLP blocks by measuring weight magnitudes. These neurons were not “deleted” at first. Instead we explicitly zeroed their incoming rows and outgoing columns. This made their activations collapse to zero for every input, which means they no longer contributed anything to the forward pass.\n",
    "\n",
    "**Second**, we extracted the structure that remained after pruning. Zero rows in the expansion layers and zero columns in the contraction layer mark dead hidden units. Since these units contribute nothing, the model is already functionally equivalent to a smaller model. At this stage the model still *looks* large on disk and in memory since the zeroed neurons remain as part of the tensors.\n",
    "\n",
    "**Third**, we rebuilt the MLPs into truly smaller Linear layers. For each transformer block we created new projection layers whose shapes exactly matched the surviving neurons. We sliced the original dense weights to keep only the rows and columns associated with the active hidden units. This step removes the dead neurons entirely rather than just setting them to zero. Importantly, this does not change the function of the model because the removed units already produced zero outputs and had zero influence on downstream computations.\n",
    "\n",
    "This manual reconstruction step is the key difference between cosmetic sparsity and actual model compression. Standard PyTorch pruning only applies masks and leaves tensor shapes unchanged. The notebook instead compresses the architecture itself, producing a smaller and more efficient MLP while preserving the model’s behavior.\n",
    "\n",
    "In practice, this approach is useful for educational insight, research experiments or controlled model surgery. Production workflows often rely on pruning frameworks that integrate sparsity aware training, export pipelines and hardware aligned sparse kernels. However, the manual approach used here exposes the algebraic structure behind pruning in its clearest possible form. You can now see both the limitations of naïve masking and the precise mathematical reason why zero structured neurons can be safely removed.\n",
    "\n",
    "At this point you have a functional, pruned and rebuilt model that is smaller in parameter count and computational cost, yet mathematically equivalent to the pruned dense model. This is the foundational idea behind structured MLP pruning and a stepping stone toward more advanced sparsity methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c5b70",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
